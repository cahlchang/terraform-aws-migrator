Project Context for Claude

Generated at: 2025-02-08T12:03:50.388071

Project Structure:
```
├── htmlcov
│   ├── class_index.html
│   ├── coverage_html_cb_6fb7b396.js
│   ├── favicon_32_cb_58284776.png
│   ├── function_index.html
│   ├── index.html
│   ├── keybd_closed_cb_ce680311.png
│   ├── status.json
│   ├── style_cb_8e611ae1.css
│   ├── z_1cae05a581ae8bfd___init___py.html
│   ├── z_1cae05a581ae8bfd_resource_utils_py.html
│   ├── z_53ded241bbba5aa0___init___py.html
│   ├── z_53ded241bbba5aa0_network_py.html
│   ├── z_53ded241bbba5aa0_route_py.html
│   ├── z_53ded241bbba5aa0_vpc_py.html
│   ├── z_618e05d3bc92774d___init___py.html
│   ├── z_618e05d3bc92774d_instance_profile_py.html
│   ├── z_618e05d3bc92774d_policy_py.html
│   ├── z_618e05d3bc92774d_role_policy_attachment_py.html
│   ├── z_618e05d3bc92774d_role_py.html
│   ├── z_618e05d3bc92774d_user_policy_attachment_py.html
│   ├── z_618e05d3bc92774d_user_policy_py.html
│   ├── z_618e05d3bc92774d_user_py.html
│   ├── z_6f1ed9ecb843eceb___init___py.html
│   ├── z_6f1ed9ecb843eceb_base_py.html
│   ├── z_79bc2eb159d603d3___init___py.html
│   ├── z_79bc2eb159d603d3_output_formatter_py.html
│   ├── z_a01a1bf0550e1bf5___init___py.html
│   ├── z_a01a1bf0550e1bf5_group_py.html
│   ├── z_a01a1bf0550e1bf5_instance_profile_py.html
│   ├── z_a01a1bf0550e1bf5_policy_py.html
│   ├── z_a01a1bf0550e1bf5_role_py.html
│   ├── z_a01a1bf0550e1bf5_user_py.html
│   ├── z_cc5618d91ff2863c___init___py.html
│   ├── z_cc5618d91ff2863c_lb_py.html
│   ├── z_cc5618d91ff2863c_listener_py.html
│   ├── z_cc5618d91ff2863c_listener_rule_py.html
│   ├── z_cc5618d91ff2863c_nacl_dhcp_py.html
│   ├── z_cc5618d91ff2863c_nat_gateway_py.html
│   ├── z_cc5618d91ff2863c_route_py.html
│   ├── z_cc5618d91ff2863c_route_table_py.html
│   ├── z_cc5618d91ff2863c_subnet_py.html
│   ├── z_cc5618d91ff2863c_target_group_py.html
│   ├── z_cc5618d91ff2863c_vpc_endpoint_py.html
│   ├── z_cc5618d91ff2863c_vpc_py.html
│   ├── z_cf88d2db7849d309___init___py.html
│   ├── z_cf88d2db7849d309___main___py.html
│   ├── z_cf88d2db7849d309_auditor_py.html
│   ├── z_cf88d2db7849d309_collection_status_py.html
│   ├── z_cf88d2db7849d309_exclusion_py.html
│   ├── z_cf88d2db7849d309_main_py.html
│   ├── z_cf88d2db7849d309_state_reader_py.html
│   ├── z_e950f48fc324c2e7___init___py.html
│   ├── z_e950f48fc324c2e7_aws_application_py.html
│   ├── z_e950f48fc324c2e7_aws_compute_py.html
│   ├── z_e950f48fc324c2e7_aws_database_py.html
│   ├── z_e950f48fc324c2e7_aws_security_py.html
│   ├── z_e950f48fc324c2e7_aws_storage_py.html
│   ├── z_e950f48fc324c2e7_base_py.html
│   ├── z_fb2a7edc80d4b66c___init___py.html
│   ├── z_fb2a7edc80d4b66c_ebs_py.html
│   ├── z_fb2a7edc80d4b66c_efs_py.html
│   └── z_fb2a7edc80d4b66c_s3_py.html
├── terraform_aws_migrator
│   ├── collectors
│   │   ├── aws_iam
│   │   │   ├── __init__.py
│   │   │   ├── group.py
│   │   │   ├── instance_profile.py
│   │   │   ├── policy.py
│   │   │   ├── role.py
│   │   │   └── user.py
│   │   ├── aws_network
│   │   │   ├── __init__.py
│   │   │   ├── network.py
│   │   │   ├── route.py
│   │   │   └── vpc.py
│   │   ├── aws_storage
│   │   │   ├── __init__.py
│   │   │   ├── ebs.py
│   │   │   ├── efs.py
│   │   │   └── s3.py
│   │   ├── __init__.py
│   │   ├── aws_application.py
│   │   ├── aws_compute.py
│   │   ├── aws_database.py
│   │   ├── aws_security.py
│   │   ├── aws_storage.py
│   │   └── base.py
│   ├── formatters
│   │   ├── __init__.py
│   │   └── output_formatter.py
│   ├── generators
│   │   ├── aws_compute
│   │   │   ├── ec2.py
│   │   │   ├── lambda.py
│   │   │   └── security_group.py
│   │   ├── aws_iam
│   │   │   ├── __init__.py
│   │   │   ├── instance_profile.py
│   │   │   ├── policy.py
│   │   │   ├── role.py
│   │   │   ├── role_policy_attachment.py
│   │   │   ├── user.py
│   │   │   ├── user_policy.py
│   │   │   └── user_policy_attachment.py
│   │   ├── aws_network
│   │   │   ├── __init__.py
│   │   │   ├── lb.py
│   │   │   ├── listener.py
│   │   │   ├── listener_rule.py
│   │   │   ├── load_balancer_resources.tf
│   │   │   ├── nacl_dhcp.py
│   │   │   ├── nat_gateway.py
│   │   │   ├── route.py
│   │   │   ├── route_table.py
│   │   │   ├── subnet.py
│   │   │   ├── target_group.py
│   │   │   ├── vpc.py
│   │   │   └── vpc_endpoint.py
│   │   ├── aws_storage
│   │   │   └── s3.py
│   │   ├── __init__.py
│   │   └── base.py
│   ├── utils
│   │   ├── __init__.py
│   │   └── resource_utils.py
│   ├── __init__.py
│   ├── __main__.py
│   ├── auditor.py
│   ├── collection_status.py
│   ├── exclusion.py
│   ├── main.py
│   └── state_reader.py
├── terraform_aws_migrator.egg-info
│   ├── PKG-INFO
│   ├── SOURCES.txt
│   ├── dependency_links.txt
│   ├── entry_points.txt
│   ├── requires.txt
│   └── top_level.txt
├── tests
│   ├── fixtures
│   │   ├── lb_test.tfstate
│   │   ├── lb_unmanaged_test.tfstate
│   │   └── test.tfstate
│   ├── unit
│   │   ├── collectors
│   │   │   ├── test_base.py
│   │   │   └── test_network.py
│   │   ├── test_auditor.py
│   │   ├── test_lb_collector.py
│   │   ├── test_state_reader.py
│   │   └── test_vpc_endpoint.py
│   ├── __init__.py
│   └── conftest.py
├── tmp
├── LICENSE
├── MANIFEST.in
├── README.md
├── context_creator.py
├── context_sync.py
├── debug.py
├── ignore_exampe.md
├── project_context.txt
├── pyproject.toml
├── pytest.ini
├── requirements.txt
└── setup.py
```

Project Files:

<document>
<source>context_creator.py</source>
<document_content>import os
import asyncio
from pathlib import Path
from typing import List, Set, Optional
from datetime import datetime

class ProjectContextGenerator:
    def __init__(self, project_path: str, output_path: str):
        self.project_path = Path(project_path)
        self.output_path = Path(output_path)
        self.excluded_dirs = {
            '.venv',
            '__pycache__',
            'node_modules',
            '.git',
            'venv'
        }
        self.excluded_patterns = {
            '.pyc',
            '.pyo',
            '.pyd',
            '.so',
            '.dll',
            '.dylib'
        }

    def should_exclude_path(self, path: Path) -> bool:
        try:
            rel_path = path.relative_to(self.project_path)
        except ValueError:
            return True

        parts = rel_path.parts
        for part in parts:
            if part in self.excluded_dirs:
                return True
            if part.startswith('.') and part != '.':
                return True

        if path.is_file() and path.suffix in self.excluded_patterns:
            return True

        return False

    async def read_file_content(self, file_path: Path) -> str:
        """ファイルを読み込む"""
        try:
            def read_file():
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            
            content = await asyncio.to_thread(read_file)
            return content
        except Exception as e:
            print(f"Warning: Error reading file {file_path}: {e}")
            return ""

    async def generate_project_tree(self, directory: Optional[Path] = None, prefix: str = '') -> str:
        """プロジェクトのツリー構造を生成"""
        if directory is None:
            directory = self.project_path

        tree = ''
        try:
            items = sorted(await asyncio.to_thread(list, directory.iterdir()),
                         key=lambda x: (x.is_file(), x.name))
        except PermissionError:
            return tree

        for i, item in enumerate(items):
            if self.should_exclude_path(item):
                continue

            is_last = i == len(items) - 1
            branch = '└── ' if is_last else '├── '
            tree += f"{prefix}{branch}{item.name}\n"

            if item.is_dir():
                new_prefix = prefix + ('    ' if is_last else '│   ')
                tree += await self.generate_project_tree(item, new_prefix)

        return tree

    async def collect_files(self, extensions: Set[str]) -> dict:
        """指定された拡張子のファイルを収集"""
        files_data = {}
        for file_path in self.project_path.rglob('*'):
            if self.should_exclude_path(file_path):
                continue

            if file_path.is_file() and file_path.suffix in extensions:
                relative_path = str(file_path.relative_to(self.project_path))
                content = await self.read_file_content(file_path)
                files_data[relative_path] = content
        return files_data

    async def format_document_tags(self, files_data: dict) -> str:
        """ファイルデータをドキュメントタグ形式にフォーマット"""
        formatted = ""
        for file_path, content in files_data.items():
            formatted += f"<document>\n<source>{file_path}</source>\n"
            formatted += f"<document_content>{content}</document_content>\n"
            formatted += "</document>\n\n"
        return formatted

    async def generate_context_file(self, extensions: List[str]) -> None:
        """コンテキストファイルを生成"""
        try:
            # プロジェクト構造の取得
            tree = await self.generate_project_tree()
            
            # ファイル内容の収集
            files_data = await self.collect_files(set(extensions))
            
            # 出力ディレクトリの作成
            self.output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # コンテキストファイルの作成
            async with asyncio.Lock():
                def write_file():
                    with open(self.output_path, 'w', encoding='utf-8') as f:
                        f.write("Project Context for Claude\n\n")
                        f.write(f"Generated at: {datetime.now().isoformat()}\n\n")
                        f.write("Project Structure:\n")
                        f.write("```\n")
                        f.write(tree)
                        f.write("```\n\n")
                        f.write("Project Files:\n\n")
                        formatted_files = asyncio.run(self.format_document_tags(files_data))
                        f.write(formatted_files)
                
                await asyncio.to_thread(write_file)
                
            print(f"Context file generated successfully at: {self.output_path}")
            print(f"Total files processed: {len(files_data)}")
                    
        except Exception as e:
            print(f"Error generating context file: {e}")
            raise

    def add_excluded_dir(self, dir_name: str) -> None:
        """除外ディレクトリを追加"""
        self.excluded_dirs.add(dir_name)

    def add_excluded_pattern(self, pattern: str) -> None:
        """除外パターンを追加"""
        self.excluded_patterns.add(pattern)

async def main():
    """使用例"""
    try:
        # プロジェクトパスと出力パスを設定
        project_path = "/Users/morin/git/terraform-aws-migrator"
        output_path = "./project_context.txt"

        # ジェネレーターを初期化
        generator = ProjectContextGenerator(
            project_path=project_path,
            output_path=output_path
        )

        # 必要に応じて除外設定を追加
        generator.add_excluded_dir('build')
        generator.add_excluded_pattern('.cache')

        # コンテキストファイルを生成
        await generator.generate_context_file(
            extensions=[".py", ".md", ".tf", ".json"]
        )

    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == "__main__":
    asyncio.run(main())
</document_content>
</document>

<document>
<source>README.md</source>
<document_content># Terraform AWS Migrator

**Terraform AWS Migrator** is a tool designed to audit AWS resources and identify which ones are not managed by Terraform. It compares resources defined in Terraform state files against the actual resources found in your AWS account, helping you maintain proper resource management and avoid resource drift.

## Features

- **Seamless Terraform State Integration**  
  Automatically reads local and remote (S3-backed) Terraform state files to determine which AWS resources are currently under Terraform's control, ensuring consistency and accuracy in your infrastructure management.

- **Extensive AWS Resource Coverage**  
  Employing a pluggable architecture, the tool uses multiple collectors to discover and map a wide array of AWS services:

  - **Application**:
    - Step Functions State Machines
  - **Compute**:
    - EC2 Instances
    - ECS Clusters and Services
    - Lambda Functions
    - EBS Volumes
    - Virtual Private Clouds (VPC)
    - Security Groups
  - **Database**:
    - RDS Database Instances and Clusters
    - DynamoDB Tables
    - ElastiCache Clusters and Replication Groups
  - **Network**:
    - API Gateway REST APIs
    - API Gateway HTTP/WebSocket APIs
    - CloudFront Distributions
    - Legacy Load Balancers (ELB)
    - Application/Network Load Balancers (ALB/NLB)
    - LB Listeners and Listener Rules
    - LB Target Groups
    - Route 53 Hosted Zones
  - **Security**:
    - IAM Users, Groups, Roles, and Policies
    - KMS Customer-Managed Keys
    - Secrets Manager Secrets
  - **Storage**:
    - S3 Buckets
    - EFS File Systems
    - EBS Volumes

- **Unmanaged Resource Detection**  
  Identifies AWS resources not currently represented in your Terraform state, making it easy to bring these unmanaged components under Infrastructure as Code for streamlined operations.

- **Flexible Output Formats**  
  Outputs findings as both JSON and human-readable text, enabling effortless integration into CI/CD pipelines or direct review, ensuring that results are accessible and actionable.

## Getting Started

### Prerequisites

- Python 3.8+
- AWS credentials configured (e.g., via `aws configure` or environment variables)
- Terraform state files locally or accessible via S3

### Installation

1. pip install from GitHub:

   ```bash
   pip install git+https://github.com/cahlchang/terraform-aws-migrator.git
   ```

## Usage

Run the tool by specifying the directory that contains your Terraform configuration and state files:

```bash
terraform_aws_migrator --tf-dir path/to/terraform --output json
```

Command-line options:

- `--tf-dir`: The directory containing Terraform files and state.
- `--output`: The desired output format (text or json). Defaults to text.
- `--output-file`: Optional path to write the results instead of printing to stdout.
- `--list-resources`: List all supported AWS resource types.

### Example

```bash
terraform_aws_migrator --tf-dir ./terraform-code --output json --output-file unmanaged_resources.json
```

This command will:

- Scan your Terraform directory for state files.
- Retrieve AWS resources using various AWS service collectors.
- Identify which AWS resources are unmanaged.
- Output the result in JSON format to unmanaged_resources.json.

## Project Structure

- `terraform_aws_migrator/`:
  Core implementation files:
  - `auditor.py`: Main logic for auditing and comparing Terraform-managed vs. AWS-discovered resources.
  - `state_reader.py`: Reads Terraform state (local or S3) and extracts managed resource IDs.
  - `collectors/`: Contains AWS service-specific collectors (e.g., aws_compute.py, aws_database.py).
  - `formatters/output_formatter.py`: Formats the final report output.
  - `main.py`: Entry point for the CLI.
- `requirements.txt`:
  Lists Python dependencies for easy setup.
- `setup.py`:
  Enables installation as a Python package.

## Contributing

Contributions are welcome! To contribute:

1. Fork this repository.
2. Create a feature branch.
3. Submit a pull request with your changes.

We appreciate feedback on code quality, performance improvements, or suggestions for additional AWS services and Terraform integrations.

## License

This project is licensed under the MIT License.

## Contact

For questions or suggestions, please open an issue or reach out to the maintainer at kahlua.dane@gmail.com.
</document_content>
</document>

<document>
<source>setup.py</source>
<document_content>from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = [line.strip() for line in fh if line.strip() and not line.startswith("#")]

setup(
    name="terraform-aws-migrator",
    version="0.1.0",
    packages=find_packages(),
    include_package_data=True,
    package_data={
        'terraform_aws_migrator': ['*', '**/*'],
    },
    install_requires=requirements,
    entry_points={
        'console_scripts': [
            'terraform_aws_migrator=terraform_aws_migrator.main:main',
        ],
    },
    author="morin_river",
    author_email="kahlua.dane@gmail.com",
    description="A tool to migrate unmanaged AWS resources to Terraform",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/cahlchang/terraform-aws-migrator",
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires='>=3.8',
)
</document_content>
</document>

<document>
<source>debug.py</source>
<document_content># debug_terraform_state.py

import json
import argparse
from pathlib import Path
import boto3
import hcl2

class TerraformStateDebugger:
    def __init__(self):
        self.session = boto3.Session()

    def find_s3_backend(self, tf_dir: str) -> dict:
        """Find S3 backend configuration - only searches main.tf for backend config"""
        main_tf = Path(tf_dir) / "main.tf"
        if not main_tf.exists():
            print("main.tf not found")
            return None

        try:
            with open(main_tf) as f:
                content = hcl2.load(f)
                if 'terraform' not in content:
                    return None
                
                for terraform_block in content['terraform']:
                    if 'backend' not in terraform_block:
                        continue
                    
                    backend = terraform_block['backend']
                    if not isinstance(backend, list):
                        continue
                    
                    for backend_config in backend:
                        if 's3' in backend_config:
                            return backend_config['s3']
        except Exception as e:
            print(f"Error reading main.tf: {e}")
            return None
        
        return None

    def read_s3_state(self, bucket: str, key: str, region: str = None) -> dict:
        """Read Terraform state from S3"""
        print(f"\nReading state from S3: {bucket}/{key}")
        region = region or self.session.region_name
        s3_client = self.session.client('s3', region_name=region)
        
        try:
            response = s3_client.get_object(Bucket=bucket, Key=key)
            return json.loads(response['Body'].read().decode('utf-8'))
        except Exception as e:
            print(f"Error reading S3 state: {e}")
            return None

    def analyze_state(self, state_data: dict) -> dict:
        """Analyze terraform state and return managed resources info"""
        if not state_data:
            print("No state data to analyze")
            return {}

        managed_resources = {}

        # For Terraform 0.13+ format
        if 'resources' in state_data:
            for resource in state_data['resources']:
                module_path = resource.get('module', 'root')
                resource_type = resource.get('type', '')
                resource_name = resource.get('name', '')
                
                # Skip data sources
                if resource.get('mode', '') == "data":
                    continue

                instances = resource.get('instances', [])
                for instance in instances:
                    attributes = instance.get('attributes', {})
                    resource_info = {
                        'type': resource_type,
                        'name': resource_name,
                        'module': module_path,
                        'id': attributes.get('id', 'N/A'),
                        'arn': attributes.get('arn'),
                        'tags': attributes.get('tags', {})
                    }
                    
                    key = f"{module_path}/{resource_type}.{resource_name}"
                    managed_resources[key] = resource_info

        # For older state format
        if 'modules' in state_data:
            for module in state_data['modules']:
                module_path = '.'.join(module.get('path', ['root']))
                resources = module.get('resources', {})
                
                for resource_addr, resource in resources.items():
                    # Skip data sources
                    if resource.get('type', '').startswith('data.'):
                        continue
                    
                    primary = resource.get('primary', {})
                    attributes = primary.get('attributes', {})
                    
                    resource_info = {
                        'type': resource.get('type', ''),
                        'name': resource_addr,
                        'module': module_path,
                        'id': attributes.get('id', 'N/A'),
                        'arn': attributes.get('arn'),
                        'tags': attributes.get('tags', {})
                    }
                    
                    key = f"{module_path}/{resource.get('type')}.{resource_addr}"
                    managed_resources[key] = resource_info

        return managed_resources

    def print_resources(self, resources: dict):
        """Print managed resources in a structured format"""
        if not resources:
            print("No managed resources found")
            return

        print("\nManaged Resources:")
        print("-" * 80)
        
        # Group by module
        by_module = {}
        for key, resource in resources.items():
            module = resource['module']
            if module not in by_module:
                by_module[module] = []
            by_module[module].append(resource)

        for module, module_resources in by_module.items():
            print(f"\nModule: {module}")
            print("-" * 40)
            
            # Sort resources by type and name
            module_resources.sort(key=lambda x: (x['type'], x['name']))
            
            for resource in module_resources:
                print(f"\n{resource['type']}.{resource['name']}:")
                print(f"  ID:  {resource['id']}")
                if resource['arn']:
                    print(f"  ARN: {resource['arn']}")
                if resource['tags']:
                    print("  Tags:")
                    for key, value in resource['tags'].items():
                        print(f"    {key}: {value}")

def main():
    parser = argparse.ArgumentParser(description='Debug Terraform state')
    parser.add_argument('--tf-dir', type=str, required=True,
                       help='Directory containing Terraform files')
    args = parser.parse_args()

    debugger = TerraformStateDebugger()

    # Find S3 backend config
    s3_config = debugger.find_s3_backend(args.tf_dir)
    if s3_config:
        print("\nFound S3 backend configuration:")
        print(f"Bucket: {s3_config['bucket']}")
        print(f"Key: {s3_config['key']}")
        
        # Read and analyze S3 state
        state_data = debugger.read_s3_state(
            bucket=s3_config['bucket'],
            key=s3_config['key'],
            region=s3_config.get('region')
        )
        if state_data:
            resources = debugger.analyze_state(state_data)
            debugger.print_resources(resources)
    
    # Also check local state
    tf_dir_path = Path(args.tf_dir)
    state_files = list(tf_dir_path.glob('**/*.tfstate'))
    for state_file in state_files:
        print(f"\nAnalyzing local state file: {state_file}")
        try:
            with open(state_file) as f:
                state_data = json.load(f)
                resources = debugger.analyze_state(state_data)
                debugger.print_resources(resources)
        except Exception as e:
            print(f"Error reading local state file: {e}")

if __name__ == '__main__':
    main()
</document_content>
</document>

<document>
<source>ignore_exampe.md</source>
<document_content># Setting up .tfignore

## Initial Setup

1. Rename the example file to .tfignore:
```bash
mv ignore_exampe.md .tfignore
```

2. If you're on Windows and having trouble with the rename:
```cmd
ren ignore_exampe.md .tfignore
```

## File Location

Place the .tfignore file in one of these locations:
- In your Terraform project root directory (recommended)
- In the same directory where you run terraform_aws_migrator
- Specify a custom path using the --ignore-file flag:
  ```bash
  terraform_aws_migrator --tf-dir ./terraform --ignore-file /path/to/.tfignore
  ```

## Example Configuration

Here's a recommended .tfignore configuration:

```
# Terraform AWS Resource Exclusions
# Format: 
# - aws_<service>_<resource>:<identifier> (Terraform resource type format)
# - <service>:<resource_type>/* (AWS service format)
# - Direct resource IDs or ARNs
# - Name tag values (full value or pattern)
# - <service>:<name-tag-value> (Service with Name tag format)

# EC2 Resources
aws_instance:i-0123456789abcdef0     # Specific test instance by ID
aws_instance:test-*                  # Instances with Name tag starting with "test-"
prod-backup-*                        # Any resource with Name tag starting with "prod-backup-"
ec2:staging-*                        # EC2 instances with Name tag starting with "staging-"

# VPC Resources with Name Tags
aws_vpc:prod-vpc-*                   # VPCs with Name tag starting with "prod-vpc-"
vpc:test-network-*                   # VPCs with Name tag starting with "test-network-"

# Load Balancer Resources
aws_lb:internal-*                    # Load balancers with Name tag starting with "internal-"
lb:test-alb-*                        # Load balancers with Name tag starting with "test-alb-"

# Lambda Resources
aws_lambda_function:maintenance-*     # Maintenance functions (by ID or Name tag)
lambda:backup-*                       # Backup functions (by ID or Name tag)

# Database Resources
aws_dynamodb_table:audit-*           # Audit tables (by ID or Name tag)
aws_db_instance:*-snapshot           # RDS instances with "-snapshot" suffix (ID or Name tag)
rds:dev-*                            # RDS instances with Name tag starting with "dev-"

# IAM Resources
aws_iam_role:service-*               # Service roles
aws_iam_user:system-*                # System users
aws_iam_group:readonly-*             # Read-only groups
aws_iam_policy:AWS*                  # AWS managed policies
```

## Pattern Formats

The .tfignore file supports these pattern formats:

1. Terraform Resource Type Format (Recommended):
   ```
   aws_iam_role:role-name*
   aws_lambda_function:function-name*
   ```

2. AWS Service Format:
   ```
   iam:role/role-name*
   lambda:function/function-name*
   ```

3. Name Tag Format:
   ```
   prod-*              # Match any resource with Name tag starting with "prod-"
   ec2:test-*          # Match EC2 resources with Name tag starting with "test-"
   aws_instance:dev-*  # Match EC2 instances with Name tag starting with "dev-"
   ```

4. Direct ARN Format:
   ```
   arn:aws:iam::*:role/role-name*
   arn:aws:lambda:*:*:function:function-name*
   ```

## Pattern Matching Rules

1. Resource ID Matching:
   - Patterns match against resource IDs directly
   - Example: `i-1234567890abcdef0`, `vpc-12345678`

2. Name Tag Matching:
   - Patterns match against the value of the "Name" tag
   - Can be used with or without service/resource type prefix
   - More flexible for resources that follow naming conventions

3. Service-Specific Matching:
   - Prefix patterns with service name for more targeted exclusions
   - Example: `ec2:prod-*` only matches EC2 resources

4. Full Resource Type Matching:
   - Most specific matching using complete AWS resource type
   - Example: `aws_instance:prod-*`

## Usage Tips

- Use # for comments
- Patterns are case-sensitive
- * matches any number of characters
- Blank lines are ignored
- Each pattern should be on a new line
- More specific patterns should be listed before general patterns
- When using aws_ prefix format, match the exact Terraform resource type name
- Name tag patterns can be used with or without service/type prefixes
- Service-specific patterns (e.g., ec2:prod-*) are more precise than general patterns (prod-*)

## Command Line Usage

```bash
# Use default .tfignore in current directory
terraform_aws_migrator --tf-dir ./terraform

# Specify custom ignore file location
terraform_aws_migrator --tf-dir ./terraform --ignore-file /path/to/.tfignore
```

## Best Practices

1. Start with specific exclusions:
   ```
   # Specific instance
   aws_instance:i-0123456789abcdef0
   ```

2. Add service-prefixed patterns:
   ```
   # All test EC2 instances
   ec2:test-*
   ```

3. Use Name tag patterns for groups of resources:
   ```
   # All resources tagged as production backups
   prod-backup-*
   ```

4. Document your exclusions with comments:
   ```
   # Development environment resources
   dev-*                  # All dev resources
   ec2:dev-*             # Dev EC2 instances
   aws_rds_cluster:dev-* # Dev RDS clusters
   ```
</document_content>
</document>

<document>
<source>context_sync.py</source>
<document_content>import os
import json
import asyncio
import subprocess
from pathlib import Path
from typing import List, Set, Optional, Tuple, Dict
from datetime import datetime
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.api_core import exceptions

SCOPES = ['https://www.googleapis.com/auth/drive.file']
CONFIG_DIR = os.path.expanduser('~/.claudesync')
CONFIG_FILE = os.path.join(CONFIG_DIR, 'config.json')
DEFAULT_CONFIG = {
    "service_account_path": os.path.join(CONFIG_DIR, "service-account.json"),
    "drive_folder_id": "",
    "excluded_dirs": [
        ".venv",
        "__pycache__",
        "node_modules",
        ".git",
        "venv",
        "build"
    ],
    "excluded_patterns": [
        ".pyc",
        ".pyo",
        ".pyd",
        ".so",
        ".dll",
        ".dylib",
        ".cache"
    ]
}

class ConfigManager:
    @staticmethod
    def initialize_config() -> None:
        """設定ディレクトリと設定ファイルの初期化"""
        if not os.path.exists(CONFIG_DIR):
            os.makedirs(CONFIG_DIR)
            
        if not os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'w') as f:
                json.dump(DEFAULT_CONFIG, f, indent=2)
                
            print(f"\nCreated default config file at: {CONFIG_FILE}")
            print("Please update the following settings:")
            print("1. service_account_path: Path to your Google service account JSON file")
            print("2. drive_folder_id: ID of the Google Drive folder (e.g., '1ttkULdSquyogNnL5Nrc4cnYuUIlKkUW2')")
            
    @staticmethod
    def load_config() -> Dict:
        """設定ファイルの読み込み"""
        if not os.path.exists(CONFIG_FILE):
            ConfigManager.initialize_config()
            
        try:
            with open(CONFIG_FILE, 'r') as f:
                config = json.load(f)
                
            # 必須項目の検証
            if not config.get('service_account_path'):
                raise ValueError("service_account_path is not set in config file")
            if not config.get('drive_folder_id'):
                raise ValueError("drive_folder_id is not set in config file")
                
            return config
            
        except Exception as e:
            print(f"\nError loading config file: {e}")
            print(f"Please check the configuration at: {CONFIG_FILE}")
            raise

class ProjectContextGenerator:
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.config = ConfigManager.load_config()
        self.excluded_dirs = set(self.config['excluded_dirs'])
        self.excluded_patterns = set(self.config['excluded_patterns'])

    def add_excluded_dir(self, dir_name: str) -> None:
        """除外ディレクトリを追加"""
        self.excluded_dirs.add(dir_name)

    def add_excluded_pattern(self, pattern: str) -> None:
        """除外パターンを追加"""
        self.excluded_patterns.add(pattern)

    def get_google_drive_service(self):
        """Service Account を使用してGoogle Drive API の認証とサービスを取得"""
        try:
            service_account_path = os.path.expanduser(self.config['service_account_path'])
            if not os.path.exists(service_account_path):
                print(f"\nError: Service account file not found at: {service_account_path}")
                print("\nPlease update the service_account_path in the config file:")
                print(f"Config file location: {CONFIG_FILE}")
                raise FileNotFoundError(f"Service account file not found: {service_account_path}")

            credentials = service_account.Credentials.from_service_account_file(
                service_account_path,
                scopes=SCOPES
            )
            return build('drive', 'v3', credentials=credentials)

        except exceptions.PermissionDenied as e:
            print("\nError: Insufficient permissions")
            print("Required permissions for the service account:")
            print("- https://www.googleapis.com/auth/drive.file (Google Drive File Access)")
            raise

        except Exception as e:
            print(f"\nError initializing Google Drive service: {str(e)}")
            raise

    def get_git_info(self) -> Tuple[str, str]:
        """Gitのアカウント名（organization）とリポジトリ名を取得"""
        try:
            original_dir = os.getcwd()
            os.chdir(self.project_path)

            remote_url = subprocess.check_output(
                ["git", "config", "--get", "remote.origin.url"], 
                text=True
            ).strip()
            
            # URLからアカウント名とリポジトリ名を抽出
            if remote_url.startswith('git@'):
                # SSH形式 (git@github.com:account/repo.git)
                path = remote_url.split(':')[1]
            else:
                # HTTPS形式 (https://github.com/account/repo.git)
                path = remote_url.split('github.com/')[-1]
                
            account_name = path.split('/')[0]
            repo_name = path.split('/')[-1].replace('.git', '')

            os.chdir(original_dir)
            return account_name, repo_name
        except subprocess.CalledProcessError as e:
            print(f"Error getting git info: {e}")
            return "unknown-user", "unknown-repo"

    async def generate_project_tree(self) -> str:
        """プロジェクトのディレクトリ構造をツリー形式で生成"""
        output = []
        
        def should_exclude(path: Path) -> bool:
            """除外すべきパスかどうかを判定"""
            return (any(excluded in path.parts for excluded in self.excluded_dirs) or
                    any(path.name.endswith(pattern) for pattern in self.excluded_patterns))

        def add_to_tree(directory: Path, prefix: str = ""):
            """ディレクトリツリーを再帰的に生成"""
            entries = sorted(directory.iterdir(), key=lambda x: (x.is_file(), x.name))
            
            for i, entry in enumerate(entries):
                if should_exclude(entry):
                    continue
                    
                is_last = i == len(entries) - 1
                current_prefix = "└── " if is_last else "├── "
                next_prefix = "    " if is_last else "│   "
                
                output.append(f"{prefix}{current_prefix}{entry.name}")
                
                if entry.is_dir():
                    add_to_tree(entry, prefix + next_prefix)

        add_to_tree(self.project_path)
        return "\n".join(output)

    async def collect_files(self, extensions: Set[str]) -> List[Tuple[str, str]]:
        """指定された拡張子のファイルを収集"""
        files_data = []

        async def process_file(file_path: Path) -> Optional[Tuple[str, str]]:
            """ファイルの内容を読み込む"""
            try:
                if file_path.suffix not in extensions:
                    return None
                    
                async with asyncio.Lock():
                    content = await asyncio.to_thread(
                        lambda: file_path.read_text(encoding='utf-8')
                    )
                return (str(file_path.relative_to(self.project_path)), content)
            except Exception as e:
                print(f"Error reading file {file_path}: {e}")
                return None

        tasks = []
        for root, _, files in os.walk(self.project_path):
            root_path = Path(root)
            if any(excluded in root_path.parts for excluded in self.excluded_dirs):
                continue

            for file in files:
                file_path = root_path / file
                if any(file_path.name.endswith(pattern) for pattern in self.excluded_patterns):
                    continue
                    
                tasks.append(process_file(file_path))

        results = await asyncio.gather(*tasks)
        return [r for r in results if r is not None]

    async def format_document_tags(self, files_data: List[Tuple[str, str]]) -> str:
        """ファイルデータをドキュメントタグ形式にフォーマット"""
        formatted = []
        
        for file_path, content in files_data:
            formatted.extend([
                "<document>",
                f"<source>{file_path}</source>",
                "<document_content>",
                content,
                "</document_content>",
                "</document>",
                ""
            ])
            
        return "\n".join(formatted)

    async def upload_to_google_drive(self, file_path: Path) -> None:
        """Google Drive にファイルをアップロード"""
        try:
            service = self.get_google_drive_service()
            filename = file_path.name

            # ファイルのMIMEタイプを設定
            media = MediaFileUpload(
                str(file_path),
                mimetype='text/plain',
                resumable=True
            )

            try:
                # 既存のファイルを検索
                results = service.files().list(
                    q=f"name = '{filename}' and '{self.config['drive_folder_id']}' in parents and trashed = false",
                    spaces='drive',
                    fields='files(id, name)'
                ).execute()
                
                existing_files = results.get('files', [])

                if existing_files:
                    # 既存のファイルを更新
                    file_id = existing_files[0]['id']
                    service.files().update(
                        fileId=file_id,
                        media_body=media
                    ).execute()
                    print(f"Updated existing file in Google Drive: {filename}")
                else:
                    # 新規ファイルを作成
                    file_metadata = {
                        'name': filename,
                        'parents': [self.config['drive_folder_id']]
                    }
                    service.files().create(
                        body=file_metadata,
                        media_body=media,
                        fields='id',
                        supportsAllDrives=True
                    ).execute()
                    print(f"Created new file in Google Drive: {filename}")

                # 一時ファイルを削除
                file_path.unlink()
                print(f"Cleaned up temporary file: {file_path}")

            except exceptions.PermissionDenied:
                print("\nError: Unable to access the specified Google Drive folder")
                print(f"Please ensure the service account has access to folder ID: {self.config['drive_folder_id']}")
                raise

        except Exception as e:
            print(f"\nError uploading to Google Drive: {str(e)}")
            raise

    async def generate_and_upload_context(self, extensions: List[str]) -> None:
        """コンテキストファイルを生成してGoogle Driveにアップロード"""
        try:
            # Gitの情報を取得してファイル名を生成
            username, repo_name = self.get_git_info()
            filename = f"{username}-{repo_name}.txt"
            temp_path = self.project_path / "tmp" / filename
            
            # 一時ディレクトリを作成
            temp_path.parent.mkdir(parents=True, exist_ok=True)
            
            # コンテンツを生成
            tree = await self.generate_project_tree()
            files_data = await self.collect_files(set(extensions))
            
            # 一時ファイルに書き込み
            async with asyncio.Lock():
                def write_file():
                    with open(temp_path, 'w', encoding='utf-8') as f:
                        f.write("Project Context for Claude\n\n")
                        f.write(f"Generated at: {datetime.now().isoformat()}\n\n")
                        f.write("Project Structure:\n")
                        f.write("```\n")
                        f.write(tree)
                        f.write("```\n\n")
                        f.write("Project Files:\n\n")
                        formatted_files = asyncio.run(self.format_document_tags(files_data))
                        f.write(formatted_files)
                
                await asyncio.to_thread(write_file)
            
            # Google Driveにアップロード
            await self.upload_to_google_drive(temp_path)
            
            print(f"Context generation and upload completed successfully")
            print(f"Total files processed: {len(files_data)}")
                    
        except Exception as e:
            print(f"Error in generate_and_upload_context: {e}")
            raise

def parse_args():
    """コマンドライン引数をパース"""
    import argparse
    parser = argparse.ArgumentParser(description='Project Context Generator for Claude')
    
    parser.add_argument(
        '-t', '--target',
        required=True,
        help='Target directory path for context generation'
    )
    
    parser.add_argument(
        '-e', '--extensions',
        nargs='+',
        default=['.py', '.md'],
        help='File extensions to include (default: .py .md .tf .json)'
    )
    
    parser.add_argument(
        '--exclude-dirs',
        nargs='+',
        default=[],
        help='Additional directories to exclude'
    )
    
    parser.add_argument(
        '--exclude-patterns',
        nargs='+',
        default=[],
        help='Additional file patterns to exclude'
    )
    
    return parser.parse_args()

async def main():
    """メイン処理"""
    try:
        args = parse_args()
        
        # パスを絶対パスに変換
        target_path = str(Path(args.target).resolve())
        # 拡張子の形式を統一（ドットがない場合は追加）
        extensions = [
            ext if ext.startswith('.') else f'.{ext}'
            for ext in args.extensions
        ]
        
        # ジェネレーターの初期化
        generator = ProjectContextGenerator(
            project_path=target_path
        )

        # 除外設定の追加
        for dir_name in args.exclude_dirs:
            generator.add_excluded_dir(dir_name)
            
        for pattern in args.exclude_patterns:
            generator.add_excluded_pattern(pattern)

        # コンテキストを生成してGoogle Driveにアップロード
        await generator.generate_and_upload_context(extensions=extensions)

    except Exception as e:
        print(f"Error in main: {e}")
        raise SystemExit(1)

if __name__ == "__main__":
    asyncio.run(main())
</document_content>
</document>

<document>
<source>terraform_aws_migrator/state_reader.py</source>
<document_content># terraform_aws_migrator/state_reader.py

import json
from pathlib import Path
from typing import Dict, List, Any, Set, Optional
import boto3
import hcl2
from rich.console import Console
import logging
import traceback

logger = logging.getLogger(__name__)


class TerraformStateReader:
    """Handler for reading and processing Terraform state files"""

    def __init__(self, session: boto3.Session):
        self.session = session
        self.console = Console()
        self._account_id = None

    @property
    def account_id(self):
        if not self._account_id:
            self._account_id = self.session.client("sts").get_caller_identity()[
                "Account"
            ]
        return self._account_id

    def read_backend_config(self, tf_dir: str, progress=None) -> List[Dict[str, Any]]:
        """Reads backend configuration from Terraform files"""
        tf_dir_path = Path(tf_dir)
        backend_config = self._find_s3_backend(tf_dir_path)
        return [{"s3": backend_config}] if backend_config else []

    def _extract_resources_from_state(
        self, state_data: Dict[str, Any], managed_resources: Dict[str, Dict[str, Any]]
    ) -> None:
        """
        Extract resource information from state data
        Args:
            state_data: Terraform state data
            managed_resources: Dictionary to store managed resource information
        """
        try:
            if "resources" not in state_data:
                return

            for resource in state_data["resources"]:
                # Skip data sources and non-managed mode resources
                if resource.get("mode") != "managed":
                    continue
                
                # Check if resource is part of a module
                module_path = resource.get("module", "")
                resource_type = resource.get("type", "")
                for instance in resource.get("instances", []):
                    try:
                        attributes = instance.get("attributes", {})
                        identifier = None
                        resource_info = None
            
                        if resource_type == "aws_iam_role_policy_attachment":
                            role_name = attributes.get("role")
                            policy_arn = attributes.get("policy_arn")
                            if role_name and policy_arn:
                                identifier = f"arn:aws:iam::{self.account_id}:role/{role_name}/{policy_arn}"
                                resource_info = {
                                    "id": identifier,
                                    "type": resource_type,
                                    "role_name": role_name,
                                    "policy_arn": policy_arn,
                                    "managed": True
                                }
                        elif resource_type == "aws_iam_user_policy":
                            user_name = attributes.get("user")
                            policy_name = attributes.get("name")
                            if user_name and policy_name:
                                identifier = f"{user_name}:{policy_name}"
                                resource_info = {
                                    "id": attributes.get('id', ''),
                                    "type": resource_type,
                                    "user_name": user_name,
                                    "managed": True
                                }
                        elif resource_type == "aws_iam_user_policy_attachment":
                            user_name = attributes.get("user")
                            policy_arn = attributes.get("policy_arn")
                            if user_name and policy_arn:
                                identifier = f"{user_name}:{policy_arn}"
                                resource_info = {
                                    "id": identifier,
                                    "type": resource_type,
                                    "user_name": user_name,
                                    "policy_arn": policy_arn,
                                    "managed": True
                                }
                        else:
                            formatted_resource = self._format_resource(
                                resource_type,
                                attributes,
                                instance.get("index_key"),
                            )
                            if formatted_resource:
                                identifier = self._get_identifier_for_managed_set(
                                    formatted_resource
                                )
                                if identifier:
                                    resource_info = formatted_resource
                                    resource_info["managed"] = True

                        if identifier and resource_info:
                            if identifier not in managed_resources:
                                managed_resources[identifier] = resource_info
                            else:
                                logger.debug(f"Skipping duplicate resource: {identifier}")

                    except Exception as e:
                        logger.error(f"Error processing instance in resource {resource_type}: {str(e)}")
                        continue

        except Exception as e:
            logger.error(f"Error extracting resources from state: {str(e)}")
            logger.debug(traceback.format_exc())

    def _get_identifier_for_managed_set(
        self, resource: Dict[str, Any]
    ) -> Optional[str]:
        """
        Get the appropriate identifier for the managed_resources set
        Args:
            resource: Formatted resource dictionary
        Returns:
            String identifier for the managed_resources set
        """
        # Use ARN if available
        if "arn" in resource:
            return resource["arn"]

        resource_type = resource.get("type")
        resource_id = resource.get("id")

        # Special handling for IAM resources
        if resource_type and resource_type.startswith("aws_iam_"):
            if resource_type == "aws_iam_role_policy_attachment":
                role_name = resource.get("role")
                policy_arn = resource.get("policy_arn")
                if role_name and policy_arn:
                    return f"arn:aws:iam::{self.account_id}:role/{role_name}/{policy_arn}"
            elif resource_type == "aws_iam_user_policy":
                user_name = resource.get("user")
                policy_name = resource.get("name")
                if user_name and policy_name:
                    return f"{user_name}:{policy_name}"
            elif resource_type == "aws_iam_user_policy_attachment":
                user_name = resource.get("user")
                policy_arn = resource.get("policy_arn")
                if user_name and policy_arn:
                    return f"{user_name}:{policy_arn}"

        # Basic identifier generation
        if resource_type and resource_id:
            return f"{resource_type}:{resource_id}"

        return resource.get("id")

    def _format_resource(
        self, resource_type: str, attributes: Dict[str, Any], index_key: Any = None
    ) -> Optional[Dict[str, Any]]:
        """Format a single resource into our expected structure"""
        try:
            resource_id = self._get_resource_id(resource_type, attributes, index_key)
            if not resource_id:
                return None

            formatted: Dict[str, Any] = {
                "id": resource_id,
                "type": resource_type,
                "tags": self._extract_tags(attributes),
                "details": {},
            }

            # Add ARN if available
            if "arn" in attributes:
                formatted["arn"] = attributes["arn"]
            elif resource_type.startswith("aws_iam_"):
                formatted["arn"] = (
                    f"arn:aws:iam::{self.account_id}:{resource_type.replace('aws_', '')}/{resource_id}"
                )

            # Add resource-specific details
            if resource_type == "aws_vpc":
                formatted["details"] = {
                    "cidr_block": attributes.get("cidr_block"),
                    "instance_tenancy": attributes.get("instance_tenancy", "default"),
                    "enable_dns_support": attributes.get("enable_dns_support", True),
                    "enable_dns_hostnames": attributes.get("enable_dns_hostnames", False),
                    "is_default": attributes.get("is_default", False),
                    "cidr_block_associations": attributes.get("cidr_block_associations", []),
                    "ipv6_cidr_block": attributes.get("ipv6_cidr_block"),
                    "ipv6_association_id": attributes.get("ipv6_association_id"),
                    "dhcp_options_id": attributes.get("dhcp_options_id"),
                    "enable_network_address_usage_metrics": attributes.get("enable_network_address_usage_metrics", False)
                }
            elif resource_type == "aws_subnet":
                formatted["details"] = {
                    "vpc_id": attributes.get("vpc_id"),
                    "cidr_block": attributes.get("cidr_block"),
                    "availability_zone": attributes.get("availability_zone"),
                    "map_public_ip_on_launch": attributes.get("map_public_ip_on_launch", False),
                    "assign_ipv6_address_on_creation": attributes.get("assign_ipv6_address_on_creation", False),
                    "ipv6_cidr_block": attributes.get("ipv6_cidr_block"),
                    "enable_dns64": attributes.get("enable_dns64", False),
                    "enable_resource_name_dns_aaaa_record_on_launch": attributes.get("enable_resource_name_dns_aaaa_record_on_launch", False),
                    "enable_resource_name_dns_a_record_on_launch": attributes.get("enable_resource_name_dns_a_record_on_launch", False),
                    "private_dns_hostname_type_on_launch": attributes.get("private_dns_hostname_type_on_launch", "ip-name")
                }
            elif resource_type == "aws_iam_role":
                formatted["details"] = {
                    "path": attributes.get("path", "/"),
                    "assume_role_policy": json.loads(
                        attributes.get("assume_role_policy", "{}")
                    ),
                    "description": attributes.get("description", ""),
                    "max_session_duration": attributes.get("max_session_duration"),
                    "permissions_boundary": attributes.get("permissions_boundary"),
                }
            elif resource_type == "aws_iam_role_policy_attachment":
                formatted["details"] = {
                    "role": attributes.get("role"),
                    "policy_arn": attributes.get("policy_arn"),
                }

            # Add all attributes to details (don't overwrite existing values)
            for key, value in attributes.items():
                if key not in ["id", "arn", "tags"] and key not in formatted["details"]:
                    formatted["details"][key] = value

            return formatted

        except Exception as e:
            logger.error(f"Error formatting resource {resource_type}: {str(e)}")
            return None

    def _get_resource_id(
        self, resource_type: str, attributes: Dict[str, Any], index_key: Any = None
    ) -> Optional[str]:
        """Get the appropriate identifier for a resource"""
        if "id" in attributes:
            return attributes["id"]
        elif "name" in attributes:
            return attributes["name"]
        return None

    def _extract_tags(self, attributes: Dict[str, Any]) -> List[Dict[str, str]]:
        """Extract tags from attributes in a consistent format"""
        tags: List[Dict[str, str]] = []
        if "tags" in attributes:
            if isinstance(attributes["tags"], dict):
                tags.extend(
                    {"Key": k, "Value": str(v)} for k, v in attributes["tags"].items()
                )
            elif isinstance(attributes["tags"], list):
                tags.extend(attributes["tags"])
        return tags

    def _find_s3_backend(self, tf_dir: Path) -> Optional[Dict[str, str]]:
        """Find S3 backend configuration in Terraform files"""
        try:
            # Check all .tf files in the directory
            for tf_file in tf_dir.glob("*.tf"):
                try:
                    with open(tf_file) as f:
                        content = hcl2.load(f)
                        if "terraform" not in content:
                            continue

                        for terraform_block in content["terraform"]:
                            if "backend" not in terraform_block:
                                continue

                            backend = terraform_block["backend"]
                            if not isinstance(backend, list):
                                continue

                            for backend_config in backend:
                                if "s3" in backend_config:
                                    logger.debug(f"Found S3 backend configuration in {tf_file}")
                                    return backend_config["s3"]

                except Exception as e:
                    logger.warning(f"Error reading {tf_file}: {str(e)}")
                    continue

            logger.debug("No S3 backend configuration found")
            return None

        except Exception as e:
            logger.error(f"Error searching for S3 backend: {str(e)}")
            logger.debug(traceback.format_exc())
            return None

    def _get_s3_state(
        self, bucket: str, key: str, region: str
    ) -> Optional[Dict[str, Any]]:
        """Read Terraform state file from S3"""
        try:
            s3_client = self.session.client("s3", region_name=region)
            
            # Check if object exists
            try:
                head = s3_client.head_object(Bucket=bucket, Key=key)
            except s3_client.exceptions.ClientError as e:
                if e.response['Error']['Code'] == '404':
                    logger.warning(f"State file does not exist in S3: s3://{bucket}/{key}")
                    return None
                raise

            # Check file size
            content_length = int(head['ContentLength'])
            size_mb = content_length / (1024 * 1024)
            if size_mb > 100:
                logger.warning(f"S3 state file is very large ({size_mb:.2f}MB): s3://{bucket}/{key}")

            # Get the object
            response = s3_client.get_object(Bucket=bucket, Key=key)
            
            try:
                # S3のレスポンスボディを取得
                body = response["Body"]
                # readメソッドを呼び出してデータを取得
                content = body.read()
                # バイト列の場合はデコード
                if isinstance(content, (bytes, bytearray)):
                    content = content.decode("utf-8")
                elif isinstance(content, str):
                    pass
                else:
                    # その他の場合（モックなど）は文字列として扱う
                    content = str(content)
                # JSONとしてパース
                state_data = json.loads(content)
                if not isinstance(state_data, dict):
                    logger.warning(f"Invalid state file format (not a dictionary): s3://{bucket}/{key}")
                    return None
                
                # Basic validation of state file structure
                if "version" not in state_data:
                    logger.warning(f"State file missing version field: s3://{bucket}/{key}")
                    return None

                logger.debug(f"Successfully read state file from S3: s3://{bucket}/{key}")
                return state_data
                
            except json.JSONDecodeError as e:
                logger.error(f"Invalid JSON in S3 state file s3://{bucket}/{key}: {str(e)}")
                return None

        except Exception as e:
            logger.error(f"Error reading state file from S3 s3://{bucket}/{key}: {str(e)}")
            logger.debug(traceback.format_exc())
            return None

    def _read_local_state(self, state_file: Path) -> Optional[Dict[str, Any]]:
        """Read a local Terraform state file"""
        try:
            if not state_file.exists():
                logger.warning(f"State file does not exist: {state_file}")
                return None

            if state_file.stat().st_size == 0:
                logger.warning(f"State file is empty: {state_file}")
                return None

            # Check if file is too large (> 100MB)
            if state_file.stat().st_size > 100 * 1024 * 1024:
                logger.warning(f"State file is very large ({state_file.stat().st_size / 1024 / 1024:.2f}MB): {state_file}")

            with open(state_file) as f:
                try:
                    state_data = json.load(f)
                    if not isinstance(state_data, dict):
                        logger.warning(f"Invalid state file format (not a dictionary): {state_file}")
                        return None
                    
                    # Basic validation of state file structure
                    if "version" not in state_data:
                        logger.warning(f"State file missing version field: {state_file}")
                        return None

                    logger.debug(f"Successfully read state file: {state_file}")
                    return state_data
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON in state file {state_file}: {str(e)}")
                    return None

        except Exception as e:
            logger.error(f"Error reading state file {state_file}: {str(e)}")
            logger.debug(traceback.format_exc())
            return None

    # terraform_aws_migrator/state_reader.py

    def get_managed_resources(
        self, tf_dir: str, progress=None
    ) -> Dict[str, Dict[str, Any]]:
        """
        Get all resources managed by Terraform from state files with their complete information

        Args:
            tf_dir: Directory containing Terraform files
            progress: Optional progress callback

        Returns:
            Dictionary of managed resources with their complete information
            Format:
            {
                "resource_identifier": {
                    "id": "example_id",
                    "type": "aws_iam_role",
                    "arn": "arn:aws:iam::...",
                    "tags": [...],
                    "details": {...},
                    "managed": True
                },
                ...
            }
        """
        managed_resources: Dict[str, Dict[str, Any]] = {}
        tf_dir_path = Path(tf_dir)
        processed_states = set()  # Track processed state files to avoid duplicates

        try:
            # First check S3 backend
            s3_config = self._find_s3_backend(tf_dir_path)
            if s3_config:
                state_data = self._get_s3_state(
                    bucket=s3_config["bucket"],
                    key=s3_config["key"],
                    region=s3_config.get("region", self.session.region_name),
                )
                if state_data:
                    try:
                        self._extract_resources_from_state(state_data, managed_resources)
                        processed_states.add(s3_config["key"])
                    except Exception as e:
                        logger.error(f"Error processing S3 state: {str(e)}")
                        logger.debug(traceback.format_exc())

            # Then check local state files
            state_files = list(Path(tf_dir).rglob("*.tfstate"))
            for state_file in state_files:
                if str(state_file) in processed_states:
                    logger.debug(f"Skipping already processed state file: {state_file}")
                    continue

                state_data = self._read_local_state(state_file)
                if state_data:
                    try:
                        self._extract_resources_from_state(state_data, managed_resources)
                        processed_states.add(str(state_file))
                    except Exception as e:
                        logger.error(f"Error processing local state {state_file}: {str(e)}")
                        logger.debug(traceback.format_exc())

            logger.info(f"Processed {len(processed_states)} state files")
            logger.info(f"Found {len(managed_resources)} managed resources")
            return managed_resources

        except Exception as e:
            logger.error(f"Error reading Terraform state: {str(e)}")
            logger.debug(traceback.format_exc())
            return {}

    def get_s3_state_file(
        self, bucket: str, key: str, region: str, progress=None
    ) -> Dict[str, Any]:
        """Read Terraform state file from S3 (for backward compatibility)"""
        return self._get_s3_state(bucket, key, region) or {}
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collection_status.py</source>
<document_content># terraform_aws_migrator/collection_status.py

from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime


@dataclass
class CollectionStatus:
    """Track the status of resource collection"""

    service: str
    status: str
    start_time: datetime
    end_time: datetime = None

    @property
    def duration(self) -> str:
        """Format duration as [MM:SS]"""
        if not self.end_time:
            duration = datetime.now() - self.start_time
        else:
            duration = self.end_time - self.start_time

        total_seconds = int(duration.total_seconds())
        minutes = total_seconds // 60
        seconds = total_seconds % 60
        return f"[{minutes:02d}:{seconds:02d}]"


class StatusTracker:
    """Track collection status across multiple services"""

    def __init__(self):
        self.statuses: Dict[str, CollectionStatus] = {}

    def start_collection(self, service: str):
        """Record the start of collection for a service"""
        self.statuses[service] = CollectionStatus(
            service=service, status="Processing", start_time=datetime.now()
        )

    def complete_collection(self, service: str, success: bool = True):
        """Record the completion of collection for a service"""
        if service in self.statuses:
            status = self.statuses[service]
            status.status = "Completed" if success else "Failed"
            status.end_time = datetime.now()

    def get_progress_data(self) -> List[Dict[str, Any]]:
        """Get formatted progress data for all services"""
        progress_data = []
        for status in self.statuses.values():
            progress_data.append(
                {
                    "service": status.service,
                    "status": status.status,
                    "time": status.duration,
                }
            )
        return sorted(
            progress_data, key=lambda x: (x["status"] != "Processing", x["service"])
        )


# Ensure the class is properly exported
__all__ = ["StatusTracker", "CollectionStatus"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/__init__.py</source>
<document_content># terraform_aws_migrator/__init__.py
"""Terraform AWS Migrator パッケージ"""
</document_content>
</document>

<document>
<source>terraform_aws_migrator/exclusion.py</source>
<document_content>import fnmatch
import re
from pathlib import Path
from typing import List, Pattern, Optional
import logging

logger = logging.getLogger(__name__)

class ResourceExclusionConfig:
    """Handles the parsing and matching of resource exclusion patterns"""

    DEFAULT_FILENAME = ".tfignore"

    def __init__(self, exclusion_file: Optional[str] = None):
        self.exclusion_file = exclusion_file or self.DEFAULT_FILENAME
        self.patterns: List[str] = []
        self.regex_patterns: List[Pattern] = []
        self._load_patterns()

    def _load_patterns(self) -> None:
        """Load exclusion patterns from the configuration file"""
        try:
            config_path = Path(self.exclusion_file)
            if not config_path.exists():
                logger.debug(f"No exclusion file found at {self.exclusion_file}")
                return

            with open(config_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        line = line.split("#")[0].strip()
                        if line:
                            self.patterns.append(line)
                            # Convert pattern to regex, handling service prefixes
                            pattern = self._convert_pattern_to_regex(line)
                            self.regex_patterns.append(re.compile(pattern))

            logger.info(f"Loaded {len(self.patterns)} exclusion patterns from {self.exclusion_file}")

        except Exception as e:
            logger.error(f"Error loading exclusion patterns: {str(e)}")
            self.patterns = []
            self.regex_patterns = []

    def _convert_pattern_to_regex(self, pattern: str) -> str:
        """Convert an exclusion pattern to regex, handling both AWS resource types and service prefixes"""
        if ":" in pattern:
            type_or_service, identifier = pattern.split(":", 1)
            # Convert glob pattern to regex
            identifier_pattern = fnmatch.translate(identifier)

            # Handle both aws_ prefixed and non-prefixed patterns
            if type_or_service.startswith("aws_"):
                # For exact AWS resource type matching
                return f"{type_or_service}:{identifier_pattern[:-2]}"
            else:
                # For service prefix matching (e.g., iam:)
                aws_prefix = f"aws_{type_or_service}"
                return f"(({type_or_service}:|{aws_prefix}.*:){identifier_pattern[:-2]})"
        else:
            # If no service prefix, just convert glob pattern
            return fnmatch.translate(pattern)[:-2]  # Remove \Z$ from fnmatch.translate()


    def should_exclude(self, resource: dict) -> bool:
            """
            Check if a resource should be excluded based on the patterns

            Args:
                resource (dict): Resource dictionary containing 'id', 'arn', 'type', 'tags', etc.

            Returns:
                bool: True if the resource should be excluded, False otherwise
            """
            if not self.patterns:
                return False

            # Values to check against patterns
            check_values = set()

            # Add basic identifiers
            resource_id = resource.get("id")
            if resource_id:
                check_values.add(resource_id)

            # Add ARN
            arn = resource.get("arn")
            if arn:
                check_values.add(arn)

            # Add type:id format and terraform resource name
            resource_type = resource.get("type")
            if resource_type and resource_id:
                # Remove 'aws_' prefix if present
                service_name = resource_type.replace("aws_", "", 1).split("_")[0]
                check_values.add(f"{service_name}:{resource_id}")
                # Also add the full type:id format
                check_values.add(f"{resource_type}:{resource_id}")
                
                # Add terraform resource name for EC2 instances
                if resource_type == "aws_instance":
                    name_tag_value = None
                    tags = resource.get("tags", [])
                    if isinstance(tags, list):
                        for tag in tags:
                            if isinstance(tag, dict) and tag.get("Key") == "Name":
                                name_tag_value = tag.get("Value")
                                break
                    elif isinstance(tags, dict):
                        name_tag_value = tags.get("Name")
                    
                    if name_tag_value:
                        # Generate resource name in the same way as EC2InstanceGenerator
                        base_name = name_tag_value.replace("-", "_").replace(" ", "_")
                        short_id = resource_id[-4:] if resource_id else ""
                        terraform_resource_name = f"{base_name}_{short_id}"
                        check_values.add(terraform_resource_name)
                        check_values.add(f"{resource_type}:{terraform_resource_name}")

            # Add values from Name tag if present
            tags = resource.get("tags", [])
            name_tag_value = None
            if isinstance(tags, list):
                for tag in tags:
                    if isinstance(tag, dict) and tag.get("Key") == "Name":
                        name_tag_value = tag.get("Value")
                        break
            elif isinstance(tags, dict):
                name_tag_value = tags.get("Name")

            if name_tag_value:
                # Add name tag value directly
                check_values.add(name_tag_value)
                # Add service:name-tag format
                if resource_type:
                    service_name = resource_type.replace("aws_", "", 1).split("_")[0]
                    check_values.add(f"{service_name}:{name_tag_value}")
                    check_values.add(f"{resource_type}:{name_tag_value}")

            # Check each value against all patterns
            for value in check_values:
                for pattern in self.regex_patterns:
                    if pattern.search(str(value)):
                        logger.debug(f"Resource {value} excluded by pattern {pattern.pattern} (values checked: {check_values})")
                        return True

            return False

    def get_patterns(self) -> List[str]:
        """Return the current list of exclusion patterns"""
        return self.patterns.copy()
</document_content>
</document>

<document>
<source>terraform_aws_migrator/auditor.py</source>
<document_content>import time
from typing import Dict, List, Set, Any, Optional, Union
import boto3
import traceback
import copy
from rich.console import Console
from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    ProgressColumn,
    Task,
)
from rich.text import Text

from terraform_aws_migrator.collectors.base import registry
from terraform_aws_migrator.state_reader import TerraformStateReader
from terraform_aws_migrator.exclusion import ResourceExclusionConfig

import logging

logger = logging.getLogger(__name__)


class CompactTimeColumn(ProgressColumn):
    """Custom time column that displays elapsed time in a compact format"""

    def __init__(self):
        super().__init__()
        self.start_time = time.time()

    def render(self, task: "Task") -> Text:
        """Render the time column."""
        elapsed = int(time.time() - self.start_time)
        minutes = elapsed // 60
        seconds = elapsed % 60
        return Text(f"[{minutes:02d}:{seconds:02d}]")


class AWSResourceAuditor:
    """Main class for detecting unmanaged AWS resources"""

    def __init__(
        self,
        exclusion_file: Optional[str] = None,
        target_resource_type: Optional[str] = None,
    ):
        self.session = boto3.Session()
        self.state_reader = TerraformStateReader(self.session)
        self.console = Console()
        self.start_time: Optional[float] = None
        self.exclusion_config = ResourceExclusionConfig(exclusion_file)
        self.target_resource_type = target_resource_type
        self.resource_type_mappings: Dict[str, str] = {}

    def get_terraform_managed_resources(
        self, tf_dir: str, progress=None
    ) -> Dict[str, Dict[str, Any]]:
        """Get dictionary of resource identifiers managed by Terraform"""
        try:
            managed_resources = self.state_reader.get_managed_resources(
                tf_dir, progress
            )
            self.console.print(
                f"[cyan]Found {len(managed_resources)} managed resources in Terraform state"
            )
            return managed_resources
        except Exception as e:
            self.console.print(f"[red]Error reading Terraform state: {str(e)}")
            return {}

    def _get_relevant_collectors(self) -> List[Any]:
        """Get collectors based on target_resource_type"""
        collectors = registry.get_collectors(self.session)

        if not self.target_resource_type:
            logger.debug(f"Getting all collectors: {len(collectors)}")
            return collectors

        matching_collectors = []
        for collector in collectors:
            resource_types = collector.get_resource_types()
            service_name = collector.get_service_name()

            # apply category filter
            if not self.target_resource_type.startswith("aws_"):
                if self.target_resource_type == service_name:
                    matching_collectors.append(collector)
            # apply resource type filter
            elif self.target_resource_type in resource_types:
                matching_collectors.append(collector)

        if not matching_collectors:
            logger.error(
                f"No collector found supporting resource type or service: {self.target_resource_type}"
            )
            return []

        # Add resource type mappings from all matching collectors
        for collector in matching_collectors:
            self.resource_type_mappings.update(collector.get_resource_types())

        return matching_collectors

    def audit_resources(
        self, tf_dir: str
    ) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
        """Detect AWS resources that are not managed by Terraform, optionally filtered by type"""
        self.start_time = time.time()
        result: Dict[str, Dict[str, List[Dict[str, Any]]]] = {"all_resources": {}}
        managed_resources = {}
        processed_identifiers = set()

        def get_elapsed_time() -> str:
            if self.start_time is None:
                return "[00:00]"
            elapsed = int(time.time() - self.start_time)
            minutes = elapsed // 60
            seconds = elapsed % 60
            return f"[{minutes:02d}:{seconds:02d}]"

        progress = Progress(
            SpinnerColumn(),
            TextColumn("{task.description}", style="bold blue"),
            CompactTimeColumn(),
            console=self.console,
            expand=False,
            refresh_per_second=10,
        )

        with progress:
            # Get Terraform managed resources
            tf_task = progress.add_task(
                "[yellow]Reading Terraform state...", total=None
            )
            managed_resources = self.get_terraform_managed_resources(tf_dir, progress)
            progress.update(tf_task, completed=True)

            # Initialize collectors once
            collectors = self._get_relevant_collectors()
            if not collectors:
                if self.target_resource_type:
                    self.console.print(
                        f"[red]No collectors found for resource type: {self.target_resource_type}"
                    )
                return result

            # Add main AWS resource collection task
            aws_task = progress.add_task(
                "[cyan]Collecting AWS resources...", total=None
            )

            # Process each collector
            for collector in collectors:
                service_name = collector.get_service_name()
                try:
                    # Update progress description
                    progress.update(
                        aws_task,
                        description=f"[cyan]Collecting {service_name} resources...",
                    )

                    # Collect resources, passing target_resource_type if specified
                    resources = collector.collect(
                        target_resource_type=self.target_resource_type
                    )

                    # Create managed resources lookup using standardized identifiers
                    managed_lookup: Dict[str, Dict[str, Any]] = {}
                    for managed_resource in managed_resources.values():
                        # Get resource type (remove module name)
                        resource_type = managed_resource.get("type", "")
                        if "." in resource_type:
                            resource_type = resource_type.split(".")[-1]
                        logger.debug(
                            f"Processing managed resource type: {resource_type} for service: {service_name}"
                        )

                        # Check service name and resource type association
                        resource_service = collector.get_service_for_resource_type(
                            resource_type
                        )

                        # If service name is unknown, infer from resource type
                        if not resource_service and resource_type.startswith("aws_"):
                            inferred_service = resource_type[4:].split("_")[0]
                            if inferred_service in [
                                "iam",
                                "s3",
                                "ec2",
                                "lambda",
                                "rds",
                                "dynamodb",
                            ]:
                                resource_service = inferred_service

                        # Skip if service name doesn't match
                        if resource_service and resource_service != service_name:
                            logger.debug(
                                f"Resource type {resource_type} belongs to service {resource_service}, not {service_name}"
                            )
                            continue

                        logger.debug(
                            f"Resource type {resource_type} matches service {service_name}"
                        )

                        # Use resource type without module name
                        base_type = (
                            resource_type.split(".")[-1]
                            if "." in resource_type
                            else resource_type
                        )

                        # Generate identifiers (multiple formats)
                        identifiers = []

                        # ARN-based identifier
                        if "arn" in managed_resource:
                            identifiers.append(managed_resource["arn"])

                        # Resource type and ID based identifier
                        if base_type and "id" in managed_resource:
                            identifiers.append(f"{base_type}:{managed_resource['id']}")

                        # Name-based identifier (from tags)
                        if "tags" in managed_resource:
                            tags = managed_resource["tags"]
                            if isinstance(tags, list):
                                for tag in tags:
                                    if (
                                        isinstance(tag, dict)
                                        and tag.get("Key") == "Name"
                                    ):
                                        identifiers.append(
                                            f"{base_type}:{tag['Value']}"
                                        )
                                        break
                            elif isinstance(tags, dict) and "Name" in tags:
                                identifiers.append(f"{base_type}:{tags['Name']}")

                        # Custom identifier
                        managed_copy = managed_resource.copy()
                        managed_copy["type"] = base_type
                        custom_identifier = collector.generate_resource_identifier(
                            managed_copy
                        )
                        if custom_identifier:
                            identifiers.append(custom_identifier)

                        # Remove duplicates and add as managed resource
                        if identifiers:
                            managed_copy = copy.deepcopy(managed_resource)
                            managed_copy["managed"] = True
                            for identifier in set(identifiers):
                                managed_lookup[identifier] = managed_copy
                                logger.debug(
                                    f"Added managed resource to lookup: {identifier} (type: {resource_type})"
                                )

                    # Process collected resources
                    processed_resources = []
                    for resource in resources:
                        resource_type = resource.get("type")
                        if not resource_type or not self._matches_target_type(
                            resource_type
                        ):
                            continue

                        # Generate standard identifier for comparison
                        resource_identifier = collector.generate_resource_identifier(
                            resource
                        )

                        if not resource_identifier:
                            logger.warning(
                                f"Could not generate identifier for resource: {resource}"
                            )
                            continue

                        # check for duplicate resources
                        if resource_identifier in processed_identifiers:
                            logger.debug(
                                f"Skipping duplicate resource: {resource_identifier}"
                            )
                            continue

                        # Skip excluded resources
                        if self.exclusion_config.should_exclude(resource):
                            logger.debug(f"Excluding resource: {resource_identifier}")
                            continue

                        # Check resource management status
                        found_managed_resource: Optional[Dict[str, Any]] = None

                        # Check different identifiers in sequence
                        identifiers_to_check = [resource_identifier]
                        if "arn" in resource:
                            identifiers_to_check.append(resource["arn"])
                        if resource_type and "id" in resource:
                            identifiers_to_check.append(
                                f"{resource_type}:{resource['id']}"
                            )

                        # Check each identifier
                        for identifier in identifiers_to_check:
                            if identifier in managed_lookup:
                                found_managed_resource = managed_lookup[identifier]
                                logger.debug(
                                    f"Found managed resource with identifier: {identifier}"
                                )
                                break
                            logger.debug(
                                f"No managed resource found for identifier: {identifier}"
                            )

                        # Create resource copy
                        resource_copy = copy.deepcopy(resource)
                        resource_copy["identifier"] = resource_identifier
                        if found_managed_resource is not None:
                            # Use managed resource but keep details from collector
                            resource_copy.update(found_managed_resource)
                            resource_copy["details"] = resource.get("details", {})
                            resource_copy["managed"] = True
                            logger.debug(
                                f"Resource {resource_identifier} marked as managed"
                            )
                        else:
                            resource_copy["managed"] = False
                            logger.debug(
                                f"Resource {resource_identifier} marked as unmanaged"
                            )

                        processed_resources.append(resource_copy)
                        processed_identifiers.add(resource_identifier)

                        logger.debug(
                            f"Processed resource {resource_identifier} (managed: {bool(found_managed_resource)})"
                        )

                    if processed_resources:
                        type_groups: Dict[str, List[Dict[str, Any]]] = {}
                        for resource in processed_resources:
                            resource_type = resource.get("type", "unknown")
                            if resource_type not in type_groups:
                                type_groups[resource_type] = []
                            type_groups[resource_type].append(resource)

                        for resource_type, resources_list in type_groups.items():
                            display_name = collector.get_type_display_name(
                                resource_type
                            )
                            managed_count = sum(
                                1 for r in resources_list if r.get("managed", False)
                            )
                            unmanaged_count = len(resources_list) - managed_count
                            if unmanaged_count > 0:
                                self.console.print(
                                    f"[green]Found {unmanaged_count} unmanaged {display_name} {get_elapsed_time()}"
                                )

                        # Add to result['all_resources']
                        if service_name not in result["all_resources"]:
                            result["all_resources"][service_name] = []
                        result["all_resources"][service_name].extend(
                            processed_resources
                        )

                except Exception as e:
                    self.console.print(
                        f"[red]Error collecting {service_name} resources: {str(e)}"
                    )

            # Complete the collection task
            progress.update(aws_task, completed=True)

        # Display total execution time
        if self.start_time is not None:
            total_time = int(time.time() - self.start_time)
            minutes = total_time // 60
            seconds = total_time % 60
            self.console.print(
                f"\n[green]Total execution time: [{minutes:02d}:{seconds:02d}]"
            )

        return result

    def _matches_target_type(self, resource_type: Optional[str]) -> bool:
        """Check if resource type matches target type filter"""
        if not self.target_resource_type:
            return True

        if not resource_type:
            return False

        if self.target_resource_type.startswith("aws_"):
            return resource_type == self.target_resource_type

        return resource_type.startswith(f"aws_{self.target_resource_type}_")

    def _process_s3_resource(
        self, resource: Dict[str, Any], managed_resources: Dict[str, Any]
    ) -> bool:
        """Check if S3 resource is managed"""
        resource_type = resource.get("type")
        if resource_type not in ["aws_s3_bucket_policy", "aws_s3_bucket_acl"]:
            return False

        for state_resource in managed_resources.values():
            if state_resource.get("type") == resource_type and state_resource.get(
                "id"
            ) == resource.get("id"):
                return True
        return False
</document_content>
</document>

<document>
<source>terraform_aws_migrator/main.py</source>
<document_content># terraform_aws_migrator/main.py

import argparse
import logging
import traceback
from rich.console import Console
from terraform_aws_migrator.utils.resource_utils import show_supported_resources
from terraform_aws_migrator.auditor import AWSResourceAuditor
from terraform_aws_migrator.formatters.output_formatter import format_output
from terraform_aws_migrator.generators import HCLGeneratorRegistry
from terraform_aws_migrator.collectors.base import registry as collector_registry
import boto3  # type: ignore

logger = logging.getLogger(__name__)


def setup_logging(debug: bool = False):
    """Configure logging settings"""
    level = logging.DEBUG if debug else logging.WARNING

    # Configure basic logging format and root logger level
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        force=True
    )

    # Always suppress boto3/botocore logs
    logging.getLogger("boto3").setLevel(logging.WARNING)
    logging.getLogger("botocore").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)

    # Ensure terraform_aws_migrator logger and its children are set to the correct level
    app_logger = logging.getLogger("terraform_aws_migrator")
    app_logger.setLevel(level)
    
    # Propagate the level to all child loggers
    for name in logging.root.manager.loggerDict:
        if name.startswith("terraform_aws_migrator"):
            logging.getLogger(name).setLevel(level)


class ResourceProcessor:
    """Resource processing and filtering functionality"""

    @staticmethod
    def get_service_name(resource_type: str) -> str:
        """Get service name from resource type using collectors"""
        # Create a session for collector initialization
        session = boto3.Session()
        
        # Get all collectors that handle this resource type
        collectors = collector_registry.get_collectors(session, resource_type)
        
        for collector in collectors:
            # Check resource service mappings first
            service_mappings = collector.get_resource_service_mappings()
            if resource_type in service_mappings:
                return service_mappings[resource_type]
            
            # Check if the collector handles this resource type
            if resource_type in collector.get_resource_types():
                return collector.get_service_name()
        
        # Fallback: Extract service name from resource type
        if resource_type.startswith("aws_"):
            return resource_type.split("_")[1]
        return "other"

    @classmethod
    def process_resources(
        cls, resources_result: dict, resource_type: str, include_managed: bool = False
    ) -> dict:
        """Process and filter resources based on type and management status"""
        target_resources = {}
        service_name = cls.get_service_name(resource_type)
        logger.debug(f"Processing resources for service: {service_name}")

        if service_name in resources_result.get("all_resources", {}):
            logger.debug(
                f"Found {len(resources_result['all_resources'][service_name])} resources in service {service_name}"
            )
            for resource in resources_result["all_resources"][service_name]:
                if resource.get("type") == resource_type:
                    resource_id = resource.get("id")
                    if resource_id:
                        if not include_managed and resource.get("managed", False):
                            logger.debug(
                                f"Skipping managed resource {resource_type}: {resource_id}"
                            )
                            continue

                        target_resources[resource_id] = resource
                        status = (
                            "managed" if resource.get("managed", False) else "unmanaged"
                        )
                        logger.info(f"Found {status} {resource_type}: {resource_id}")
                        logger.debug(f"Resource details: {resource.get('details')}")

        logger.debug(f"Found {len(target_resources)} total resources")
        return target_resources


class HCLGenerator:
    """Handles HCL generation and output"""

    def __init__(self, console: Console, output_file: str | None = None):
        self.console = console
        self.output_file = output_file

    def write_output(self, content: str, header: str = ""):
        """Write content to file or console"""
        if self.output_file:
            with open(self.output_file, "a") as f:
                if header:
                    f.write(f"\n{header}\n")
                f.write(f"{content}\n")
        else:
            if header:
                self.console.print(header)
            self.console.print(content)

    def generate_category_hcl(
        self, generators: dict, resources_result: dict, args
    ) -> bool:
        """Generate HCL for a resource category"""
        all_hcl = []
        all_imports = []

        for resource_type, generator_class in generators.items():
            target_resources = ResourceProcessor.process_resources(
                resources_result, resource_type, args.include_managed
            )

            if not target_resources:
                continue

            generator = generator_class(
                module_prefix=args.module_prefix, state_reader=args.state_reader
            )

            for resource_id, resource in target_resources.items():
                logger.debug(f"Generating HCL for {resource_type} {resource_id}")
                logger.debug(f"Resource details: {resource.get('details', {})}")
                hcl = generator.generate(resource)
                if hcl:
                    all_hcl.append(hcl)
                import_cmd = generator.generate_import(resource)
                if import_cmd:
                    all_imports.append(import_cmd)

        if all_hcl:
            self.write_output("\n\n".join(all_hcl))
        if all_imports:
            self.write_output("\n".join(all_imports), "\n# Import commands")

        return bool(all_hcl or all_imports)

    def generate_resource_hcl(
        self,
        generator,
        target_resources: dict,
        resource_type: str,
        include_default_vpc: bool = False,
    ) -> bool:
        """Generate HCL for a specific resource type"""
        all_hcl = []
        all_imports = []

        for resource_id, resource in target_resources.items():
            logger.debug(f"Generating HCL for {resource_type} {resource_id}")
            logger.debug(f"Resource details: {resource.get('details', {})}")

            # Pass include_default parameter only for VPC resources
            if resource_type == "aws_vpc":
                hcl = generator.generate(resource, include_default=include_default_vpc)
            else:
                hcl = generator.generate(resource)

            if hcl:
                all_hcl.append(hcl)
            import_cmd = generator.generate_import(resource)
            if import_cmd:
                all_imports.append(import_cmd)

        if all_hcl:
            self.write_output("\n\n".join(all_hcl))
        if all_imports:
            self.write_output("\n".join(all_imports), "\n# Import commands")

        return bool(all_hcl or all_imports)


def validate_args(args: argparse.Namespace, console: Console) -> bool:
    """Validate command line arguments"""
    if args.list_resources:
        show_supported_resources()
        return False

    if not args.tf_dir:
        console.print(
            "[red]Error: --tf-dir is required when not using --list-resources[/red]"
        )
        return False

    if args.generate and not args.type:
        console.print("[red]Error: --type is required when using --generate")
        return False

    if args.generate and not HCLGeneratorRegistry.is_supported(args.type):
        console.print(
            f"[yellow]Warning: Resource type or category '{args.type}' is not yet supported for HCL generation"
        )
        return False

    return True


def handle_generation(args: argparse.Namespace, console: Console) -> int:
    """Handle HCL generation mode"""
    auditor = AWSResourceAuditor(
        exclusion_file=args.ignore_file, target_resource_type=args.type
    )
    resources_result = auditor.audit_resources(args.tf_dir)
    args.state_reader = auditor.state_reader
    hcl_generator = HCLGenerator(console, args.output_file)

    if not args.type.startswith("aws_"):
        generators = HCLGeneratorRegistry.get_generators_for_category(args.type)
        if not generators:
            console.print(
                f"[yellow]Warning: No generators found for category '{args.type}'"
            )
            return 1

        if not hcl_generator.generate_category_hcl(generators, resources_result, args):
            logger.warning(f"No HCL generated for category {args.type}")
            return 1
    else:
        target_resources = ResourceProcessor.process_resources(
            resources_result, args.type, args.include_managed
        )
        generator = HCLGeneratorRegistry.get_generator(
            args.type,
            module_prefix=args.module_prefix,
            state_reader=auditor.state_reader,
        )

        console.print(
            f"Generating HCL for {len(target_resources)} {args.type} resources"
        )
        if not hcl_generator.generate_resource_hcl(
            generator, target_resources, args.type, args.include_default_vpc
        ):
            logger.warning(f"No HCL generated for {args.type}")
            return 1

    return 0


def handle_detection(args: argparse.Namespace, console: Console) -> int:
    """Handle normal detection mode"""
    auditor = AWSResourceAuditor(
        exclusion_file=args.ignore_file, target_resource_type=args.type
    )
    resources_result = auditor.audit_resources(args.tf_dir)
    
    # Pass all resources to format_output
    formatted_output = format_output(resources_result.get("all_resources", {}), args.output)

    if args.output_file:
        with open(args.output_file, "w") as f:
            f.write(formatted_output)
        console.print(f"[green]Detection results written to {args.output_file}")
    else:
        console.print(formatted_output)

    return 0


def main():
    parser = argparse.ArgumentParser(
        description="Detect and migrate AWS resources that are not managed by Terraform"
    )
    parser.add_argument(
        "-t", "--tf-dir", type=str, help="Directory containing Terraform files"
    )
    parser.add_argument(
        "--output",
        type=str,
        choices=["text", "json"],
        default="text",
        help="Output format (text or json)",
    )
    parser.add_argument(
        "--output-file",
        type=str,
        help="Output file path (optional, defaults to stdout)",
    )
    parser.add_argument(
        "--list-resources", action="store_true", help="List supported resource types"
    )
    parser.add_argument(
        "-i",
        "--ignore-file",
        type=str,
        help="Path to resource exclusion file (default: .tfignore)",
        metavar="FILE",
    )
    parser.add_argument(
        "--type", type=str, help="Resource type to audit/generate (e.g., aws_iam_role)"
    )
    parser.add_argument(
        "--generate", action="store_true", help="Generate HCL for unmanaged resources"
    )
    parser.add_argument(
        "--module-prefix",
        type=str,
        help="Module prefix for import commands (e.g., 'my_module')",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    parser.add_argument(
        "--include-default-vpc",
        action="store_true",
        help="Include default VPC in the generation (only applies to aws_vpc resources)",
    )
    parser.add_argument(
        "--include-managed",
        action="store_true",
        help="Include resources that are already managed by Terraform in HCL generation",
    )

    args = parser.parse_args()
    setup_logging(args.debug)
    console = Console(stderr=True)

    try:
        if not validate_args(args, console):
            return 1

        return (
            handle_generation(args, console)
            if args.generate
            else handle_detection(args, console)
        )

    except KeyboardInterrupt:
        console.print("\n[yellow]Detection cancelled by user")
        return 1
    except Exception as e:
        console.print(f"[red]Error during detection: {str(e)}")
        console.print(f"[red]Error during detection: {traceback.format_exc()}")
        return 1


if __name__ == "__main__":
    exit(main())
</document_content>
</document>

<document>
<source>terraform_aws_migrator/__main__.py</source>
<document_content>from .main import main

if __name__ == "__main__":
    main()
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_application.py</source>
<document_content># terraform_aws_migrator/collectors/aws_application.py

from typing import Dict, List, Any
from .base import ResourceCollector, register_collector


@register_collector
class StepFunctionCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "stepfunctions"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_sfn_state_machine": "Step Functions State Machines"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            paginator = self.client.get_paginator("list_state_machines")
            for page in paginator.paginate():
                for state_machine in page["stateMachines"]:
                    # Get detailed information about the state machine
                    try:
                        details = self.client.describe_state_machine(
                            stateMachineArn=state_machine["stateMachineArn"]
                        )
                        tags = self.client.list_tags_for_resource(
                            resourceArn=state_machine["stateMachineArn"]
                        ).get("tags", [])
                    except Exception:
                        details = {}
                        tags = []

                    resources.append(
                        {
                            "type": "aws_sfn_state_machine",
                            "id": state_machine["name"],
                            "arn": state_machine["stateMachineArn"],
                            "tags": tags,
                            "details": {
                                "creation_date": str(state_machine.get("creationDate")),
                                "type": details.get("type"),
                                "status": details.get("status"),
                                "revision_id": details.get("revisionId"),
                            },
                        }
                    )
        except Exception as e:
            print(f"Error collecting Step Functions: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/__init__.py</source>
<document_content># terraform_aws_migrator/collectors/__init__.py

import importlib
from pathlib import Path
from .base import ResourceCollector, register_collector


def _import_collectors():
    current_dir = Path(__file__).parent

    for file_path in current_dir.glob("aws_*.py"):
        module_name = f".{file_path.stem}"
        try:
            importlib.import_module(
                module_name, package="terraform_aws_migrator.collectors"
            )
        except ImportError as e:
            print(f"Warning: Failed to import {module_name}: {e}")

    for dir_path in current_dir.glob("aws_*"):
        if not dir_path.is_dir():
            continue

        for file_path in dir_path.glob("*.py"):
            if file_path.name == "__init__.py":
                continue

            relative_path = file_path.relative_to(current_dir)
            module_name = f".{relative_path.parent.name}.{file_path.stem}"
            try:
                importlib.import_module(
                    module_name, package="terraform_aws_migrator.collectors"
                )
            except ImportError as e:
                print(f"Warning: Failed to import {module_name}: {e}")


_import_collectors()

__all__ = ["ResourceCollector", "register_collector"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_database.py</source>
<document_content># resource_collectors/database.py

from typing import Dict, List, Any
from .base import ResourceCollector, register_collector


@register_collector
class RDSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "rds"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_db_instance": "RDS Database Instances",
            "aws_rds_cluster": "RDS Database Clusters"
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # DB instances
            paginator = self.client.get_paginator("describe_db_instances")
            for page in paginator.paginate():
                for instance in page["DBInstances"]:
                    resources.append(
                        {
                            "type": "instance",
                            "id": instance["DBInstanceIdentifier"],
                            "arn": instance["DBInstanceArn"],
                            "engine": instance["Engine"],
                            "tags": self.client.list_tags_for_resource(
                                ResourceName=instance["DBInstanceArn"]
                            )["TagList"],
                        }
                    )

            # DB clusters
            paginator = self.client.get_paginator("describe_db_clusters")
            for page in paginator.paginate():
                for cluster in page["DBClusters"]:
                    resources.append(
                        {
                            "type": "cluster",
                            "id": cluster["DBClusterIdentifier"],
                            "arn": cluster["DBClusterArn"],
                            "engine": cluster["Engine"],
                            "tags": self.client.list_tags_for_resource(
                                ResourceName=cluster["DBClusterArn"]
                            )["TagList"],
                        }
                    )
        except Exception as e:
            print(f"Error collecting RDS resources: {str(e)}")

        return resources


@register_collector
class DynamoDBCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "dynamodb"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_dynamodb_table": "DynamoDB Tables"
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("list_tables")
            for page in paginator.paginate():
                for table_name in page["TableNames"]:
                    table = self.client.describe_table(TableName=table_name)["Table"]
                    tags = self.client.list_tags_of_resource(
                        ResourceArn=f"arn:aws:dynamodb:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:table/{table_name}"
                    ).get("Tags", [])

                    resources.append(
                        {
                            "type": "table",
                            "id": table_name,
                            "arn": table["TableArn"],
                            "tags": tags,
                        }
                    )
        except Exception as e:
            print(f"Error collecting DynamoDB tables: {str(e)}")

        return resources


@register_collector
class ElastiCacheCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "elasticache"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_elasticache_cluster": "ElastiCache Clusters",
            "aws_elasticache_replication_group": "ElastiCache Replication Groups"
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # Cache clusters
            paginator = self.client.get_paginator("describe_cache_clusters")
            for page in paginator.paginate():
                for cluster in page["CacheClusters"]:
                    resources.append(
                        {
                            "type": "cluster",
                            "id": cluster["CacheClusterId"],
                            "arn": f"arn:aws:elasticache:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:cluster:{cluster['CacheClusterId']}",
                            "engine": cluster["Engine"],
                            "tags": self.client.list_tags_for_resource(
                                ResourceName=f"arn:aws:elasticache:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:cluster:{cluster['CacheClusterId']}"
                            )["TagList"],
                        }
                    )

            # Replication groups
            paginator = self.client.get_paginator("describe_replication_groups")
            for page in paginator.paginate():
                for group in page["ReplicationGroups"]:
                    resources.append(
                        {
                            "type": "replication_group",
                            "id": group["ReplicationGroupId"],
                            "arn": f"arn:aws:elasticache:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:replicationgroup:{group['ReplicationGroupId']}",
                            "tags": self.client.list_tags_for_resource(
                                ResourceName=f"arn:aws:elasticache:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:replicationgroup:{group['ReplicationGroupId']}"
                            )["TagList"],
                        }
                    )
        except Exception as e:
            print(f"Error collecting ElastiCache resources: {str(e)}")

        return resources

</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_security.py</source>
<document_content># resource_collectors/security.py

from typing import Dict, List, Any
from .base import ResourceCollector, register_collector


@register_collector
class KMSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "kms"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_kms_key": "KMS Customer-Managed Keys"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("list_keys")
            for page in paginator.paginate():
                for key in page["Keys"]:
                    key_id = key["KeyId"]
                    try:
                        key_info = self.client.describe_key(KeyId=key_id)["KeyMetadata"]
                        if (
                            key_info["KeyManager"] == "CUSTOMER"
                        ):  # Only collect customer-managed keys
                            tags = self.client.list_resource_tags(KeyId=key_id)["Tags"]
                            resources.append(
                                {
                                    "type": "key",
                                    "id": key_id,
                                    "arn": key_info["Arn"],
                                    "tags": tags,
                                }
                            )
                    except self.client.exceptions.NotFoundException:
                        continue
        except Exception as e:
            print(f"Error collecting KMS resources: {str(e)}")

        return resources


@register_collector
class SecretsManagerCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "secretsmanager"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_secretsmanager_secret": "Secrets Manager Secrets"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("list_secrets")
            for page in paginator.paginate():
                for secret in page["SecretList"]:
                    resources.append(
                        {
                            "type": "secret",
                            "id": secret["Name"],
                            "arn": secret["ARN"],
                            "tags": secret.get("Tags", []),
                        }
                    )
        except Exception as e:
            print(f"Error collecting Secrets Manager resources: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_compute.py</source>
<document_content># terraform_aws_migrator/collectors/aws_compute.py

from typing import Dict, List, Any
from .base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class EC2Collector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "ec2"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_instance": "EC2 Instances",
            "aws_security_group": "Security Groups",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            if not target_resource_type or target_resource_type == "aws_instance":
                resources.extend(self._collect_ec2_instances())

            if not target_resource_type or target_resource_type == "aws_security_group":
                resources.extend(self._collect_security_groups())

        except Exception as e:
            logger.error(f"Error collecting EC2 resources: {str(e)}")

        return resources

    def _collect_ec2_instances(self) -> List[Dict[str, Any]]:
        """Collect EC2 instance resources"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_instances")
            for page in paginator.paginate():
                for reservation in page["Reservations"]:
                    for instance in reservation["Instances"]:
                        instance_details = {
                            "type": "aws_instance",
                            "id": instance["InstanceId"],
                            "arn": self.build_arn("instance", instance["InstanceId"]),
                            "tags": instance.get("Tags", []),
                            "details": {
                                "instance_type": instance.get("InstanceType"),
                                "ami": instance.get("ImageId"),
                                "availability_zone": instance.get("Placement", {}).get(
                                    "AvailabilityZone"
                                ),
                                "subnet_id": instance.get("SubnetId"),
                                "vpc_id": instance.get("VpcId"),
                                "key_name": instance.get("KeyName"),
                                "vpc_security_group_ids": [
                                    sg["GroupId"]
                                    for sg in instance.get("SecurityGroups", [])
                                ],
                                "ebs_optimized": instance.get("EbsOptimized", False),
                                "monitoring": instance.get("Monitoring", {}).get(
                                    "State"
                                )
                                == "enabled",
                            },
                        }

                        # Get block device mapping
                        block_devices = []
                        for device in instance.get("BlockDeviceMappings", []):
                            if "Ebs" in device:
                                block_devices.append(
                                    {
                                        "device_name": device["DeviceName"],
                                        "volume_id": device["Ebs"]["VolumeId"],
                                        "delete_on_termination": device["Ebs"].get(
                                            "DeleteOnTermination", True
                                        ),
                                    }
                                )
                        if block_devices:
                            instance_details["details"]["block_devices"] = block_devices

                        # Get IPs
                        if instance.get("PublicIpAddress"):
                            instance_details["details"]["public_ip"] = instance[
                                "PublicIpAddress"
                            ]
                        if instance.get("PrivateIpAddress"):
                            instance_details["details"]["private_ip"] = instance[
                                "PrivateIpAddress"
                            ]

                        resources.append(instance_details)

            logger.debug(f"Found {len(resources)} EC2 instances")
            return resources

        except Exception as e:
            logger.error(f"Error collecting EC2 instances: {str(e)}")
            return []

    def _collect_security_groups(self) -> List[Dict[str, Any]]:
        """Collect security group resources"""
        resources = []
        try:
            for sg in self.client.describe_security_groups()["SecurityGroups"]:
                resources.append(
                    {
                        "type": "aws_security_group",
                        "id": sg["GroupId"],
                        "arn": self.build_arn("security-group", sg["GroupId"]),
                        "tags": sg.get("Tags", []),
                        "details": {
                            "name": sg["GroupName"],
                            "description": sg.get("Description", ""),
                            "vpc_id": sg.get("VpcId"),
                            "revoke_rules_on_delete": False, #default value
                            "ingress_rules": [
                                {
                                    "from_port": rule.get("FromPort"),
                                    "to_port": rule.get("ToPort"),
                                    "protocol": rule.get("IpProtocol"),
                                    "cidr_blocks": [
                                        ip_range["CidrIp"]
                                        for ip_range in rule.get("IpRanges", [])
                                    ],
                                    "ipv6_cidr_blocks": [
                                        ip_range["CidrIpv6"]
                                        for ip_range in rule.get("Ipv6Ranges", [])
                                    ],
                                    "security_groups": [
                                        sg_ref["GroupId"]
                                        for sg_ref in rule.get("UserIdGroupPairs", [])
                                    ],
                                }
                                for rule in sg.get("IpPermissions", [])
                            ],
                            "egress_rules": [
                                {
                                    "from_port": rule.get("FromPort"),
                                    "to_port": rule.get("ToPort"),
                                    "protocol": rule.get("IpProtocol"),
                                    "cidr_blocks": [
                                        ip_range["CidrIp"]
                                        for ip_range in rule.get("IpRanges", [])
                                    ],
                                    "ipv6_cidr_blocks": [
                                        ip_range["CidrIpv6"]
                                        for ip_range in rule.get("Ipv6Ranges", [])
                                    ],
                                    "security_groups": [
                                        sg_ref["GroupId"]
                                        for sg_ref in rule.get("UserIdGroupPairs", [])
                                    ],
                                }
                                for rule in sg.get("IpPermissionsEgress", [])
                            ],
                        },
                    }
                )

            logger.debug(f"Found {len(resources)} security groups")
            return resources

        except Exception as e:
            logger.error(f"Error collecting security groups: {str(e)}")
            return []


@register_collector
class ECSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "ecs"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_ecs_cluster": "ECS Clusters", "aws_ecs_service": "ECS Services"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # Clusters
            cluster_arns = self.client.list_clusters()["clusterArns"]
            if cluster_arns:
                clusters = self.client.describe_clusters(clusters=cluster_arns)[
                    "clusters"
                ]
                for cluster in clusters:
                    resources.append(
                        {
                            "type": "cluster",
                            "id": cluster["clusterName"],
                            "arn": cluster["clusterArn"],
                            "tags": cluster.get("tags", []),
                        }
                    )

                    # Services in each cluster
                    paginator = self.client.get_paginator("list_services")
                    for page in paginator.paginate(cluster=cluster["clusterName"]):
                        service_arns = page["serviceArns"]
                        if service_arns:
                            services = self.client.describe_services(
                                cluster=cluster["clusterName"], services=service_arns
                            )["services"]
                            for service in services:
                                resources.append(
                                    {
                                        "type": "service",
                                        "id": service["serviceName"],
                                        "arn": service["serviceArn"],
                                        "cluster": cluster["clusterName"],
                                        "tags": service.get("tags", []),
                                    }
                                )

        except Exception as e:
            logger.error(f"Error collecting ECS resources: {str(e)}")

        return resources


@register_collector
class LambdaCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "lambda"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_lambda_function": "Lambda Functions"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            paginator = self.client.get_paginator("list_functions")
            for page in paginator.paginate():
                for function in page["Functions"]:
                    # Get function tags
                    try:
                        tags = self.client.list_tags(
                            Resource=function["FunctionArn"]
                        ).get("Tags", {})
                    except Exception:
                        tags = {}

                    details = {
                        "runtime": function.get("Runtime"),
                        "role": function.get("Role"),
                        "handler": function.get("Handler"),
                        "description": function.get("Description"),
                        "memory_size": function.get("MemorySize"),
                        "timeout": function.get("Timeout"),
                        "last_modified": str(function.get("LastModified")),
                        "version": function.get("Version"),
                        "package_type": function.get("PackageType"),
                        "publish": function.get("Publish", False),
                    }

                    if function.get("PackageType") == "Image":
                        try:
                            function_detail = self.client.get_function(FunctionName=function["FunctionName"])
                            code = function_detail.get("Code", {})
                            logger.debug(f"Lambda function code info: {code}")
                            details["image_uri"] = code.get("ImageUri")
                            if image_config := function_detail.get("ImageConfigResponse"):
                                logger.debug(f"Lambda function image config: {image_config}")
                        except Exception as e:
                            logger.error(f"Error getting function details: {str(e)}")
                            details["image_config"] = {
                                "command": image_config.get("ImageConfig", {}).get("Command"),
                                "entry_point": image_config.get("ImageConfig", {}).get("EntryPoint"),
                                "working_directory": image_config.get("ImageConfig", {}).get("WorkingDirectory"),
                            }

                    if env_vars := function.get("Environment", {}).get("Variables"):
                        details["environment"] = {"variables": env_vars}

                    if vpc_config := function.get("VpcConfig"):
                        details["vpc_config"] = {
                            "subnet_ids": vpc_config.get("SubnetIds", []),
                            "security_group_ids": vpc_config.get("SecurityGroupIds", []),
                        }

                    if layers := function.get("Layers"):
                        details["layers"] = [layer.get("Arn") for layer in layers]

                    if dlq := function.get("DeadLetterConfig"):
                        details["dead_letter_config"] = {
                            "target_arn": dlq.get("TargetArn")
                        }

                    if tracing := function.get("TracingConfig"):
                        details["tracing_config"] = {
                            "mode": tracing.get("Mode")
                        }

                    if fs_configs := function.get("FileSystemConfigs", []):
                        details["file_system_config"] = [{
                            "arn": fs.get("Arn"),
                            "local_mount_path": fs.get("LocalMountPath")
                        } for fs in fs_configs]

                    resources.append({
                        "type": "aws_lambda_function",
                        "id": function["FunctionName"],
                        "arn": function["FunctionArn"],
                        "tags": tags,
                        "details": details,
                    })
        except Exception as e:
            print(f"Error collecting Lambda functions: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage.py</source>
<document_content>from .aws_storage.s3 import S3Collector
from .aws_storage.efs import EFSCollector
from .aws_storage.ebs import EBSCollector

__all__ = ["S3Collector", "EFSCollector", "EBSCollector"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/base.py</source>
<document_content>from abc import ABC, abstractmethod
from typing import Dict, List, Any, Callable, Optional
import boto3
import logging

logger = logging.getLogger(__name__)


class ResourceCollector(ABC):
    """Base class for AWS resource collectors"""

    def __init__(
        self,
        session: boto3.Session = None,
        progress_callback: Optional[Callable] = None,
    ):
        self._client = None
        self._account_id = None
        self._region = None
        self.session = session or boto3.Session()
        self.progress_callback = progress_callback

    @abstractmethod
    def get_service_name(self) -> str:
        """Return the AWS service name for this collector"""
        raise NotImplementedError("Subclasses must implement get_service_name")

    @classmethod
    def get_resource_types(cls) -> Dict[str, str]:
        """Return dictionary of resource types supported by this collector"""
        return {}

    @classmethod
    def get_service_for_resource_type(cls, resource_type: str) -> str:
        """
        Return the AWS service name for a resource type

        Args:
            resource_type: AWS resource type (e.g., aws_vpc, aws_subnet)
        Returns:
            Service name (e.g., ec2, s3)
        """
        # Special handling for EC2 service
        ec2_prefixes = [
            "aws_vpc", "aws_subnet", "aws_instance", "aws_ebs_volume",
            "aws_internet_gateway", "aws_nat_gateway", "aws_network_acl",
            "aws_route", "aws_route_table", "aws_vpc_dhcp_options",
            "aws_vpc_endpoint"
        ]
        if any(resource_type.startswith(prefix) for prefix in ec2_prefixes):
            return "ec2"

        # General case: aws_<service>_* or aws_<service>
        if resource_type.startswith("aws_"):
            parts = resource_type[4:].split("_", 1)
            return parts[0]

        return ""

    @classmethod
    def get_type_display_name(cls, resource_type: str) -> str:
        """Get display name for a resource type"""
        resource_types = cls.get_resource_types()
        return resource_types.get(resource_type, resource_type)

    @property
    def client(self):
        if self._client is None:
            self._client = self.session.client(self.get_service_name())
            logger.debug(f"Created client for service: {self.get_service_name()}")
        return self._client

    @property
    def account_id(self):
        if self._account_id is None:
            self._account_id = self.session.client("sts").get_caller_identity()[
                "Account"
            ]
        return self._account_id

    @property
    def region(self):
        """Get current AWS region"""
        if self._region is None:
            self._region = self.session.region_name
        return self._region

    @abstractmethod
    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        """Collect resources for the service"""
        pass

    @staticmethod
    def extract_tags(tags: List[Dict[str, str]]) -> Dict[str, str]:
        """Convert AWS tags list to dictionary"""
        return {tag["Key"]: tag["Value"] for tag in tags} if tags else {}

    @classmethod
    def get_resource_service_mappings(cls) -> Dict[str, str]:
        """Return dictionary of resource type to service name mappings"""
        return {}

    def build_arn(self, resource_type: str, resource_id: str) -> str:
        """Build ARN for a resource"""
        service = self.get_service_name()
        account = self.account_id
        region = self.region

        # Determine service based on resource type
        if resource_type == "bucket":
            return f"arn:aws:s3:::{resource_id}"
        elif resource_type.startswith("role") or resource_type.startswith("policy"):
            return f"arn:aws:iam::{account}:{resource_type}/{resource_id}"
        elif service == "ec2":
            return f"arn:aws:ec2:{region}:{account}:{resource_type}/{resource_id}"
        else:
            return f"arn:aws:{service}:{region}:{account}:{resource_type}/{resource_id}"

    def generate_resource_identifier(self, resource: Dict[str, Any]) -> str:
        """
        Generate a standardized resource identifier

        Args:
            resource: Dictionary containing resource information
        Returns:
            Unique resource identifier
        """
        resource_type = resource.get("type")
        resource_id = resource.get("id")
        
        # Use ARN if available
        if "arn" in resource:
            return resource["arn"]
            
        # Special handling for IAM resources
        if resource_type and resource_type.startswith("aws_iam_"):
            if resource_type == "aws_iam_role_policy_attachment":
                role_name = resource.get("role")
                policy_arn = resource.get("policy_arn")
                if role_name and policy_arn:
                    return f"arn:aws:iam::{self.account_id}:role/{role_name}/{policy_arn}"
            elif resource_type == "aws_iam_user_policy":
                user_name = resource.get("user")
                policy_name = resource.get("name")
                if user_name and policy_name:
                    return f"{user_name}:{policy_name}"
            elif resource_type == "aws_iam_user_policy_attachment":
                user_name = resource.get("user")
                policy_arn = resource.get("policy_arn")
                if user_name and policy_arn:
                    return f"{user_name}:{policy_arn}"
        
        # Special handling for VPC endpoints
        if resource_type == "aws_vpc_endpoint":
            try:
                details = resource.get("details", {})
                vpc_id = details.get("vpc_id")
                service_name = details.get("service_name")
                endpoint_id = resource.get("id")

                # Output debug information
                logger.debug(f"Generating identifier for VPC endpoint:")
                logger.debug(f"  vpc_id: {vpc_id}")
                logger.debug(f"  service_name: {service_name}")
                logger.debug(f"  endpoint_id: {endpoint_id}")
                logger.debug(f"  details: {details}")

                # Get Name tag (first from details, then from tags)
                name = details.get("name")
                if not name:
                    tags = resource.get("tags", [])
                    if isinstance(tags, list):
                        for tag in tags:
                            if isinstance(tag, dict) and tag.get("Key") == "Name":
                                name = tag.get("Value")
                                break

                # Use endpoint ID as primary identifier if available
                if endpoint_id:
                    # Basic identifier
                    identifier = f"{resource_type}:{endpoint_id}"
                    
                    # Generate more detailed identifier if additional information is available
                    if name and vpc_id and service_name:
                        identifier = f"{resource_type}:{name}:{vpc_id}:{service_name}:{endpoint_id}"
                    elif vpc_id and service_name:
                        identifier = f"{resource_type}:{vpc_id}:{service_name}:{endpoint_id}"

                logger.debug(f"Generated identifier for VPC endpoint: {identifier}")
                return identifier

            except Exception as e:
                logger.error(f"Error generating identifier for VPC endpoint: {str(e)}")
                logger.debug("Resource data:", exc_info=True)
                logger.debug(f"Resource: {resource}")
                return None

        # Basic identifier generation
        if resource_type and resource_id:
            # Tag-based identifier (prioritize Name tag)
            name_tag = None
            tags = resource.get("tags", [])
            if isinstance(tags, dict):
                name_tag = tags.get("Name")
            elif isinstance(tags, list):
                for tag in tags:
                    if isinstance(tag, dict) and tag.get("Key") == "Name":
                        name_tag = tag.get("Value")
                        break
            
            if name_tag:
                return f"{resource_type}:{name_tag}:{resource_id}"
            return f"{resource_type}:{resource_id}"
            
        # Fallback: ID only
        return resource_id if resource_id else ""

    def _extract_resources_from_state(self, state_data: Dict[str, Any], managed_resources: Dict[str, Any]):
        """
        Terraformの状態データからリソースを抽出し、managed_resourcesに追加する。
        
        Args:
            state_data (Dict[str, Any]): Terraformの状態データ
            managed_resources (Dict[str, Any]): 抽出したリソースを格納する辞書
        """
        for resource in state_data.get("resources", []):
            resource_type = resource.get("type")
            if resource_type not in self.get_resource_types():
                continue  # 対応していないリソースタイプはスキップ

            for instance in resource.get("instances", []):
                attributes = instance.get("attributes", {})
                resource_id = attributes.get("id") or attributes.get("name")  # 適切なIDの取得
                if not resource_id:
                    continue  # IDがないリソースはスキップ

                arn = attributes.get("arn")
                if not arn:
                    arn = self.build_arn(resource_type, resource_id)

                identifier = self.generate_resource_identifier({
                    "type": resource_type,
                    "id": resource_id,
                    "arn": arn,
                    "tags": self.extract_tags(attributes.get("tags", []))
                })

                managed_resources[arn] = {
                    "managed": True,
                    "resource": {
                        "type": resource_type,
                        "id": resource_id,
                        "arn": arn,
                        "tags": self.extract_tags(attributes.get("tags", [])),
                    }
                }


class CollectorRegistry:
    """Registry for resource collectors"""

    def __init__(self):
        self.collectors = []

    def register(self, collector_class: type):
        """Register a collector class"""
        self.collectors.append(collector_class)
        return collector_class

    def get_collectors(self, session: boto3.Session, target_type: str = "") -> List[ResourceCollector]:
        """
        Get collector instances with the given session, optionally filtered by target type.
        target_type can be either a full resource type (e.g. aws_s3_bucket) or a service category (e.g. s3)
        """
        logger.debug(f"Getting collectors, total registered: {len(self.collectors)}")
        instances = []
        for collector_cls in self.collectors:
            try:
                collector = collector_cls(session)
                # category or resource type filter
                if target_type:
                    resource_types = collector.get_resource_types()
                    service_name = collector.get_service_name()
                    if target_type != service_name and target_type not in resource_types:
                        continue
                instances.append(collector)
            except Exception as e:
                logger.error(
                    f"Failed to initialize collector {collector_cls.__name__}: {e}"
                )
        return instances

    def iter_classes(self):
        """Iterator over collector classes"""
        return iter(self.collectors)

    def __iter__(self):
        """Make the registry iterable over collector classes"""
        return self.iter_classes()

    def __len__(self):
        """Get number of registered collectors"""
        return len(self.collectors)


# Global registry instance
registry = CollectorRegistry()


def register_collector(collector_class: type):
    """Decorator to register a collector class"""
    return registry.register(collector_class)
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage/__init__.py</source>
<document_content>from .s3 import S3Collector
from .efs import EFSCollector
from .ebs import EBSCollector

__all__ = ["S3Collector", "EFSCollector", "EBSCollector"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage/efs.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class EFSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "efs"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_efs_file_system": "EFS File Systems"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("describe_file_systems")
            for page in paginator.paginate():
                for fs in page["FileSystems"]:
                    resources.append(
                        {
                            "type": "aws_efs_file_system",
                            "id": fs["FileSystemId"],
                            "arn": fs["FileSystemArn"],
                            "tags": fs.get("Tags", []),
                        }
                    )
        except Exception as e:
            print(f"Error collecting EFS filesystems: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage/s3.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class S3Collector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "s3"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_s3_bucket": "S3 Buckets",
            "aws_s3_bucket_versioning": "S3 Bucket Versioning",
            "aws_s3_bucket_server_side_encryption_configuration": "S3 Bucket Encryption",
            "aws_s3_bucket_public_access_block": "S3 Bucket Public Access Block",
            "aws_s3_bucket_acl": "S3 Bucket ACL",
            "aws_s3_bucket_policy": "S3 Bucket Policy",
            "aws_s3_bucket_cors_configuration": "S3 Bucket CORS",
            "aws_s3_bucket_website_configuration": "S3 Bucket Website",
            "aws_s3_bucket_logging": "S3 Bucket Logging",
            "aws_s3_bucket_lifecycle_configuration": "S3 Bucket Lifecycle"
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            for bucket in self.client.list_buckets()["Buckets"]:
                bucket_name = bucket["Name"]
                try:
                    tags = self.client.get_bucket_tagging(Bucket=bucket_name).get(
                        "TagSet", []
                    )
                except:  # noqa: E722
                    tags = []

                # Get encryption configuration
                try:
                    encryption = self.client.get_bucket_encryption(Bucket=bucket_name)
                    encryption_rules = encryption.get('ServerSideEncryptionConfiguration', {}).get('Rules', [])
                    if encryption_rules:
                        encryption_config = encryption_rules[0].get('ApplyServerSideEncryptionByDefault', {})
                        encryption_details = {
                            "sse_algorithm": encryption_config.get('SSEAlgorithm'),
                            "kms_master_key_id": encryption_config.get('KMSMasterKeyID')
                        }
                except:  # noqa: E722
                    encryption_details = {}

                # Get ACL configuration
                # Get ACL configuration
                acl_details = None
                try:
                    acl = self.client.get_bucket_acl(Bucket=bucket_name)
                    if acl and isinstance(acl, dict):
                        owner = acl.get('Owner', {})
                        grants = acl.get('Grants', [])
                        if owner or grants:
                            acl_details = {
                                "owner": owner,
                                "grants": grants
                            }
                            logger.info(f"Successfully retrieved ACL for bucket: {bucket_name}")
                except self.client.exceptions.ClientError as e:
                    error_code = e.response['Error']['Code']
                    error_message = e.response['Error']['Message']
                    logger.warning(f"Error getting ACL for bucket {bucket_name}: Code={error_code}, Message={error_message}")
                except Exception as e:
                    logger.warning(f"Unexpected error getting ACL for bucket {bucket_name}: {str(e)}")

                # Get bucket policy
                try:
                    policy = self.client.get_bucket_policy(Bucket=bucket_name)
                    policy_text = policy.get('Policy')
                    if policy_text:
                        try:
                            # Verify that the policy is valid JSON
                            import json
                            json.loads(policy_text)
                            logger.info(f"Found valid bucket policy for {bucket_name}")
                            logger.debug(f"Policy content: {policy_text}")
                            
                            # Add only if policy exists and is valid JSON
                            resources.append({
                                "type": "aws_s3_bucket_policy",
                                "id": bucket_name,
                                "arn": f"arn:aws:s3:::{bucket_name}",  # Add ARN
                                "details": {
                                    "policy": policy_text
                                }
                            })
                            logger.info(f"Successfully added bucket policy for: {bucket_name}")
                        except json.JSONDecodeError as je:
                            logger.error(f"Invalid JSON in bucket policy for {bucket_name}: {je}")
                    else:
                        logger.warning(f"Empty policy returned for bucket: {bucket_name}")
                except self.client.exceptions.ClientError as e:
                    error_code = e.response['Error']['Code']
                    error_message = e.response['Error']['Message']
                    if error_code in ['NoSuchPolicy', 'NoSuchBucketPolicy']:
                        logger.debug(f"No bucket policy found for bucket: {bucket_name} (Code={error_code}, Message={error_message})")
                    else:
                        logger.warning(f"Unexpected error getting bucket policy for {bucket_name}: Code={error_code}, Message={error_message}")
                except Exception as e:
                    logger.warning(f"Unexpected error getting bucket policy for {bucket_name}: {str(e)}")

                # Get CORS configuration
                try:
                    cors = self.client.get_bucket_cors(Bucket=bucket_name)
                    cors_rules = cors.get('CORSRules', [])
                except:  # noqa: E722
                    cors_rules = []

                # Get website configuration
                try:
                    website = self.client.get_bucket_website(Bucket=bucket_name)
                    website_config = {
                        "index_document": website.get('IndexDocument', {}).get('Suffix'),
                        "error_document": website.get('ErrorDocument', {}).get('Key'),
                        "routing_rules": website.get('RoutingRules', [])
                    }
                except:  # noqa: E722
                    website_config = {}

                # Get logging configuration
                try:
                    logging = self.client.get_bucket_logging(Bucket=bucket_name)
                    logging_config = logging.get('LoggingEnabled', {})
                    if logging_config:
                        logging_details = {
                            "target_bucket": logging_config.get('TargetBucket'),
                            "target_prefix": logging_config.get('TargetPrefix')
                        }
                    else:
                        logging_details = {}
                except:  # noqa: E722
                    logging_details = {}

                # Get versioning configuration
                try:
                    versioning = self.client.get_bucket_versioning(Bucket=bucket_name)
                    versioning_status = versioning.get('Status')
                except:  # noqa: E722
                    versioning_status = None

                # Get public access block configuration
                try:
                    public_access = self.client.get_public_access_block(Bucket=bucket_name)
                    public_access_block = public_access.get('PublicAccessBlockConfiguration', {})
                except:  # noqa: E722
                    public_access_block = {}

                # Main bucket resource
                resources.append({
                    "type": "aws_s3_bucket",
                    "id": bucket_name,
                    "arn": f"arn:aws:s3:::{bucket_name}",
                    "tags": tags
                })

                # Versioning configuration
                if versioning_status:
                    resources.append({
                        "type": "aws_s3_bucket_versioning",
                        "id": bucket_name,
                        "details": {
                            "versioning_status": versioning_status
                        }
                    })

                # Encryption configuration
                if encryption_details:
                    resources.append({
                        "type": "aws_s3_bucket_server_side_encryption_configuration",
                        "id": bucket_name,
                        "details": encryption_details
                    })

                # Public access block configuration
                if public_access_block:
                    resources.append({
                        "type": "aws_s3_bucket_public_access_block",
                        "id": bucket_name,
                        "details": public_access_block
                    })

                # ACL configuration
                if acl_details and isinstance(acl_details, dict):
                    owner = acl_details.get("owner", {})
                    grants = acl_details.get("grants", [])
                    if owner or grants:
                        logger.info(f"Adding ACL resource for bucket: {bucket_name}")
                        resources.append({
                            "type": "aws_s3_bucket_acl",
                            "id": bucket_name,
                            "details": acl_details
                        })
                        logger.debug(f"ACL details for {bucket_name}: {acl_details}")

                # Bucket policy has already been added above

                # CORS configuration
                if cors_rules:
                    resources.append({
                        "type": "aws_s3_bucket_cors_configuration",
                        "id": bucket_name,
                        "details": {
                            "cors_rules": cors_rules
                        }
                    })

                # Website configuration
                if website_config.get("index_document") or website_config.get("error_document"):
                    resources.append({
                        "type": "aws_s3_bucket_website_configuration",
                        "id": bucket_name,
                        "details": website_config
                    })

                # Logging configuration
                if logging_details:
                    resources.append({
                        "type": "aws_s3_bucket_logging",
                        "id": bucket_name,
                        "details": logging_details
                    })
        except Exception as e:
            print(f"Error collecting S3 buckets: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage/ebs.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class EBSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "ec2"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_ebs_volume": "EBS Volumes"}

    def _should_manage_volume(self, volume: Dict[str, Any]) -> bool:
        """
        Determine if an EBS volume should be explicitly managed.

        Returns True if:
        - Volume is not attached to any instance (unattached volumes should be managed)
        - Volume has DeleteOnTermination=False for any attachment (preserved volumes should be managed)

        Returns False if:
        - Volume is attached with DeleteOnTermination=True (these are managed with EC2)
        """
        attachments = volume.get("Attachments", [])

        # If volume is not attached, it should be managed
        if not attachments:
            return True

        # Check DeleteOnTermination flag for all attachments
        # If any attachment has DeleteOnTermination=False, the volume should be managed
        for attachment in attachments:
            if not attachment.get("DeleteOnTermination", True):
                return True

        # Volume is attached and all attachments have DeleteOnTermination=True
        return False

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("describe_volumes")
            for page in paginator.paginate():
                for volume in page["Volumes"]:
                    # Only include volumes that should be explicitly managed
                    if not self._should_manage_volume(volume):
                        continue

                    resources.append(
                        {
                            "type": "aws_ebs_volume",
                            "id": volume["VolumeId"],
                            "arn": self.build_arn("volume", volume["VolumeId"]),
                            "tags": volume.get("Tags", []),
                            "details": {
                                "size": volume.get("Size"),
                                "encrypted": volume.get("Encrypted"),
                                "volume_type": volume.get("VolumeType"),
                                "create_time": str(volume.get("CreateTime")),
                                "attachments": [
                                    {
                                        "instance_id": att.get("InstanceId"),
                                        "device": att.get("Device"),
                                        "delete_on_termination": att.get(
                                            "DeleteOnTermination", True
                                        ),
                                    }
                                    for att in volume.get("Attachments", [])
                                ],
                            },
                        }
                    )

        except Exception as e:
            logger.error(f"Error collecting EBS volumes: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_network/vpc.py</source>
<document_content># terraform_aws_migrator/collectors/aws_network/vpc.py

from typing import Dict, List, Any
import logging
import copy
from ..base import ResourceCollector, register_collector

logger = logging.getLogger(__name__)


@register_collector
class VPCCollector(ResourceCollector):
    """Collector for VPC resources"""

    @classmethod
    def get_service_name(cls) -> str:
        return "ec2"

    @classmethod
    def get_resource_types(cls) -> Dict[str, str]:
        return {"aws_vpc": "VPC"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources: List[Dict[str, Any]] = []
        try:
            if not target_resource_type or target_resource_type == "aws_vpc":
                resources.extend(self._collect_vpcs())
        except Exception as e:
            logger.error(f"Error collecting VPCs: {str(e)}")

        return resources

    def _collect_vpcs(self) -> List[Dict[str, Any]]:
        """Collect VPCs"""
        resources: List[Dict[str, Any]] = []
        try:
            if not self.client:
                logger.error("Failed to initialize EC2 client")
                return resources

            paginator = self.client.get_paginator("describe_vpcs")
            for page in paginator.paginate():
                for vpc in page["Vpcs"]:
                    # Get VPC attributes
                    vpc_attributes = {
                        "enable_dns_support": True,
                        "enable_dns_hostnames": False,
                    }

                    try:
                        dns_support = self.client.describe_vpc_attribute(
                            VpcId=vpc["VpcId"], Attribute="enableDnsSupport"
                        )
                        vpc_attributes["enable_dns_support"] = dns_support[
                            "EnableDnsSupport"
                        ]["Value"]
                    except Exception as e:
                        logger.error(
                            f"Error getting DNS support attribute for VPC {vpc['VpcId']}: {str(e)}"
                        )
                        logger.error(
                            f"Using default value for enable_dns_support: {vpc_attributes['enable_dns_support']}"
                        )

                    try:
                        dns_hostnames = self.client.describe_vpc_attribute(
                            VpcId=vpc["VpcId"], Attribute="enableDnsHostnames"
                        )
                        vpc_attributes["enable_dns_hostnames"] = dns_hostnames[
                            "EnableDnsHostnames"
                        ]["Value"]
                    except Exception as e:
                        logger.error(
                            f"Error getting DNS hostnames attribute for VPC {vpc['VpcId']}: {str(e)}"
                        )
                        logger.error(
                            f"Using default value for enable_dns_hostnames: {vpc_attributes['enable_dns_hostnames']}"
                        )

                    try:
                        # Create a deep copy of vpc_attributes to avoid reference issues
                        vpc_details = {
                            "cidr_block": vpc["CidrBlock"],
                            "instance_tenancy": vpc.get("InstanceTenancy", "default"),
                            "enable_dns_support": dict(vpc_attributes).get(
                                "enable_dns_support", True
                            ),
                            "enable_dns_hostnames": dict(vpc_attributes).get(
                                "enable_dns_hostnames", False
                            ),
                            "is_default": vpc.get("IsDefault", False),
                            "cidr_block_associations": [],
                            "ipv6_cidr_block": None,
                            "ipv6_association_id": None,
                            "dhcp_options_id": vpc.get("DhcpOptionsId"),
                            "enable_network_address_usage_metrics": vpc.get(
                                "EnableNetworkAddressUsageMetrics", False
                            ),
                            "enable_classiclink": False,
                            "enable_classiclink_dns_support": False,
                        }

                        # Get Classic Link status
                        try:
                            classic_link_response = (
                                self.client.describe_vpc_classic_link(
                                    VpcIds=[vpc["VpcId"]]
                                )
                            )

                            if classic_link_response["Vpcs"]:
                                vpc_details["enable_classiclink"] = (
                                    classic_link_response["Vpcs"][0].get(
                                        "ClassicLinkEnabled", False
                                    )
                                )
                        except self.client.exceptions.UnsupportedOperation:
                            logger.info(
                                f"Classic Link is not supported in this region for VPC {vpc['VpcId']}"
                            )
                            vpc_details["enable_classiclink"] = False
                        except self.client.exceptions.ClientError as e:
                            if e.response["Error"]["Code"] == "InvalidVpcID.NotFound":
                                logger.error(
                                    f"VPC {vpc['VpcId']} not found when checking Classic Link status"
                                )
                            else:
                                logger.error(
                                    f"Error getting Classic Link status for VPC {vpc['VpcId']}: {str(e)}"
                                )
                            vpc_details["enable_classiclink"] = False
                        except Exception as e:
                            logger.error(
                                f"Unexpected error getting Classic Link status for VPC {vpc['VpcId']}: {str(e)}"
                            )
                            vpc_details["enable_classiclink"] = False

                        # Get Classic Link DNS support status
                        try:
                            dns_support_response = (
                                self.client.describe_vpc_classic_link_dns_support(
                                    VpcIds=[vpc["VpcId"]]
                                )
                            )

                            if dns_support_response["Vpcs"]:
                                vpc_details["enable_classiclink_dns_support"] = (
                                    dns_support_response["Vpcs"][0].get(
                                        "ClassicLinkDnsSupported", False
                                    )
                                )
                        except self.client.exceptions.UnsupportedOperation:
                            vpc_details["enable_classiclink_dns_support"] = False
                        except self.client.exceptions.ClientError as e:
                            if e.response["Error"]["Code"] == "InvalidVpcID.NotFound":
                                logger.error(
                                    f"VPC {vpc['VpcId']} not found when checking Classic Link DNS support"
                                )
                            else:
                                logger.error(
                                    f"Error getting Classic Link DNS support status for VPC {vpc['VpcId']}: {str(e)}"
                                )
                            vpc_details["enable_classiclink_dns_support"] = False
                        except Exception as e:
                            logger.error(
                                f"Unexpected error getting Classic Link DNS support status for VPC {vpc['VpcId']}: {str(e)}"
                            )
                            vpc_details["enable_classiclink_dns_support"] = False
                        logger.debug(f"Building VPC details for {vpc['VpcId']}")

                        cidr_associations = []
                        for assoc in vpc.get("CidrBlockAssociationSet", []):
                            try:
                                cidr_associations.append(
                                    {
                                        "primary": assoc["CidrBlock"]
                                        == vpc["CidrBlock"],
                                        "cidr_block": assoc["CidrBlock"],
                                        "state": assoc["CidrBlockState"]["State"],
                                    }
                                )
                            except Exception as e:
                                logger.error(
                                    f"Error processing CIDR association for VPC {vpc['VpcId']}: {str(e)}"
                                )
                        vpc_details["cidr_block_associations"] = cidr_associations
                        logger.debug(f"Processed CIDR blocks for VPC {vpc['VpcId']}")

                        # IPv6
                        try:
                            ipv6_associations = vpc.get(
                                "Ipv6CidrBlockAssociationSet", []
                            )
                            for assoc in ipv6_associations:
                                if (
                                    assoc.get("Ipv6CidrBlockState", {}).get("State")
                                    == "associated"
                                ):
                                    vpc_details["ipv6_cidr_block"] = assoc.get(
                                        "Ipv6CidrBlock"
                                    )
                                    vpc_details["ipv6_association_id"] = assoc.get(
                                        "AssociationId"
                                    )
                                    break
                            logger.debug(
                                f"Processed IPv6 configuration for VPC {vpc['VpcId']}"
                            )
                        except Exception as e:
                            logger.error(
                                f"Error processing IPv6 information for VPC {vpc['VpcId']}: {str(e)}"
                            )

                        logger.debug(
                            f"Final VPC details for {vpc['VpcId']}: {vpc_details}"
                        )
                    except Exception as e:
                        logger.error(
                            f"Error creating VPC details for {vpc['VpcId']}: {str(e)}"
                        )
                        continue

                    try:
                        # Create a deep copy of the resource structure
                        logger.debug(
                            f"Creating resource structure for VPC {vpc['VpcId']}"
                        )

                        resource = {
                            "type": "aws_vpc",
                            "id": vpc["VpcId"],
                            "arn": f"arn:aws:ec2:{self.region}:{self.account_id}:vpc/{vpc['VpcId']}",
                            "tags": copy.deepcopy(
                                vpc.get("Tags", [])
                            ),
                            "details": copy.deepcopy(
                                vpc_details
                            ),
                        }

                        logger.debug(
                            f"Created resource for VPC {vpc['VpcId']} with details"
                        )

                        # Create final copy for appending
                        final_resource = copy.deepcopy(resource)

                        # Verify details are present and not empty
                        if not resource["details"]:
                            logger.error(f"Empty details for VPC {vpc['VpcId']}")
                            continue

                        resources.append(
                            final_resource
                        )  # Add the already deep copied resource
                        logger.debug(f"Added VPC resource: {vpc['VpcId']}")
                    except Exception as e:
                        logger.error(
                            f"Error creating resource for VPC {vpc['VpcId']}: {str(e)}"
                        )
                        continue
        except Exception as e:
            logger.error(f"Error collecting VPCs: {str(e)}")
        return resources


@register_collector
class VPCNetworkComponentsCollector(ResourceCollector):
    """Collector for VPC and related network components"""

    @classmethod
    def get_service_name(cls) -> str:
        return "ec2"

    @classmethod
    def get_resource_types(cls) -> Dict[str, str]:
        return {
            "aws_subnet": "VPC Subnets",
            "aws_route_table": "VPC Route Tables",
            "aws_route": "VPC Routes",
            "aws_internet_gateway": "Internet Gateways",
            "aws_nat_gateway": "NAT Gateways",
            "aws_vpc_endpoint": "VPC Endpoints",
            "aws_network_acl": "Network ACLs",
            "aws_vpc_dhcp_options": "VPC DHCP Options",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources: List[Dict[str, Any]] = []
        try:
            logger.debug(f"Collecting resources for type: {target_resource_type}")

            # Map resource types to collection methods
            collection_methods = {
                "aws_subnet": self._collect_subnets,
                "aws_route_table": self._collect_route_tables,
                "aws_internet_gateway": self._collect_internet_gateways,
                "aws_nat_gateway": self._collect_nat_gateways,
                "aws_vpc_endpoint": self._collect_vpc_endpoints,
                "aws_network_acl": self._collect_network_acls,
                "aws_vpc_dhcp_options": self._collect_dhcp_options,
            }

            if target_resource_type:
                if target_resource_type in collection_methods:
                    logger.debug(
                        f"Collecting specific resource type: {target_resource_type}"
                    )
                    collected = collection_methods[target_resource_type]()
                    logger.debug(
                        f"Collected {len(collected)} resources of type {target_resource_type}"
                    )
                    resources.extend(collected)
            else:
                # Collect all resource types if no specific type is specified
                for resource_type, collect_method in collection_methods.items():
                    logger.debug(f"Collecting all resources of type: {resource_type}")
                    collected = collect_method()
                    logger.debug(
                        f"Collected {len(collected)} resources of type {resource_type}"
                    )
                    resources.extend(collected)
        except Exception as e:
            logger.error(f"Error collecting VPC components: {str(e)}")

        return resources

    def _collect_subnets(self) -> List[Dict[str, Any]]:
        """Collect VPC Subnets"""
        resources: List[Dict[str, Any]] = []
        try:
            logger.debug("Creating paginator for describe_subnets")
            paginator = self.client.get_paginator("describe_subnets")

            for page in paginator.paginate():
                logger.debug(
                    f"Processing subnet page with {len(page.get('Subnets', []))} subnets"
                )
                for subnet in page["Subnets"]:
                    try:
                        subnet_id = subnet["SubnetId"]
                        logger.debug(f"Processing subnet: {subnet_id}")

                        details = {
                            "vpc_id": subnet["VpcId"],
                            "subnet_id": subnet_id,
                            "cidr_block": subnet["CidrBlock"],
                            "availability_zone": subnet["AvailabilityZone"],
                            "map_public_ip_on_launch": subnet["MapPublicIpOnLaunch"],
                            "assign_ipv6_address_on_creation": subnet.get(
                                "AssignIpv6AddressOnCreation", False
                            ),
                            "ipv6_cidr_block": subnet.get("Ipv6CidrBlock", ""),
                            "enable_dns64": subnet.get("EnableDns64", False),
                        }

                        resource = {
                            "type": "aws_subnet",
                            "id": subnet_id,
                            "arn": f"arn:aws:ec2:{self.region}:{self.account_id}:subnet/{subnet_id}",
                            "tags": subnet.get("Tags", []),
                            "details": details,
                        }

                        resources.append(resource)

                    except KeyError as ke:
                        logger.error(
                            f"Missing required field for subnet {subnet.get('SubnetId', 'unknown')}: {ke}"
                        )
                    except Exception as e:
                        logger.error(
                            f"Error processing subnet {subnet.get('SubnetId', 'unknown')}: {str(e)}"
                        )

        except Exception as e:
            logger.error(f"Error collecting subnets: {str(e)}")
            logger.debug(f"Full error details: {str(e)}", exc_info=True)

        logger.debug(f"Collected {len(resources)} subnets in total")
        return resources

    def _collect_route_tables(self) -> List[Dict[str, Any]]:
        """Collect VPC Route Tables and Routes"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_route_tables")
            for page in paginator.paginate():
                for rt in page["RouteTables"]:
                    route_table = {
                        "type": "aws_route_table",
                        "id": rt["RouteTableId"],
                        "arn": f"arn:aws:ec2:{self.region}:{self.account_id}:route-table/{rt['RouteTableId']}",
                        "tags": rt.get("Tags", []),
                        "details": {
                            "vpc_id": rt["VpcId"],
                            "routes": [],
                            "associations": [],
                        },
                    }

                    # Collect routes
                    for route in rt.get("Routes", []):
                        route_detail = {
                            "destination_cidr_block": route.get("DestinationCidrBlock"),
                            "destination_ipv6_cidr_block": route.get(
                                "DestinationIpv6CidrBlock"
                            ),
                            "gateway_id": route.get("GatewayId"),
                            "instance_id": route.get("InstanceId"),
                            "nat_gateway_id": route.get("NatGatewayId"),
                            "network_interface_id": route.get("NetworkInterfaceId"),
                            "vpc_peering_connection_id": route.get(
                                "VpcPeeringConnectionId"
                            ),
                        }
                        route_table["details"]["routes"].append(route_detail)

                    # Collect associations
                    for assoc in rt.get("Associations", []):
                        association = {
                            "id": assoc["RouteTableAssociationId"],
                            "subnet_id": assoc.get("SubnetId"),
                            "gateway_id": assoc.get("GatewayId"),
                            "main": assoc.get("Main", False),
                        }
                        route_table["details"]["associations"].append(association)

                    resources.append(route_table)
        except Exception as e:
            logger.error(f"Error collecting route tables: {str(e)}")
        return resources

    def _collect_internet_gateways(self) -> List[Dict[str, Any]]:
        """Collect Internet Gateways"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_internet_gateways")
            for page in paginator.paginate():
                for igw in page["InternetGateways"]:
                    vpc_attachments = []
                    for attachment in igw.get("Attachments", []):
                        vpc_attachments.append(
                            {
                                "vpc_id": attachment.get("VpcId"),
                                "state": attachment.get("State"),
                            }
                        )

                    resources.append(
                        {
                            "type": "aws_internet_gateway",
                            "id": igw["InternetGatewayId"],
                            "arn": f"arn:aws:ec2:{self.region}:{self.account_id}:internet-gateway/{igw['InternetGatewayId']}",
                            "tags": igw.get("Tags", []),
                            "details": {"vpc_attachments": vpc_attachments},
                        }
                    )
        except Exception as e:
            logger.error(f"Error collecting internet gateways: {str(e)}")
        return resources

    def _collect_nat_gateways(self) -> List[Dict[str, Any]]:
        """Collect NAT Gateways"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_nat_gateways")
            for page in paginator.paginate():
                for nat in page["NatGateways"]:
                    resources.append(
                        {
                            "type": "aws_nat_gateway",
                            "id": nat["NatGatewayId"],
                            "arn": f"arn:aws:ec2:{self.region}:{self.account_id}:nat-gateway/{nat['NatGatewayId']}",
                            "tags": nat.get("Tags", []),
                            "details": {
                                "vpc_id": nat["VpcId"],
                                "subnet_id": nat["SubnetId"],
                                "state": nat["State"],
                                "connectivity_type": nat.get(
                                    "ConnectivityType", "public"
                                ),
                                "elastic_ip_address": nat.get(
                                    "NatGatewayAddresses", [{}]
                                )[0].get("PublicIp"),
                                "private_ip": nat.get("NatGatewayAddresses", [{}])[
                                    0
                                ].get("PrivateIp"),
                                "network_interface_id": nat.get(
                                    "NatGatewayAddresses", [{}]
                                )[0].get("NetworkInterfaceId"),
                            },
                        }
                    )
        except Exception as e:
            logger.error(f"Error collecting NAT gateways: {str(e)}")
        return resources

    def _collect_vpc_endpoints(self) -> List[Dict[str, Any]]:
        """Collect VPC Endpoints"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_vpc_endpoints")
            for page in paginator.paginate():
                for endpoint in page["VpcEndpoints"]:
                    try:
                        # Check for required fields
                        if not all(k in endpoint for k in ["VpcEndpointId", "VpcId", "ServiceName"]):
                            logger.warning(f"Skipping VPC endpoint due to missing required fields: {endpoint.get('VpcEndpointId', 'unknown')}")
                            continue

                        # Normalize policy document
                        policy_doc = endpoint.get("PolicyDocument", "{}")
                        if isinstance(policy_doc, str):
                            try:
                                import json
                                policy_doc = json.dumps(json.loads(policy_doc), sort_keys=True)
                            except json.JSONDecodeError as e:
                                logger.warning(f"Invalid policy document for VPC endpoint {endpoint['VpcEndpointId']}: {str(e)}")
                                policy_doc = "{}"

                        # Get Name tag
                        name_tag = None
                        for tag in endpoint.get("Tags", []):
                            if isinstance(tag, dict) and tag.get("Key") == "Name":
                                name_tag = tag.get("Value")
                                break

                        resource = {
                            "type": "aws_vpc_endpoint",
                            "id": endpoint["VpcEndpointId"],
                            "arn": endpoint.get("VpcEndpointArn", ""),
                            "tags": endpoint.get("Tags", []),
                            "details": {
                                "vpc_id": endpoint["VpcId"],
                                "service_name": endpoint["ServiceName"],
                                "state": endpoint.get("State", "available"),
                                "vpc_endpoint_type": endpoint.get("VpcEndpointType", "Gateway"),
                                "subnet_ids": endpoint.get("SubnetIds", []),
                                "route_table_ids": endpoint.get("RouteTableIds", []),
                                "private_dns_enabled": endpoint.get("PrivateDnsEnabled", False),
                                "network_interface_ids": endpoint.get("NetworkInterfaceIds", []),
                                "dns_entries": endpoint.get("DnsEntries", []),
                                "policy": policy_doc,
                                "name": name_tag,
                            },
                        }

                        # Verify required fields for identifier generation
                        if not resource["details"]["vpc_id"] or not resource["details"]["service_name"]:
                            logger.warning(f"Missing required fields for VPC endpoint identifier: {endpoint['VpcEndpointId']}")
                            continue

                        resources.append(resource)
                        logger.debug(f"Successfully processed VPC endpoint: {endpoint['VpcEndpointId']}")
                    except Exception as e:
                        logger.error(f"Error processing VPC endpoint {endpoint.get('VpcEndpointId', 'unknown')}: {str(e)}")
                        continue
        except Exception as e:
            logger.error(f"Error collecting VPC endpoints: {str(e)}")
        return resources

    def _collect_network_acls(self) -> List[Dict[str, Any]]:
        """Collect Network ACLs"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_network_acls")
            for page in paginator.paginate():
                for acl in page["NetworkAcls"]:
                    resources.append(
                        {
                            "type": "aws_network_acl",
                            "id": acl["NetworkAclId"],
                            "arn": f"arn:aws:ec2:{self.region}:{self.account_id}:network-acl/{acl['NetworkAclId']}",
                            "tags": acl.get("Tags", []),
                            "details": {
                                "vpc_id": acl["VpcId"],
                                "is_default": acl["IsDefault"],
                                "ingress_rules": [
                                    rule
                                    for rule in acl.get("Entries", [])
                                    if not rule["Egress"]
                                ],
                                "egress_rules": [
                                    rule
                                    for rule in acl.get("Entries", [])
                                    if rule["Egress"]
                                ],
                                "associations": acl.get("Associations", []),
                            },
                        }
                    )
        except Exception as e:
            logger.error(f"Error collecting network ACLs: {str(e)}")
        return resources

    def _collect_dhcp_options(self) -> List[Dict[str, Any]]:
        """Collect VPC DHCP Options Sets"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_dhcp_options")
            for page in paginator.paginate():
                for dhcp in page["DhcpOptions"]:
                    resources.append(
                        {
                            "type": "aws_vpc_dhcp_options",
                            "id": dhcp["DhcpOptionsId"],
                            "arn": f"arn:aws:ec2:{self.region}:{self.account_id}:dhcp-options/{dhcp['DhcpOptionsId']}",
                            "tags": dhcp.get("Tags", []),
                            "details": {
                                "domain_name": next(
                                    (
                                        item["Values"]
                                        for item in dhcp["DhcpConfigurations"]
                                        if item["Key"] == "domain-name"
                                    ),
                                    [],
                                ),
                                "domain_name_servers": next(
                                    (
                                        item["Values"]
                                        for item in dhcp["DhcpConfigurations"]
                                        if item["Key"] == "domain-name-servers"
                                    ),
                                    [],
                                ),
                                "ntp_servers": next(
                                    (
                                        item["Values"]
                                        for item in dhcp["DhcpConfigurations"]
                                        if item["Key"] == "ntp-servers"
                                    ),
                                    [],
                                ),
                                "netbios_name_servers": next(
                                    (
                                        item["Values"]
                                        for item in dhcp["DhcpConfigurations"]
                                        if item["Key"] == "netbios-name-servers"
                                    ),
                                    [],
                                ),
                                "netbios_node_type": next(
                                    (
                                        item["Values"]
                                        for item in dhcp["DhcpConfigurations"]
                                        if item["Key"] == "netbios-node-type"
                                    ),
                                    [],
                                ),
                            },
                        }
                    )
        except Exception as e:
            logger.error(f"Error collecting DHCP options: {str(e)}")
        return resources

    def generate_resource_identifier(self, resource: Dict[str, Any]) -> str:
        """Generate resource identifier"""
        if resource.get("type") != "aws_vpc_endpoint":
            return super().generate_resource_identifier(resource)
        
        try:
            details = resource.get("details", {})
            vpc_id = details.get("vpc_id")
            service_name = details.get("service_name")
            endpoint_id = resource.get("id")
            name = details.get("name")

            # Add debug logging
            logger.debug(f"Generating identifier for VPC endpoint:")
            logger.debug(f"  vpc_id: {vpc_id}")
            logger.debug(f"  service_name: {service_name}")
            logger.debug(f"  endpoint_id: {endpoint_id}")
            logger.debug(f"  name: {name}")
            logger.debug(f"  details: {details}")

            # Use endpoint ID as primary identifier if available
            if endpoint_id:
                # Basic identifier
                identifier = f"{resource['type']}:{endpoint_id}"
                
                # Generate more detailed identifier if additional information is available
                if name and vpc_id and service_name:
                    identifier = f"{resource['type']}:{name}:{vpc_id}:{service_name}:{endpoint_id}"
                elif vpc_id and service_name:
                    identifier = f"{resource['type']}:{vpc_id}:{service_name}:{endpoint_id}"
                
                logger.debug(f"Generated identifier: {identifier}")
                return identifier

            logger.warning("Missing endpoint_id for VPC endpoint")
            return None

        except Exception as e:
            logger.error(f"Error generating identifier for VPC endpoint: {str(e)}")
            logger.debug(f"Resource: {resource}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_network/route.py</source>
<document_content>from typing import Dict, List, Any, Optional
import logging
from ..base import ResourceCollector, register_collector

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

@register_collector
class RouteCollector(ResourceCollector):
    """Collector for Route resources"""

    @classmethod
    def get_service_name(cls) -> str:
        return "ec2"

    @classmethod
    def get_resource_types(cls) -> Dict[str, str]:
        return {
            "aws_route": "VPC Routes",
            "aws_vpc_endpoint_route_table_association": "VPC Endpoint Route Table Associations"
        }

    def _sanitize_destination(self, destination: str) -> str:
        """Convert destination string to a safe format"""
        return destination.replace('.', '_').replace('/', '_').replace(':', '_')

    def _get_vpc_endpoint_info(self, vpc_endpoint_id: str) -> Optional[Dict[str, Any]]:
        """Get VPC endpoint information with detailed logging"""
        try:
            logger.debug(f"Fetching VPC endpoint info for {vpc_endpoint_id}")
            response = self.client.describe_vpc_endpoints(
                VpcEndpointIds=[vpc_endpoint_id]
            )
            if response["VpcEndpoints"]:
                endpoint = response["VpcEndpoints"][0]
                endpoint_type = endpoint["VpcEndpointType"]
                service_name = endpoint.get("ServiceName", "").split(".")[-1]
                logger.debug(f"VPC endpoint {vpc_endpoint_id} info: type={endpoint_type}, service={service_name}")
                return endpoint
            logger.warning(f"No VPC endpoint found for {vpc_endpoint_id}")
            return None
        except Exception as e:
            logger.error(f"Error fetching VPC endpoint info for {vpc_endpoint_id}: {str(e)}")
            return None

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources: List[Dict[str, Any]] = []
        try:
            # Get managed resources from state for both resource types
            managed_resources = {}
            if hasattr(self, 'state_reader') and self.state_reader:
                route_resources = self.state_reader.get_managed_resources("aws_route")
                assoc_resources = self.state_reader.get_managed_resources("aws_vpc_endpoint_route_table_association")
                managed_resources.update(route_resources)
                managed_resources.update(assoc_resources)
                logger.debug(f"Found managed resources: {managed_resources}")
            # Get route tables
            paginator = self.client.get_paginator("describe_route_tables")
            for page in paginator.paginate():
                for rt in page["RouteTables"]:
                    route_table_id = rt["RouteTableId"]
                    vpc_id = rt["VpcId"]
                    logger.debug(f"Processing route table: {route_table_id}")

                    # Process routes
                    for route in rt.get("Routes", []):
                        # Skip local routes (automatically managed by AWS)
                        if route.get("GatewayId") == "local":
                            logger.debug(f"Skipping local route in {route_table_id}")
                            continue

                        # Check destination
                        destination: Optional[str] = None
                        if cidr := route.get("DestinationCidrBlock"):
                            destination = cidr
                            logger.debug(f"Found CIDR destination: {cidr}")
                        elif ipv6_cidr := route.get("DestinationIpv6CidrBlock"):
                            destination = ipv6_cidr
                            logger.debug(f"Found IPv6 CIDR destination: {ipv6_cidr}")
                        elif prefix_list_id := route.get("DestinationPrefixListId"):
                            destination = prefix_list_id
                            logger.debug(f"Found Prefix List destination: {prefix_list_id}")

                        # Skip routes without destination
                        if destination is None:
                            logger.debug(f"Skipping route in {route_table_id} with no destination")
                            continue

                        # Generate resource name ID
                        name_id = f"{route_table_id}_{self._sanitize_destination(destination)}"
                        # Generate import ID
                        import_id = f"{route_table_id}_{destination}"

                        # Create base route details
                        route_details = {
                            "route_table_id": route_table_id,
                            "vpc_id": vpc_id,
                            "destination_cidr_block": route.get("DestinationCidrBlock"),
                            "destination_ipv6_cidr_block": route.get("DestinationIpv6CidrBlock"),
                            "destination_prefix_list_id": route.get("DestinationPrefixListId"),
                            "gateway_id": None,
                            "instance_id": route.get("InstanceId"),
                            "nat_gateway_id": route.get("NatGatewayId"),
                            "network_interface_id": route.get("NetworkInterfaceId"),
                            "transit_gateway_id": route.get("TransitGatewayId"),
                            "vpc_peering_connection_id": route.get("VpcPeeringConnectionId"),
                            "vpc_endpoint_id": None,
                            "carrier_gateway_id": route.get("CarrierGatewayId"),
                            "egress_only_gateway_id": route.get("EgressOnlyInternetGatewayId"),
                            "local_gateway_id": route.get("LocalGatewayId")
                        }

                        # Handle gateway types
                        if gateway_id := route.get("GatewayId"):
                            # For prefix list destinations with VPC endpoint
                            if destination.startswith("pl-") and gateway_id.startswith("vpce-"):
                                logger.debug(f"Found VPC endpoint {gateway_id} with prefix list {destination}")
                                
                                # Create aws_route resource with special flag for VPC endpoint association
                                route_details = {
                                    "route_table_id": route_table_id,
                                    "destination_prefix_list_id": destination,
                                    "vpc_endpoint_id": gateway_id,
                                    "is_vpc_endpoint_association": True  # Special flag
                                }
                                # Create route resource
                                resource_key = f"aws_route:{name_id}"
                                is_managed = resource_key in managed_resources
                                logger.debug(f"Checking if route is managed: {resource_key} -> {is_managed}")
                                
                                resource_arn = f"arn:aws:ec2:{self.region}:{self.account_id}:route/{name_id}"
                                route_resource = {
                                    "type": "aws_route",
                                    "id": name_id,
                                    "import_id": import_id,
                                    "arn": resource_arn,
                                    "tags": rt.get("Tags", []),
                                    "details": route_details,
                                    "managed": is_managed
                                }
                                resources.append(route_resource)
                                logger.debug(f"Created route resource with VPC endpoint: {route_resource}")

                                # Create VPC endpoint association resource
                                assoc_id = f"{route_table_id}_{gateway_id}"
                                assoc_key = f"aws_vpc_endpoint_route_table_association:{assoc_id}"
                                assoc_arn = f"arn:aws:ec2:{self.region}:{self.account_id}:vpc-endpoint-rtb-assoc/{assoc_id}"
                                is_assoc_managed = assoc_key in managed_resources
                                logger.debug(f"Checking if association is managed: {assoc_key} -> {is_assoc_managed}")

                                assoc_resource = {
                                    "type": "aws_vpc_endpoint_route_table_association",
                                    "id": assoc_id,
                                    "import_id": f"{gateway_id}/{route_table_id}",
                                    "arn": assoc_arn,
                                    "tags": rt.get("Tags", []),
                                    "details": {
                                        "route_table_id": route_table_id,
                                        "vpc_endpoint_id": gateway_id
                                    },
                                    "managed": is_assoc_managed
                                }
                                resources.append(assoc_resource)
                                logger.debug(f"Created VPC endpoint association resource: {assoc_resource}")
                            else:
                                logger.debug(f"Using gateway_id: {gateway_id}")
                                route_details["gateway_id"] = gateway_id

                        # Create route resource
                        resource_key = f"aws_route:{name_id}"
                        is_managed = resource_key in managed_resources
                        logger.debug(f"Checking if route is managed: {resource_key} -> {is_managed}")
                        
                        resource_arn = f"arn:aws:ec2:{self.region}:{self.account_id}:route/{name_id}"
                        resource = {
                            "type": "aws_route",
                            "id": name_id,
                            "import_id": import_id,
                            "arn": resource_arn,
                            "tags": rt.get("Tags", []),
                            "details": route_details,
                            "managed": is_managed
                        }
                        resources.append(resource)
                        logger.debug(f"Created route resource: {resource}")

        except Exception as e:
            logger.error(f"Error collecting routes: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_network/__init__.py</source>
<document_content># terraform_aws_migrator/collectors/aws_network/__init__.py
"""aws_network サブパッケージ"""
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_network/network.py</source>
<document_content># terraform_aws_migrator/collectors/aws_networking.py

from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector

import logging
import json

logger = logging.getLogger(__name__)


@register_collector
class APIGatewayCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "apigateway"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_api_gateway_rest_api": "API Gateway REST APIs"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # REST APIs
            apis = self.client.get_rest_apis()["items"]
            for api in apis:
                resources.append(
                    {
                        "type": "aws_api_gateway_rest_api",
                        "id": api["id"],
                        "name": api["name"],
                        "arn": f"arn:aws:apigateway:{self.session.region_name}::/restapis/{api['id']}",
                        "tags": api.get("tags", {}),
                    }
                )
        except Exception as e:
            print(f"Error collecting API Gateway resources: {str(e)}")

        return resources


@register_collector
class APIGatewayV2Collector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "apigatewayv2"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_apigatewayv2_api": "API Gateway HTTP/WebSocket APIs"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # HTTP and WebSocket APIs
            apis = self.client.get_apis()["Items"]
            for api in apis:
                resources.append(
                    {
                        "type": "aws_apigatewayv2_api",
                        "id": api["ApiId"],
                        "name": api["Name"],
                        "arn": f"arn:aws:apigateway:{self.session.region_name}::/apis/{api['ApiId']}",
                        "tags": api.get("Tags", {}),
                    }
                )
        except Exception as e:
            print(f"Error collecting API Gateway V2 resources: {str(e)}")

        return resources


@register_collector
class Route53Collector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "route53"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_route53_zone": "Route 53 Hosted Zones"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # Hosted zones
            paginator = self.client.get_paginator("list_hosted_zones")
            for page in paginator.paginate():
                for zone in page["HostedZones"]:
                    tags = self.client.list_tags_for_resource(
                        ResourceType="hostedzone",
                        ResourceId=zone["Id"].replace("/hostedzone/", ""),
                    )["ResourceTagSet"]["Tags"]

                    resources.append(
                        {
                            "type": "aws_route53_zone",
                            "id": zone["Id"],
                            "name": zone["Name"],
                            "tags": tags,
                        }
                    )
        except Exception as e:
            print(f"Error collecting Route53 resources: {str(e)}")

        return resources


@register_collector
class CloudFrontCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "cloudfront"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_cloudfront_distribution": "CloudFront Distributions"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("list_distributions")
            for page in paginator.paginate():
                for dist in page["DistributionList"].get("Items", []):
                    tags = self.client.list_tags_for_resource(Resource=dist["ARN"])[
                        "Tags"
                    ]["Items"]

                    resources.append(
                        {
                            "type": "aws_cloudfront_distribution",
                            "id": dist["Id"],
                            "domain_name": dist["DomainName"],
                            "arn": dist["ARN"],
                            "tags": tags,
                        }
                    )
        except Exception as e:
            print(f"Error collecting CloudFront resources: {str(e)}")

        return resources


@register_collector
class LoadBalancerV2Collector(ResourceCollector):
    """Collector for ALB/NLB and related resources (ELBv2)"""

    @classmethod
    def get_service_name(self) -> str:
        return "elbv2"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_lb": "Application and Network Load Balancers",
            "aws_lb_target_group": "Target Groups for ALB/NLB",
            "aws_lb_listener": "Listeners for ALB/NLB",
            "aws_lb_listener_rule": "Routing rules for ALB listeners",
        }

    @classmethod
    def get_resource_service_mappings(cls) -> Dict[str, str]:
        return {
            "aws_lb_target_group": "elbv2",
            "aws_lb": "elbv2",
            "aws_lb_listener": "elbv2",
            "aws_lb_listener_rule": "elbv2",
        }

    def generate_resource_identifier(self, resource: Dict[str, Any]) -> str:
        """
        Generate a standardized resource identifier for Load Balancer resources

        Args:
            resource: Dictionary containing resource information
        Returns:
            Unique resource identifier
        """
        resource_type = resource.get("type")
        resource_id = resource.get("id")
        module_path = resource.get("module", "")
        
        # Always use ARN if available
        if "arn" in resource:
            return resource["arn"]
        
        # Generate ARN based on resource type if not available
        if resource_type and resource_id:
            if resource_type == "aws_lb":
                # ALBのARNは loadbalancer/app/{name}/{uuid} の形式
                # tfstateのidはロードバランサーの名前を含む
                return f"arn:aws:elasticloadbalancing:{self.region}:{self.account_id}:loadbalancer/app/{resource_id}/{resource.get('details', {}).get('uuid', '1234567890')}"
            elif resource_type == "aws_lb_target_group":
                return f"arn:aws:elasticloadbalancing:{self.region}:{self.account_id}:targetgroup/{resource_id}"
            elif resource_type == "aws_lb_listener":
                lb_arn = resource.get("details", {}).get("load_balancer_arn")
                if lb_arn:
                    return f"{lb_arn}/listener/{resource_id}"
            elif resource_type == "aws_lb_listener_rule":
                listener_arn = resource.get("details", {}).get("listener_arn")
                if listener_arn:
                    return f"{listener_arn}/rule/{resource_id}"
        
        # Fallback to default identifier format
        return f"{resource_type}:{resource_id}" if resource_type and resource_id else resource_id or ""

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # If a specific resource type is requested, collect only that type
            if target_resource_type:
                if target_resource_type == "aws_lb":
                    resources.extend(self._collect_load_balancers())
                elif target_resource_type == "aws_lb_listener":
                    resources.extend(self._collect_listeners())
                elif target_resource_type == "aws_lb_listener_rule":
                    resources.extend(self._collect_listener_rules())
                elif target_resource_type == "aws_lb_target_group":
                    resources.extend(self._collect_target_groups())
            else:
                # Collect all ALB-related resources
                resources.extend(self._collect_load_balancers())
                resources.extend(self._collect_listeners())
                resources.extend(self._collect_listener_rules())
                resources.extend(self._collect_target_groups())

            # テスト用のモックデータがある場合はそれを使用
            if hasattr(self, '_mock_resources'):
                resources.extend(self._mock_resources)

        except Exception as e:
            logger.error(f"Error collecting ALB resources: {str(e)}")
            raise e

        return resources

    def _collect_load_balancers(self) -> List[Dict[str, Any]]:
        """Collect Application Load Balancers"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_load_balancers")
            for page in paginator.paginate():
                for lb in page["LoadBalancers"]:
                    if lb["Type"] == "application":  # Only collect ALBs
                        try:
                            # Get tags
                            tags_response = self.client.describe_tags(
                                ResourceArns=[lb["LoadBalancerArn"]]
                            )
                            tags = (
                                tags_response["TagDescriptions"][0]["Tags"]
                                if tags_response["TagDescriptions"]
                                else []
                            )

                            resources.append({
                                "type": "aws_lb",
                                "id": lb["LoadBalancerName"],
                                "arn": lb["LoadBalancerArn"],
                                "tags": tags,
                                "details": {
                                    "dns_name": lb.get("DNSName"),
                                    "scheme": lb.get("Scheme"),
                                    "vpc_id": lb.get("VpcId"),
                                    "idle_timeout": int(
                                        next(
                                            (attr["Value"] for attr in self.client.describe_load_balancer_attributes(
                                                LoadBalancerArn=lb["LoadBalancerArn"]
                                            )["Attributes"] if attr["Key"] == "idle_timeout.timeout_seconds"),
                                            60  # default value if not found
                                        )
                                    ),
                                    "security_groups": lb.get("SecurityGroups", []),
                                    "subnets": [
                                        az["SubnetId"]
                                        for az in lb.get("AvailabilityZones", [])
                                    ],
                                    "state": lb.get("State", {}).get("Code"),
                                    "ip_address_type": lb.get("IpAddressType"),
                                    # LoadBalancerArnから最後のUUIDを抽出
                                    "uuid": lb["LoadBalancerArn"].split("/")[-1],
                                },
                            })
                        except Exception as e:
                            logger.error(f"Error collecting tags for ALB {lb['LoadBalancerName']}: {str(e)}")

        except Exception as e:
            logger.error(f"Error collecting Application Load Balancers: {str(e)}")

        return resources

    def _collect_target_groups(self) -> List[Dict[str, Any]]:
        """Collect Target Groups and their attachments"""
        resources = []
        try:
            # Collect Target Groups
            paginator = self.client.get_paginator("describe_target_groups")
            for page in paginator.paginate():
                for tg in page["TargetGroups"]:
                    try:
                        # Get tags
                        tags_response = self.client.describe_tags(
                            ResourceArns=[tg["TargetGroupArn"]]
                        )
                        tags = (
                            tags_response["TagDescriptions"][0]["Tags"]
                            if tags_response["TagDescriptions"]
                            else []
                        )
                    except Exception:
                        tags = []

                    # Build health check configuration
                    health_check = None
                    if tg.get("HealthCheckEnabled"):
                        health_check = {
                            "enabled": True,
                            "path": tg.get("HealthCheckPath", "/"),
                            "interval": tg.get("HealthCheckIntervalSeconds"),
                            "timeout": tg.get("HealthCheckTimeoutSeconds"),
                            "healthy_threshold": tg.get("HealthyThresholdCount"),
                            "unhealthy_threshold": tg.get("UnhealthyThresholdCount"),
                            "matcher": tg.get("Matcher", {}).get("HttpCode", "200"),
                        }
                        if tg.get("HealthCheckProtocol"):
                            health_check["protocol"] = tg["HealthCheckProtocol"]
                        if tg.get("HealthCheckPort"):
                            health_check["port"] = tg["HealthCheckPort"]

                    # Build resource details
                    resource = {
                        "type": "aws_lb_target_group",
                        "id": tg["TargetGroupName"],
                        "arn": tg["TargetGroupArn"],
                        "protocol": tg.get("Protocol"),
                        "port": tg.get("Port"),
                        "vpc_id": tg.get("VpcId"),
                        "target_type": tg.get("TargetType"),
                        "tags": tags,
                    }

                    # Add target group attributes
                    try:
                        attrs = self.client.describe_target_group_attributes(
                            TargetGroupArn=tg["TargetGroupArn"]
                        )["Attributes"]

                        for attr in attrs:
                            if attr["Key"] == "deregistration_delay.timeout_seconds":
                                resource["deregistration_delay"] = int(attr["Value"])
                            elif attr["Key"] == "lambda.multi_value_headers.enabled":
                                resource["lambda_multi_value_headers_enabled"] = (
                                    attr["Value"].lower() == "true"
                                )
                            elif attr["Key"] == "proxy_protocol_v2.enabled":
                                resource["proxy_protocol_v2"] = (
                                    attr["Value"].lower() == "true"
                                )
                            elif attr["Key"] == "slow_start.duration_seconds":
                                resource["slow_start"] = int(attr["Value"])
                    except Exception as e:
                        logger.warning(
                            f"Failed to get target group attributes for {tg['TargetGroupName']}: {str(e)}"
                        )

                    # Add health check configuration if any
                    if health_check:
                        resource["health_check"] = health_check

                    resources.append(resource)

        except Exception as e:
            logger.error(f"Error collecting target groups: {str(e)}", exc_info=True)

        return resources


    def _collect_listeners(self) -> List[Dict[str, Any]]:
        resources:List = []
        if not hasattr(self, 'client'):
            logger.error("No ELBv2 client available")
            return resources

        try:
            paginator = self.client.get_paginator("describe_load_balancers")
            lb_pages = paginator.paginate()
        except Exception as e:
            logger.error(f"Failed to create load balancer paginator: {e}")
            raise e

        for page in lb_pages:
            for lb in page.get("LoadBalancers", []):
                lb_arn = lb.get("LoadBalancerArn")
                if not lb_arn:
                    continue

                try:
                    listener_paginator = self.client.get_paginator("describe_listeners")
                    listener_pages = listener_paginator.paginate(LoadBalancerArn=lb_arn)
                except Exception as e:
                    logger.error(f"Failed to get listeners for LB {lb_arn}: {e}")
                    raise e

                for listener_page in listener_pages:
                    for listener in listener_page.get("Listeners", []):
                        listener_arn = listener.get("ListenerArn")
                        if not listener_arn:
                            continue

                        tags = []
                        try:
                            tags_response = self.client.describe_tags(
                                ResourceArns=[listener_arn]
                            )
                            if tags_response.get("TagDescriptions"):
                                tags = tags_response["TagDescriptions"][0].get("Tags", [])
                        except Exception as e:
                            logger.debug(f"Failed to get tags for listener {listener_arn}: {e}")

                        # Create resource with all available information
                        resources.append({
                            "type": "aws_lb_listener",
                            "id": listener_arn.split("/")[-1],
                            "arn": listener_arn,
                            "tags": tags,
                            "details": {
                                "load_balancer_arn": lb_arn,
                                "port": listener.get("Port"),
                                "protocol": listener.get("Protocol"),
                                "ssl_policy": listener.get("SslPolicy"),
                                "certificates": listener.get("Certificates", []),
                                "actions": listener.get("DefaultActions", [])
                            }
                        })
        return resources

    def _collect_listener_rules(self) -> List[Dict[str, Any]]:
        """Collect Rules for ALB Listeners"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_load_balancers")
            for page in paginator.paginate():
                for lb in page["LoadBalancers"]:
                    try:
                        listener_paginator = self.client.get_paginator("describe_listeners")
                        for listener_page in listener_paginator.paginate(LoadBalancerArn=lb["LoadBalancerArn"]):
                            for listener in listener_page["Listeners"]:
                                try:
                                    rules = self.client.describe_rules(
                                        ListenerArn=listener["ListenerArn"]
                                    ).get("Rules", [])
                                    
                                    for rule in rules:
                                        # Skip default rules
                                        if rule.get("IsDefault", False):
                                            continue
                                            
                                        # Get rule tags
                                        try:
                                            tags_response = self.client.describe_tags(
                                                ResourceArns=[rule["RuleArn"]]
                                            )
                                            tags = (
                                                tags_response["TagDescriptions"][0]["Tags"]
                                                if tags_response["TagDescriptions"]
                                                else []
                                            )
                                        except Exception:
                                            tags = []

                                        resources.append({
                                            "type": "aws_lb_listener_rule",
                                            "id": rule["RuleArn"].split("/")[-1],
                                            "arn": rule["RuleArn"],
                                            "tags": tags,
                                            "details": {
                                                "listener_arn": listener["ListenerArn"],
                                                "priority": rule.get("Priority"),
                                                "conditions": rule.get("Conditions", []),
                                                "actions": rule.get("Actions", []),
                                            },
                                        })
                                except Exception as e:
                                    logger.error(f"Error collecting rules for listener {listener['ListenerArn']}: {e}")
                    except Exception as e:
                        logger.error(f"Error collecting rules for LB {lb['LoadBalancerArn']}: {e}")
        except Exception as e:
            logger.error(f"Error collecting listener rules: {e}")
        return resources

@register_collector
class ClassicLoadBalancerCollector(ResourceCollector):
    """Collector for Classic Load Balancers (ELB)"""

    @classmethod
    def get_service_name(self) -> str:
        return "elb"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_elb": "Legacy Load Balancers"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            paginator = self.client.get_paginator("describe_load_balancers")
            for page in paginator.paginate():
                for lb in page["LoadBalancerDescriptions"]:
                    # Get tags
                    try:
                        tags_response = self.client.describe_tags(
                            LoadBalancerNames=[lb["LoadBalancerName"]]
                        )
                        tags = (
                            tags_response["TagDescriptions"][0]["Tags"]
                            if tags_response["TagDescriptions"]
                            else []
                        )
                    except Exception:
                        tags = []

                    resources.append(
                        {
                            "type": "aws_elb",
                            "id": lb["LoadBalancerName"],
                            "arn": f"arn:aws:elasticloadbalancing:{self.session.region_name}:{self.account_id}:loadbalancer/{lb['LoadBalancerName']}",
                            "tags": tags,
                            "details": {
                                "dns_name": lb.get("DNSName"),
                                "scheme": lb.get("Scheme"),
                                "vpc_id": lb.get("VPCId"),
                                "subnets": lb.get("Subnets", []),
                                "security_groups": lb.get("SecurityGroups", []),
                                "instances": [
                                    instance["InstanceId"]
                                    for instance in lb.get("Instances", [])
                                ],
                                "listeners": [
                                    {
                                        "protocol": listener.get("Protocol"),
                                        "load_balancer_port": listener.get(
                                            "LoadBalancerPort"
                                        ),
                                        "instance_protocol": listener.get(
                                            "InstanceProtocol"
                                        ),
                                        "instance_port": listener.get("InstancePort"),
                                        "ssl_certificate_id": listener.get(
                                            "SSLCertificateId"
                                        ),
                                    }
                                    for listener in lb.get("ListenerDescriptions", [])
                                ],
                                "health_check": lb.get("HealthCheck"),
                            },
                        }
                    )

            if self.progress_callback:
                self.progress_callback("elb", "Completed", len(resources))

        except Exception as e:
            if self.progress_callback:
                self.progress_callback("elb", f"Error: {str(e)}", 0)

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/user.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector

import logging

logger = logging.getLogger(__name__)


@register_collector
class IAMUserCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "iam"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_iam_user": "IAM Users",
            "aws_iam_user_policy": "IAM User Policies",
            "aws_iam_user_policy_attachment": "IAM User Policy Attachments",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            if target_resource_type:
                if target_resource_type == "aws_iam_user":
                    resources.extend(self._collect_users())
                elif target_resource_type == "aws_iam_user_policy":
                    resources.extend(self._collect_user_policies())
                elif target_resource_type == "aws_iam_user_policy_attachment":
                    resources.extend(self._collect_user_policy_attachments())
            else:
                resources.extend(self._collect_users())
                resources.extend(self._collect_user_policies())
                resources.extend(self._collect_user_policy_attachments())
        except Exception as e:
            print(f"Error collecting IAM resources: {str(e)}")

        return resources

    def _collect_users(self) -> List[Dict[str, Any]]:
        """Collect IAM users"""
        resources = []
        paginator = self.client.get_paginator("list_users")
        for page in paginator.paginate():
            for user in page["Users"]:
                try:
                    tags = self.client.list_user_tags(UserName=user["UserName"])["Tags"]
                    resources.append(
                        {
                            "type": "aws_iam_user",
                            "id": user["UserName"],
                            "arn": user["Arn"],
                            "tags": tags,
                        }
                    )
                except Exception as e:
                    print(
                        f"Error collecting tags for user {user['UserName']}: {str(e)}"
                    )
        return resources

    def _collect_user_policies(self) -> List[Dict[str, Any]]:
        """Collect inline user policies"""
        resources = []
        user_paginator = self.client.get_paginator("list_users")
        for user_page in user_paginator.paginate():
            for user in user_page["Users"]:
                try:
                    policy_paginator = self.client.get_paginator("list_user_policies")
                    for policy_page in policy_paginator.paginate(
                        UserName=user["UserName"]
                    ):
                        for policy_name in policy_page["PolicyNames"]:
                            # Get the policy document
                            try:
                                policy = self.client.get_user_policy(
                                    UserName=user["UserName"], PolicyName=policy_name
                                )
                                resources.append(
                                    {
                                        "type": "aws_iam_user_policy",
                                        "id": f"{user['UserName']}:{policy_name}",
                                        "user_name": user["UserName"],
                                        "policy_name": policy_name,
                                        "policy_document": policy["PolicyDocument"],
                                    }
                                )
                            except Exception as e:
                                logger.error(
                                    f"Error getting policy document for user {user['UserName']}, "
                                    f"policy {policy_name}: {str(e)}"
                                )
                except Exception as e:
                    logger.error(
                        f"Error collecting inline policies for user {user['UserName']}: {str(e)}"
                    )
        return resources

    def _collect_user_policy_attachments(self) -> List[Dict[str, Any]]:
        """Collect user policy attachments"""
        resources = []
        try:
            # list_users with pagination
            user_paginator = self.client.get_paginator("list_users")
            user_page_num = 0
            for user_page in user_paginator.paginate():
                user_page_num += 1
                logger.debug(
                    f"Processing user page {user_page_num} with {len(user_page['Users'])} users"
                )

                for user in user_page["Users"]:
                    user_name = user["UserName"]
                    try:
                        # list_attached_user_policies with pagination
                        attachment_paginator = self.client.get_paginator(
                            "list_attached_user_policies"
                        )
                        policy_page_num = 0
                        total_policies = 0

                        for attachment_page in attachment_paginator.paginate(
                            UserName=user_name,
                            PaginationConfig={"PageSize": 100, "MaxItems": None},
                        ):
                            policy_page_num += 1
                            policies = attachment_page["AttachedPolicies"]
                            total_policies += len(policies)

                            for policy in policies:
                                try:
                                    resources.append(
                                        {
                                            "type": "aws_iam_user_policy_attachment",
                                            "id": f"{user_name}:{policy['PolicyArn']}",
                                            "user_name": user_name,
                                            "policy_arn": policy["PolicyArn"],
                                        }
                                    )
                                except KeyError as ke:
                                    logger.error(
                                        f"Missing required key in policy data for user {user_name}: {ke}"
                                    )
                                    logger.debug(f"Policy data: {policy}")
                                    continue

                        logger.debug(
                            f"User {user_name}: Processed {policy_page_num} pages, found {total_policies} policies"
                        )

                    except Exception as e:
                        logger.error(
                            f"Error collecting policies for user {user_name}: {str(e)}"
                        )
                        logger.debug(f"Full error: {e}", exc_info=True)

        except Exception as e:
            logger.error(f"Error in user policy attachment collection: {str(e)}")
            logger.debug("Full error trace:", exc_info=True)

        logger.info(f"Collected {len(resources)} total user policy attachments")
        return resources

    # return resources
    # def _collect_user_policy_attachments(self) -> List[Dict[str, Any]]:
    #     """Collect user policy attachments"""
    #     resources = []
    #     user_paginator = self.client.get_paginator("list_users")
    #     for user_page in user_paginator.paginate():
    #         for user in user_page["Users"]:
    #             try:
    #                 attachment_paginator = self.client.get_paginator(
    #                     "list_attached_user_policies"
    #                 )
    #                 for attachment_page in attachment_paginator.paginate(
    #                     UserName=user["UserName"]
    #                 ):
    #                     for policy in attachment_page["AttachedPolicies"]:
    #                         resources.append(
    #                             {
    #                                 "type": "aws_iam_user_policy_attachment",
    #                                 "id": f"{user['UserName']}:{policy['PolicyName']}",
    #                                 "user_name": user["UserName"],
    #                                 "policy_arn": policy["PolicyArn"],
    #                             }
    #                         )
    #             except Exception as e:
    #                 print(
    #                     f"Error collecting policy attachments for user {user['UserName']}: {str(e)}"
    #                 )
    #     return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/instance_profile.py</source>
<document_content># terraform_aws_migrator/collectors/aws_iam/instance_profile.py

from typing import Dict, List, Any, Optional, Set
import concurrent.futures
import logging
from ..base import ResourceCollector, register_collector

logger = logging.getLogger(__name__)


@register_collector
class IAMInstanceProfileCollector(ResourceCollector):
    """Collector for IAM Instance Profiles with caching and parallel processing"""

    def __init__(self, session, progress_callback=None):
        super().__init__(session, progress_callback)
        self._role_details_cache = {}
        self._policy_cache = {}
        self._max_workers = 10

    @classmethod
    def get_service_name(cls) -> str:
        return "iam"

    @classmethod
    def get_resource_types(cls) -> Dict[str, str]:
        return {"aws_iam_instance_profile": "IAM Instance Profiles"}

    def _is_aws_managed_path(self, path: str) -> bool:
        """Check if the path indicates an AWS managed role/profile"""
        aws_managed_paths = {"/aws-service-role/", "/service-role/", "/aws-reserved/"}
        return any(path.startswith(prefix) for prefix in aws_managed_paths)

    def _is_aws_service_principal(self, assume_role_policy: Dict) -> bool:
        """Check if the assume role policy trusts AWS services"""
        try:
            statements = assume_role_policy.get("Statement", [])
            for statement in statements:
                principal = statement.get("Principal", {})
                service = principal.get("Service")
                if service:
                    if isinstance(service, str):
                        return service.endswith(".amazonaws.com")
                    elif isinstance(service, list):
                        return any(s.endswith(".amazonaws.com") for s in service)
        except Exception as e:
            logger.error(f"Error parsing assume role policy: {e}")
        return False

    def _get_attached_policies(self, role_name: str) -> List[Dict[str, Any]]:
        """Get attached policies for a role with caching"""
        cache_key = f"policies_{role_name}"
        if cache_key in self._policy_cache:
            return self._policy_cache[cache_key]

        try:
            attached_policies = []
            paginator = self.client.get_paginator("list_attached_role_policies")
            for page in paginator.paginate(RoleName=role_name):
                attached_policies.extend(page["AttachedPolicies"])

            self._policy_cache[cache_key] = attached_policies
            return attached_policies
        except Exception as e:
            logger.error(f"Error getting attached policies for role {role_name}: {e}")
            return []

    def _get_role_details(self, role_name: str) -> Dict[str, Any]:
        """Get detailed information about a role with caching"""
        if role_name in self._role_details_cache:
            return self._role_details_cache[role_name]

        try:
            role = self.client.get_role(RoleName=role_name)["Role"]
            attached_policies = self._get_attached_policies(role_name)

            details = {
                "Path": role.get("Path", "/"),
                "AssumeRolePolicyDocument": role.get("AssumeRolePolicyDocument", {}),
                "AttachedPolicies": attached_policies,
            }

            self._role_details_cache[role_name] = details
            return details
        except Exception as e:
            logger.error(f"Error getting role details for {role_name}: {e}")
            return {}

    def _is_aws_quick_setup_role(self, role_name: str) -> bool:
        """Check if this is specifically an SSM Quick Setup role"""
        quick_setup_patterns = ["AmazonSSMRoleForInstancesQuickSetup"]
        return any(role_name.startswith(pattern) for pattern in quick_setup_patterns)

    def _is_aws_service_managed_role(
        self, role_details: Dict[str, Any], role_name: str
    ) -> bool:
        """Determine if a role is managed by an AWS service based on multiple criteria"""
        if not role_details:
            return False

        # Check path - this is a strong indicator
        if self._is_aws_managed_path(role_details.get("Path", "/")):
            logger.debug(f"Role {role_name} has AWS managed path")
            return True

        # Check for SSM Quick Setup role
        if role_name == "AmazonSSMRoleForInstancesQuickSetup":
            logger.debug(f"Role {role_name} is an SSM Quick Setup role")
            return True

        # Check assume role policy for AWS service principals
        assume_role_policy = role_details.get("AssumeRolePolicyDocument", {})
        statements = assume_role_policy.get("Statement", [])

        # Count unique service principals
        service_principals = set()
        for statement in statements:
            principal = statement.get("Principal", {})
            service = principal.get("Service")
            if service:
                if isinstance(service, str):
                    service_principals.add(service)
                elif isinstance(service, list):
                    service_principals.update(service)

        # If only EC2 is trusted, this is likely a user-managed role
        if service_principals == {"ec2.amazonaws.com"}:
            return False

        # Get attached policies
        attached_policies = role_details.get("AttachedPolicies", [])
        if not attached_policies:
            return False

        # Exclude roles that only have SSM core policies
        if (
            len(attached_policies) == 1
            and attached_policies[0]["PolicyName"] == "AmazonSSMManagedInstanceCore"
        ):
            return False

        # Check for AWS service management indicators
        aws_service_keywords = {
            "AWSQuickSetup",
            "AWSSystemsManager",
            "aws-service-role",
            "service-role",
        }

        has_service_keyword = any(
            any(keyword in policy["PolicyName"] for keyword in aws_service_keywords)
            for policy in attached_policies
        )

        return has_service_keyword

    def _process_profile(self, profile: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Process a single instance profile"""
        try:
            if self._should_exclude_profile(profile):
                return None

            profile_name = profile["InstanceProfileName"]

            # Get profile tags
            try:
                tags = self.client.list_instance_profile_tags(
                    InstanceProfileName=profile_name
                ).get("Tags", [])
            except Exception as e:
                logger.warning(f"Failed to get tags for profile {profile_name}: {e}")
                tags = []

            return {
                "type": "aws_iam_instance_profile",
                "id": profile_name,
                "arn": profile["Arn"],
                "tags": tags,
                "details": {
                    "path": profile.get("Path", "/"),
                    "create_date": str(profile.get("CreateDate", "")),
                    "role_name": (
                        profile["Roles"][0]["RoleName"]
                        if profile.get("Roles")
                        else None
                    ),
                },
            }
        except Exception as e:
            logger.error(
                f"Error processing profile {profile.get('InstanceProfileName', 'unknown')}: {e}"
            )
            return None

    def _should_exclude_profile(self, profile: Dict[str, Any]) -> bool:
        """Determine if an instance profile should be excluded based on AWS service management"""
        profile_name = profile["InstanceProfileName"]

        # Special case for SSM Quick Setup profile
        if profile_name == "AmazonSSMRoleForInstancesQuickSetup":
            logger.debug(
                f"Excluding profile {profile_name} as it is an SSM Quick Setup profile"
            )
            return True

        # Check profile path
        if self._is_aws_managed_path(profile.get("Path", "/")):
            logger.debug(f"Excluding profile {profile_name} due to AWS managed path")
            return True

        # Check attached role
        roles = profile.get("Roles", [])
        if roles:
            role_name = roles[0]["RoleName"]
            role_details = self._get_role_details(role_name)

            if self._is_aws_service_managed_role(role_details, role_name):
                logger.debug(
                    f"Excluding profile {profile_name} due to AWS service managed role"
                )
                return True

        # By default, include the profile
        logger.debug(f"Including profile {profile_name} as customer-managed")
        return False

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        """Collect IAM Instance Profile resources with parallel processing"""
        resources = []
        profiles_to_process = []

        try:
            # First, collect all profiles
            paginator = self.client.get_paginator("list_instance_profiles")
            for page in paginator.paginate():
                profiles_to_process.extend(page["InstanceProfiles"])

            # Process profiles in parallel
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=self._max_workers
            ) as executor:
                future_to_profile = {
                    executor.submit(self._process_profile, profile): profile
                    for profile in profiles_to_process
                }

                for future in concurrent.futures.as_completed(future_to_profile):
                    profile = future_to_profile[future]
                    try:
                        if result := future.result():
                            resources.append(result)
                    except Exception as e:
                        logger.error(
                            f"Error processing profile {profile.get('InstanceProfileName', 'unknown')}: {e}"
                        )

        except Exception as e:
            logger.error(f"Error collecting IAM instance profile resources: {e}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/policy.py</source>
<document_content># terraform_aws_migrator/collectors/aws_iam/policy.py

from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class IAMPolicyCollector(ResourceCollector):
    """Collector for IAM Policies"""

    @classmethod
    def get_service_name(cls) -> str:
        """Return the AWS service name for this collector"""
        return "iam"

    @classmethod
    def get_resource_types(cls) -> Dict[str, str]:
        """Return supported resource types and their descriptions"""
        return {
            "aws_iam_policy": "IAM Policies",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        """
        Collect IAM policy resources.
        
        Args:
            target_resource_type: Optional specific resource type to collect
            
        Returns:
            List of collected IAM policy resources
        """
        if target_resource_type and target_resource_type != "aws_iam_policy":
            return []

        try:
            return self._collect_customer_managed_policies()
        except Exception as e:
            logger.error(f"Error collecting IAM policy resources: {e}")
            return []

    def _collect_customer_managed_policies(self) -> List[Dict[str, Any]]:
        """Collect customer managed IAM policies"""
        resources = []
        paginator = self.client.get_paginator("list_policies")

        try:
            for page in paginator.paginate(Scope="Local"):
                for policy in page["Policies"]:
                    policy_resource = self._process_single_policy(policy)
                    if policy_resource:
                        resources.append(policy_resource)
        except Exception as e:
            logger.error(f"Error during policy collection: {e}")

        return resources

    def _process_single_policy(self, policy: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process a single IAM policy and format its data
        
        Args:
            policy: Raw policy data from AWS
            
        Returns:
            Formatted policy resource dictionary
        """
        try:
            policy_arn = policy["Arn"]
            policy_version = self._get_policy_version(policy_arn, policy["DefaultVersionId"])
            tags = self._get_policy_tags(policy_arn)

            return {
                "type": "aws_iam_policy",
                "id": policy["PolicyName"],
                "arn": policy_arn,
                "tags": tags,
                "details": {
                    "description": policy.get("Description", ""),
                    "path": policy["Path"],
                    "policy_document": policy_version["Document"] if policy_version else {},
                    "is_attachable": policy["IsAttachable"],
                    "attachment_count": policy.get("AttachmentCount", 0),
                    "create_date": str(policy["CreateDate"]),
                    "update_date": str(policy["UpdateDate"]),
                },
            }
        except Exception as e:
            logger.error(f"Error processing policy {policy.get('PolicyName', 'Unknown')}: {e}")
            return None

    def _get_policy_version(self, policy_arn: str, version_id: str) -> Dict[str, Any]:
        """Get the specified version of an IAM policy"""
        try:
            return self.client.get_policy_version(
                PolicyArn=policy_arn,
                VersionId=version_id
            )["PolicyVersion"]
        except Exception as e:
            logger.error(f"Error getting policy version for {policy_arn}: {e}")
            return {}

    def _get_policy_tags(self, policy_arn: str) -> List[Dict[str, str]]:
        """Get tags for an IAM policy"""
        try:
            return self.client.list_policy_tags(PolicyArn=policy_arn)["Tags"]
        except Exception:
            return []
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/__init__.py</source>
<document_content>from .role import IAMRoleCollector
from .user import IAMUserCollector
from .group import IAMGroupCollector
from .policy import IAMPolicyCollector

__all__ = [
    'IAMRoleCollector',
    'IAMUserCollector',
    'IAMGroupCollector',
    'IAMPolicyCollector'
]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/role.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector

import logging
import traceback
logger = logging.getLogger(__name__)


@register_collector
class IAMRoleCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "iam"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_iam_role": "IAM Roles",
            "aws_iam_role_policy": "IAM Role Policies",
            "aws_iam_role_policy_attachment": "IAM Role Policy Attachments",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            if target_resource_type:
                if target_resource_type == "aws_iam_role":
                    resources.extend(self._collect_roles())
                elif target_resource_type == "aws_iam_role_policy":
                    resources.extend(self._collect_role_policies())
                elif target_resource_type == "aws_iam_role_policy_attachment":
                    resources.extend(self._collect_role_policy_attachments())
            else:
                resources.extend(self._collect_roles())
                resources.extend(self._collect_role_policies())
                resources.extend(self._collect_role_policy_attachments())
        except Exception as e:
            print(f"Error collecting IAM resources: {traceback.format_exc()}")

        return resources

    def _collect_roles(self) -> List[Dict[str, Any]]:
        resources = []
        paginator = self.client.get_paginator("list_roles")
        for page in paginator.paginate():
            for role in page["Roles"]:
                if not any(
                    rule(role["RoleName"]) for rule in self.get_excluded_rules()
                ):
                    try:
                        tags = self.client.list_role_tags(RoleName=role["RoleName"])[
                            "Tags"
                        ]
                        resource_id = role["RoleName"]
                        resources.append(
                            {
                                "type": "aws_iam_role",
                                "id": resource_id,
                                "arn": role["Arn"],
                                "tags": tags,
                                "details": {
                                    "path": role.get("Path"),
                                    "assume_role_policy": role.get(
                                        "AssumeRolePolicyDocument", {}
                                    ),
                                },
                            }
                        )
                    except Exception as e:
                        logger.error(
                            f"Error collecting details for role {role['RoleName']}: {str(e)}"
                        )
        return resources

    def _collect_role_policies(self) -> List[Dict[str, Any]]:
        resources = []
        role_paginator = self.client.get_paginator("list_roles")
        for role_page in role_paginator.paginate():
            for role in role_page["Roles"]:
                if not any(
                    rule(role["RoleName"]) for rule in self.get_excluded_rules()
                ):
                    try:
                        policy_paginator = self.client.get_paginator(
                            "list_role_policies"
                        )
                        for policy_page in policy_paginator.paginate(
                            RoleName=role["RoleName"]
                        ):
                            for policy_name in policy_page["PolicyNames"]:
                                resource_id = f"{role['RoleName']}_{policy_name}"
                                resources.append(
                                    {
                                        "type": "aws_iam_role_policy",
                                        "id": resource_id,
                                        "role_name": role["RoleName"],
                                        "policy_name": policy_name,
                                    }
                                )
                    except Exception as e:
                        logger.error(
                            f"Error collecting inline policies for role {role['RoleName']}: {str(e)}"
                        )
        return resources

    def _collect_role_policy_attachments(self) -> List[Dict[str, Any]]:
        """Collect role policy attachments"""
        resources = []
        role_paginator = self.client.get_paginator("list_roles")
        role_names: List = []
        for role_page in role_paginator.paginate():
            for role in role_page["Roles"]:
                role_names.append(role["RoleName"])

        logger.debug(f"Collecting policy attachments for {len(role_names)} roles")
        # Use STS client to get account ID
        sts_client = self.session.client('sts')
        account_id = sts_client.get_caller_identity()["Account"]
        
        for role_name in role_names:
                if not any(
                    rule(role_name) for rule in self.get_excluded_rules()
                ):
                    try:
                        logger.debug(f"Getting attached policies for role: {role_name}")
                        paginator = self.client.get_paginator("list_attached_role_policies")
                        for page in paginator.paginate(RoleName=role_name):
                            for policy in page["AttachedPolicies"]:
                                attachment = {
                                    "type": "aws_iam_role_policy_attachment",
                                    "id": f"arn:aws:iam::{account_id}:role/{role_name}/{policy['PolicyArn']}",
                                    "role_name": role_name,
                                    "policy_arn": policy["PolicyArn"],
                                }
                                resources.append(attachment)
                                logger.debug(f"Found policy attachment: {attachment['id']}")
                    except Exception as e:
                        logger.error(
                            f"Error collecting policy attachments for role {role_name}: {str(e)}"
                        )

        logger.debug(f"Collected total of {len(resources)} policy attachments")
        return resources

    def get_excluded_rules(self) -> List[callable]:
        """Rules for excluding AWS-managed roles"""
        return [
            lambda x: x.startswith("AWSServiceRole"),
            lambda x: x.startswith("aws-service-role"),
            lambda x: x.startswith("OrganizationAccountAccessRole"),
        ]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/group.py</source>
<document_content># terraform_aws_migrator/collectors/aws_iam/group.py

from typing import Dict, List, Any
from terraform_aws_migrator.collectors.base import ResourceCollector, register_collector
import logging
from ..base import ResourceCollector, register_collector

logger = logging.getLogger(__name__)


@register_collector
class IAMGroupCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "iam"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_iam_group": "IAM Groups",
            "aws_iam_group_policy": "IAM Group Policies",
            "aws_iam_group_policy_attachment": "IAM Group Policy Attachments",
            "aws_iam_group_membership": "IAM Group Memberships",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources: List = []
        try:
            if target_resource_type != "aws_iam_group":
                return resources
            # Collect IAM groups
            paginator = self.client.get_paginator("list_groups")
            for page in paginator.paginate():
                for group in page["Groups"]:
                    try:
                        group_name = group["GroupName"]

                        # Get group members
                        members = self.client.get_group(GroupName=group_name)["Users"]

                        # Get attached policies
                        attached_policies = self.client.list_attached_group_policies(
                            GroupName=group_name
                        )["AttachedPolicies"]

                        # Get inline policies
                        inline_policies = self.client.list_group_policies(
                            GroupName=group_name
                        )["PolicyNames"]

                        inline_policy_documents = {}
                        for policy_name in inline_policies:
                            policy = self.client.get_group_policy(
                                GroupName=group_name, PolicyName=policy_name
                            )
                            inline_policy_documents[policy_name] = policy[
                                "PolicyDocument"
                            ]

                        resources.append(
                            {
                                "type": "aws_iam_group",
                                "id": group_name,
                                "arn": group["Arn"],
                                "details": {
                                    "path": group["Path"],
                                    "members": [user["UserName"] for user in members],
                                    "attached_policies": attached_policies,
                                    "inline_policies": inline_policy_documents,
                                },
                            }
                        )
                    except Exception as e:
                        logger.error(
                            f"Error collecting details for group {group['GroupName']}: {str(e)}"
                        )
                        continue

        except Exception as e:
            logger.error(f"Error collecting IAM group resources: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/formatters/output_formatter.py</source>
<document_content># terraform_aws_migrator/formatters/output_formatter.py

import json
from typing import Dict, List, Any, Tuple
import logging
from ..collectors.base import registry

logger = logging.getLogger(__name__)


def _split_resource_type(resource_type: str) -> Tuple[str, str]:
    """
    Separate module path and resource type

    Args:
        resource_type: Resource type (e.g., 'module.fa_common_modules_stag.aws_instance')

    Returns:
        Tuple[str, str]: (module path, resource type)
        e.g., ('module.fa_common_modules_stag', 'aws_instance')
    """
    parts = resource_type.split('.')
    if len(parts) > 2 and parts[0] == 'module':
        module_path = '.'.join(parts[:-1])
        base_type = parts[-1]
        return module_path, base_type
    return '', resource_type


def format_output(resources: Dict[str, List[Dict[str, Any]]], output_format: str = "text") -> str:
    """
    Format the output of unmanaged AWS resources

    Args:
        resources: Dictionary containing unmanaged resources by service
        output_format: Desired output format ("text" or "json")

    Returns:
        Formatted string containing the unmanaged resources
    """
    try:
        if output_format == "json":
            return json.dumps(resources, indent=2, default=str)

        if not resources:
            return "No unmanaged resources found."

        output = []
        output.append("\nUnmanaged AWS Resources:")
        output.append("=" * 40)

        # Count total unmanaged resources
        total_unmanaged = sum(
            sum(1 for r in res_list if not r.get("managed", False))
            for res_list in resources.values()
        )

        # Resource Summary
        output.append("\nResource Summary:")
        output.append(f"Total Unmanaged Resources: {total_unmanaged}")
        output.append("")

        # Create collectors map for resource type lookups
        collectors: Dict[str, Any] = {
            collector_cls.get_service_name(): collector_cls
            for collector_cls in registry
        }

        # Group resources by type
        resource_counts: Dict[str, List[Dict[str, Any]]] = {}
        for service_name, service_resources in resources.items():
            collector_cls = collectors.get(service_name)
            if not collector_cls:
                continue

            for resource in service_resources:
                resource_type = resource.get("type", "unknown")
                module_path, base_type = _split_resource_type(resource_type)
                
                # Use base resource type as key
                if base_type not in resource_counts:
                    resource_counts[base_type] = {
                        "services": set(),  # Services this resource type belongs to
                        "resources": [],
                        "modules": set()  # Modules where this resource type is used
                    }
                
                # Add service name
                resource_counts[base_type]["services"].add(service_name)
                # Add module path if it exists
                if module_path:
                    resource_counts[base_type]["modules"].add(module_path)
                # Add resource
                resource_counts[base_type]["resources"].append(resource)

        # Resources by Type
        if resource_counts:
            output.append("Resources by Type:")
            for base_type, resource_data in sorted(resource_counts.items()):
                resources_list = resource_data["resources"]
                services = resource_data["services"]
                modules = resource_data["modules"]
                
                # Get resource type display name using collector from any service
                display_name = base_type
                for service_name in services:
                    collector_cls = collectors.get(service_name)
                    if collector_cls:
                        display_name = collector_cls.get_type_display_name(base_type)
                        break
                
                # Count managed state
                managed_count = sum(1 for r in resources_list if r.get("managed", False))
                unmanaged_count = len(resources_list) - managed_count
                
                if unmanaged_count > 0 or managed_count > 0:
                    # Display with module information
                    if modules:
                        module_info = f" (in modules: {', '.join(sorted(modules))})"
                    else:
                        module_info = ""
                    output.append(f"- Found {unmanaged_count} unmanaged, {managed_count} managed {display_name}{module_info}")

        # Detailed Resources Section
        if output_format == "text":
            output.append("\nResource List:")
            for base_type, resource_data in sorted(resource_counts.items()):
                resources_list = resource_data["resources"]
                services = resource_data["services"]
                
                # Get resource type display name using collector from any service
                display_name = base_type
                for service_name in services:
                    collector_cls = collectors.get(service_name)
                    if collector_cls:
                        display_name = collector_cls.get_type_display_name(base_type)
                        break

                # Filter unmanaged resources
                unmanaged_resources = [r for r in resources_list if not r.get("managed", False)]
                if unmanaged_resources:  # Only show resource type if it has unmanaged resources
                    output.append(f"\n{display_name}:")
                    for resource in unmanaged_resources:
                        resource_id = resource.get("id", "N/A")
                        # Try to get name from details or tags
                        name = None
                        details = resource.get("details", {})
                        if details and "Name" in details:
                            name = details["Name"]
                        else:
                            tags = resource.get("tags", {})
                            if isinstance(tags, dict) and "Name" in tags:
                                name = tags["Name"]
                            elif isinstance(tags, list):
                                for tag in tags:
                                    if isinstance(tag, dict) and tag.get("Key") == "Name":
                                        name = tag.get("Value")
                                        break
                        
                        if name:
                            output.append(f"  - {name} ({resource_id})")
                        else:
                            output.append(f"  - {resource_id}")

        return "\n".join(output)

    except Exception as e:
        logger.exception("Error formatting output")
        return f"Error formatting output: {str(e)}"
</document_content>
</document>

<document>
<source>terraform_aws_migrator/formatters/__init__.py</source>
<document_content></document_content>
</document>

<document>
<source>terraform_aws_migrator/utils/resource_utils.py</source>
<document_content># terraform_aws_migrator/utils/resource_utils.py

import importlib
import inspect
from pathlib import Path
from typing import Dict, List, Any
from rich.console import Console

from terraform_aws_migrator.collectors.base import ResourceCollector


def get_collectors_info() -> Dict[str, List[Dict[str, Any]]]:
    """Get information about all available collectors grouped by category"""
    collectors_dir = Path(__file__).parent.parent / "collectors"
    categories = {}

    # Find all collector modules (aws_*.py files)
    for file_path in collectors_dir.glob("aws_*.py"):
        if file_path.name == "aws_base.py":
            continue

        # Get category from filename (aws_compute.py -> Compute)
        category = file_path.stem.replace("aws_", "").title()

        # Import the module
        module_name = f"terraform_aws_migrator.collectors.{file_path.stem}"
        module = importlib.import_module(module_name)

        # Find all collector classes in the module
        collectors = []
        for name, obj in inspect.getmembers(module):
            if (
                inspect.isclass(obj)
                and issubclass(obj, ResourceCollector)
                and obj != ResourceCollector
            ):
                service_name = obj.get_service_name()
                resource_types = obj.get_resource_types()
                collectors.extend(
                    [
                        {
                            "type": resource_type,
                            "description": description,
                            "service": service_name,
                        }
                        for resource_type, description in resource_types.items()
                    ]
                )

        if collectors:
            categories[category] = collectors

    return categories


def show_supported_resources():
    """Display information about supported resource types"""
    console = Console()
    categories = get_collectors_info()

    console.print("\n[bold cyan]Supported AWS Resource Types[/bold cyan]")
    console.print("These resources can be detected by terraform_aws_migrator:\n")

    for category, resources in sorted(categories.items()):
        console.print(f"[bold yellow]{category}[/bold yellow]")
        for resource in sorted(resources, key=lambda x: x["type"]):
            console.print(f"  • {resource['type']}: {resource['description']}")
        console.print("")
</document_content>
</document>

<document>
<source>terraform_aws_migrator/utils/__init__.py</source>
<document_content></document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/__init__.py</source>
<document_content># terraform_aws_migrator/generators/__init__.py

import logging
import pkgutil
import importlib
from pathlib import Path
from .base import HCLGenerator, HCLGeneratorRegistry, register_generator

logger = logging.getLogger(__name__)


def load_generators():
    """Dynamically load all generator modules from all subdirectories"""
    logger.debug("Starting to load generator modules")

    # Get the directory containing the generators
    generators_dir = Path(__file__).parent

    def load_from_directory(directory: Path, package_prefix: str):
        """Recursively load modules from a directory"""
        if not directory.exists():
            return

        # Load modules from current directory
        for module_info in pkgutil.iter_modules([str(directory)]):
            # Skip __init__.py and base.py
            if module_info.name in ['__init__', 'base']:
                continue

            module_name = f"{package_prefix}.{module_info.name}"
            try:
                importlib.import_module(module_name)
                logger.debug(f"Successfully loaded module: {module_name}")
            except Exception as e:
                logger.error(f"Failed to load generator module {module_name}: {str(e)}")

        # Recursively process subdirectories
        for item in directory.iterdir():
            if item.is_dir() and not item.name.startswith('_'):
                subpackage = f"{package_prefix}.{item.name}"
                load_from_directory(item, subpackage)

    # Start loading from the root generators directory
    load_from_directory(generators_dir, __package__)

    # List all registered generators after loading
    registered_types = list(HCLGeneratorRegistry._generators.keys())
    logger.debug(f"Currently registered generator types: {registered_types}")


# Initialize registry and load generators
load_generators()

__all__ = ["HCLGenerator", "HCLGeneratorRegistry", "register_generator"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/base.py</source>
<document_content># terraform_aws_migrator/generators/base.py

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Type, Union
import importlib
import os
import pkgutil
import logging

logger = logging.getLogger(__name__)


class HCLGenerator(ABC):
    """Base class for HCL generators"""

    def __init__(self, module_prefix: Optional[str] = None, state_reader: Optional[Any] = None):
        """
        Initialize the generator

        Args:
            module_prefix (str, optional): Module prefix for import commands
            state_reader (TerraformStateReader, optional): State reader instance
        """
        self.module_prefix = module_prefix
        self.state_reader = state_reader
        self.managed_resources = {}
        if state_reader:
            self.managed_resources = state_reader.get_managed_resources("")

    def is_resource_managed(self, resource_type: str, resource_name: str) -> bool:
        """Check if a resource is managed by Terraform"""
        if not self.state_reader:
            return False

        for resource in self.managed_resources.values():
            if resource.get("type") == resource_type and resource.get("id") == resource_name:
                return True
        return False

    @classmethod
    @abstractmethod
    def resource_type(cls) -> str:
        """Return the resource type this generator handles"""
        pass

    @abstractmethod
    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for the given resource"""
        pass

    def get_import_prefix(self) -> str:
        """
        Get the module prefix for import commands

        Returns:
            str: Module prefix string (e.g., "module.my_module") or empty string
        """
        return f"module.{self.module_prefix}" if self.module_prefix else ""


class HCLGeneratorRegistry:
    """Registry for HCL generators"""

    _generators: Dict[str, Type[HCLGenerator]] = {}
    _initialized = False

    @classmethod
    def register(cls, generator_class: Type[HCLGenerator]) -> Type[HCLGenerator]:
        """Register a generator class"""
        resource_type = generator_class.resource_type()
        cls._generators[resource_type] = generator_class
        return generator_class

    @classmethod
    def get_generator(
        cls, resource_type: str, module_prefix: Optional[str] = None, state_reader: Optional[Any] = None
    ) -> Optional[HCLGenerator]:
        """
        Get a generator instance for the given resource type

        Args:
            resource_type (str): AWS resource type
            module_prefix (str, optional): Module prefix for import commands
            state_reader (TerraformStateReader, optional): State reader instance

        Returns:
            Optional[HCLGenerator]: Generator instance if supported, None otherwise
        """
        if not cls._initialized:
            cls._initialize()

        generator_class = cls._generators.get(resource_type)
        if generator_class:
            return generator_class(module_prefix=module_prefix, state_reader=state_reader)
        return None

    @classmethod
    def is_supported(cls, resource_type: str) -> bool:
        """
        Check if a resource type or category is supported
        Args:
            resource_type (str): Resource type (e.g., aws_s3_bucket) or category (e.g., s3)
        """
        if not cls._initialized:
            cls._initialize()

        # 完全なリソースタイプの場合
        if resource_type in cls._generators:
            return True

        # カテゴリの場合（例：s3）
        # そのカテゴリに属する任意のリソースタイプが登録されているかチェック
        for registered_type in cls._generators.keys():
            if registered_type.startswith(f"aws_{resource_type}_"):
                return True

        return False

    @classmethod
    def get_generators_for_category(cls, category: str) -> Dict[str, Type[HCLGenerator]]:
        """Get all generators for a given category"""
        if not cls._initialized:
            cls._initialize()

        generators = {
            resource_type: generator_class
            for resource_type, generator_class in cls._generators.items()
            if resource_type.startswith(f"aws_{category}_")
        }

        if not generators:
            logger.warning(f"No generators found for category: {category}")
        
        return generators

    @classmethod
    def _initialize(cls) -> None:
        """Initialize the registry by discovering and loading all generators"""
        if cls._initialized:
            logger.debug("Registry already initialized")
            return

        # Get the generators directory path
        generators_dir = os.path.dirname(__file__)

        # Function to recursively load modules from a directory
        def load_modules_from_dir(dir_path: str, package_prefix: str) -> None:
            for item in os.listdir(dir_path):
                item_path = os.path.join(dir_path, item)

                # Skip __pycache__ and files starting with _
                if item.startswith("_") or item == "__pycache__":
                    continue

                if os.path.isdir(item_path):
                    # It's a subdirectory - recurse into it
                    subpackage = f"{package_prefix}.{item}"
                    load_modules_from_dir(item_path, subpackage)

                elif item.endswith(".py"):
                    # It's a Python file - try to import it
                    module_name = (
                        f"{package_prefix}.{item[:-3]}"  # Remove .py extension
                    )
                    try:
                        if (
                            module_name != "terraform_aws_migrator.generators.base"
                        ):  # Skip base.py
                            module = importlib.import_module(module_name)
                            for attr_name in dir(module):
                                attr = getattr(module, attr_name)
                    except Exception as e:
                        logger.error(
                            f"Failed to load generator module {module_name}: {str(e)}"
                        )

        # Load all modules from the generators directory
        load_modules_from_dir(generators_dir, "terraform_aws_migrator.generators")

        if not cls._generators:
            logger.warning("No generators were registered")
        else:
            logger.debug(f"Registered generators: {list(cls._generators.keys())}")

        cls._initialized = True

    @classmethod
    def list_supported_types(cls) -> Dict[str, str]:
        """List all supported resource types"""
        if not cls._initialized:
            cls._initialize()

        return {
            resource_type: generator_class.__doc__ or ""
            for resource_type, generator_class in cls._generators.items()
        }

    @classmethod
    @abstractmethod
    def resource_type(cls) -> str:
        """Return the resource type this generator handles"""
        pass

    @abstractmethod
    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for the given resource"""
        pass

    @abstractmethod
    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate Terraform import command for the given resource"""
        pass


def register_generator(generator_class: Type[HCLGenerator]) -> Type[HCLGenerator]:
    """Decorator to register a generator class"""
    return HCLGeneratorRegistry.register(generator_class)
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_storage/s3.py</source>
<document_content>from typing import Dict, Any, Optional
import logging
import json
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class S3BucketGenerator(HCLGenerator):
    """Generator for aws_s3_bucket resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_s3_bucket"

    def _generate_resource_name(self, bucket_name: str) -> str:
        """Generate a safe resource name from bucket name"""
        return bucket_name.replace("-", "_").replace(".", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                logger.error("Missing required bucket name")
                return None

            resource_name = self._generate_resource_name(bucket_name)
            hcl_blocks = []

            # Main bucket resource
            main_block = [
                f'resource "aws_s3_bucket" "{resource_name}" {{',
                f'  bucket = "{bucket_name}"',
                '  force_destroy = false  # Default to false for safety'
            ]

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                main_block.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        main_block.append(f'    "{key}" = "{value}"')
                main_block.append("  }")

            main_block.append("}")
            return "\n".join(main_block)

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                logger.error("Missing bucket name for import command")
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket: {str(e)}")
            return None

@register_generator
class S3BucketACLGenerator(HCLGenerator):
    """Generator for aws_s3_bucket_acl resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_s3_bucket_acl"

    def _generate_resource_name(self, bucket_name: str) -> str:
        # Convert dots and underscores to hyphens first, then replace hyphens with underscores
        return bucket_name.replace(".", "-").replace("-", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            details = resource.get("details", {})
            if not details:
                return None

            owner = details.get("owner", {})
            grants = details.get("grants", [])

            hcl_blocks = [
                f'resource "aws_s3_bucket_acl" "{resource_name}" {{',
                f'  bucket = "{bucket_name}"',
                '  access_control_policy {',
            ]

            # Add owner block
            if owner:
                hcl_blocks.extend([
                    '    owner {',
                    f'      id = "{owner.get("ID", "")}"',
                    '    }',
                ])

            # Add grants
            if grants:
                for grant in grants:
                    grantee = grant.get("Grantee", {})
                    hcl_blocks.extend([
                        '    grant {',
                        f'      permission = "{grant.get("Permission", "")}"',
                        '',
                        '      grantee {',
                        f'        type = "{grantee.get("Type", "")}"',
                    ])
                    
                    # Add grantee details based on type
                    # display_nameは自動的に設定されるため、明示的に設定しない
                    if grantee.get("ID"):
                        hcl_blocks.append(f'        id = "{grantee.get("ID")}"')
                    if grantee.get("URI"):
                        hcl_blocks.append(f'        uri = "{grantee.get("URI")}"')
                    
                    hcl_blocks.extend([
                        '      }',
                        '    }',
                    ])

            hcl_blocks.extend([
                '  }',
                '}',
            ])

            return "\n".join(hcl_blocks)

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket ACL: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket_acl.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket ACL: {str(e)}")
            return None

@register_generator
class S3BucketPolicyGenerator(HCLGenerator):
    """Generator for aws_s3_bucket_policy resources"""

    @classmethod
    def resource_type(cls) -> str:
        logger.info("Registering S3BucketPolicyGenerator for aws_s3_bucket_policy")
        return "aws_s3_bucket_policy"

    def _generate_resource_name(self, bucket_name: str) -> str:
        name = bucket_name.replace("-", "_").replace(".", "_")
        logger.debug(f"Generated resource name for bucket {bucket_name}: {name}")
        return name

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        logger.info(f"Starting HCL generation for S3 bucket policy: {resource.get('id')}")
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            policy = resource.get("details", {}).get("policy")
            if not policy:
                return None

            # Ensure policy is properly formatted
            if isinstance(policy, str):
                try:
                    # バケットポリシーはすでにJSON文字列として取得されているため、
                    # 一度パースしてから使用します
                    policy_json = json.loads(policy)
                    logger.info(f"Successfully parsed policy for bucket {bucket_name}")
                    logger.debug(f"Policy content: {json.dumps(policy_json, indent=2)}")
                    
                    # ポリシーを整形して出力
                    formatted_policy = json.dumps(policy_json, indent=2)
                    hcl = [
                        f'resource "aws_s3_bucket_policy" "{resource_name}" {{',
                        f'  bucket = "{bucket_name}"',
                        '  policy = jsonencode(',
                        '    ' + formatted_policy.replace('\n', '\n    '),
                        '  )',
                        "}"
                    ]
                    
                    result = "\n".join(hcl)
                    logger.info(f"Generated HCL for bucket policy: {bucket_name}")
                    logger.debug(f"Generated HCL:\n{result}")
                    return result
                    
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON policy string for bucket {bucket_name}: {e}")
                    return None
            else:
                logger.error(f"Policy must be a JSON string, got {type(policy)} for bucket {bucket_name}")
                logger.debug(f"Actual policy content: {policy}")
                return None

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket policy: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket_policy.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket policy: {str(e)}")
            return None

@register_generator
class S3BucketPublicAccessBlockGenerator(HCLGenerator):
    """Generator for aws_s3_bucket_public_access_block resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_s3_bucket_public_access_block"

    def _generate_resource_name(self, bucket_name: str) -> str:
        return bucket_name.replace("-", "_").replace(".", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            public_access = resource.get("details", {})
            if not public_access:
                return None

            return "\n".join([
                f'resource "aws_s3_bucket_public_access_block" "{resource_name}" {{',
                f'  bucket = "{bucket_name}"',
                f'  block_public_acls       = {str(public_access.get("block_public_acls", True)).lower()}',
                f'  block_public_policy     = {str(public_access.get("block_public_policy", True)).lower()}',
                f'  ignore_public_acls      = {str(public_access.get("ignore_public_acls", True)).lower()}',
                f'  restrict_public_buckets = {str(public_access.get("restrict_public_buckets", True)).lower()}',
                "}"
            ])

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket public access block: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket_public_access_block.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket public access block: {str(e)}")
            return None

@register_generator
class S3BucketCORSGenerator(HCLGenerator):
    """Generator for aws_s3_bucket_cors_configuration resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_s3_bucket_cors_configuration"

    def _generate_resource_name(self, bucket_name: str) -> str:
        return bucket_name.replace("-", "_").replace(".", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            cors_rules = resource.get("details", {}).get("cors_rules", [])
            if not cors_rules:
                return None

            cors_block = [
                f'resource "aws_s3_bucket_cors_configuration" "{resource_name}" {{',
                f'  bucket = "{bucket_name}"'
            ]

            for rule in cors_rules:
                cors_block.append("  cors_rule {")
                if allowed_headers := rule.get("AllowedHeaders"):
                    cors_block.append(f'    allowed_headers = {json.dumps(allowed_headers)}')
                if allowed_methods := rule.get("AllowedMethods"):
                    cors_block.append(f'    allowed_methods = {json.dumps(allowed_methods)}')
                if allowed_origins := rule.get("AllowedOrigins"):
                    cors_block.append(f'    allowed_origins = {json.dumps(allowed_origins)}')
                if expose_headers := rule.get("ExposeHeaders"):
                    cors_block.append(f'    expose_headers = {json.dumps(expose_headers)}')
                if max_age_seconds := rule.get("MaxAgeSeconds"):
                    cors_block.append(f'    max_age_seconds = {max_age_seconds}')
                cors_block.append("  }")

            cors_block.append("}")
            return "\n".join(cors_block)

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket CORS configuration: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket_cors_configuration.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket CORS configuration: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/vpc.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/vpc.py

from typing import Dict, Any, Optional, List
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class VPCGenerator(HCLGenerator):
    """Generator for aws_vpc resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_vpc"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from tags or ID"""
        vpc_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = next((tag["Value"] for tag in tags if tag["Key"] == "Name"), None)

        if name_tag:
            return name_tag.replace("-", "_").replace(" ", "_").lower()
        else:
            return f"vpc_{vpc_id.replace('-', '_').lower()}"

    def _format_cidr_blocks(self, cidr_blocks: List[Dict[str, Any]]) -> List[str]:
        """Format CIDR block configurations"""
        formatted = []
        for cidr in cidr_blocks:
            block = []
            if primary := cidr.get("primary", False):
                block.append(f'    primary = {str(primary).lower()}')
            if cidr_block := cidr.get("cidr_block"):
                block.append(f'    cidr_block = "{cidr_block}"')
            if tenant_id := cidr.get("tenant_id"):
                block.append(f'    tenant_id = "{tenant_id}"')
            
            if block:
                formatted.extend(['  cidr_block_association {'] + block + ['  }'])
        return formatted

    def generate(self, resource: Dict[str, Any], include_default: bool = False) -> Optional[str]:
        try:
            vpc_id = resource.get("id")
            details = resource.get("details", {})

            if not vpc_id or not details:
                logger.error(f"Missing required VPC details for VPC {vpc_id}")
                logger.error(f"Resource: {resource}")
                return None

            # Skip default VPC unless specifically requested
            if details.get("is_default", False) and not include_default:
                logger.info(f"Skipping default VPC: {vpc_id}. Use --include-default-vpc flag to include it.")
                return None

            resource_name = self._generate_resource_name(resource)

            # Start building HCL
            hcl = [
                f'resource "aws_vpc" "{resource_name}" {{',
                f'  cidr_block = "{details["cidr_block"]}"'
            ]

            # Add instance tenancy
            instance_tenancy = details.get("instance_tenancy", "default")
            hcl.append(f'  instance_tenancy = "{instance_tenancy}"')

            # Add DNS settings
            enable_dns_support = details.get("enable_dns_support", True)
            enable_dns_hostnames = details.get("enable_dns_hostnames", False)
            hcl.append(f'  enable_dns_support = {str(enable_dns_support).lower()}')
            hcl.append(f'  enable_dns_hostnames = {str(enable_dns_hostnames).lower()}')

            # Add secondary CIDR blocks if present
            if secondary_cidrs := details.get("cidr_block_associations", []):
                for cidr in secondary_cidrs:
                    if not cidr.get("primary", False):  # Skip primary CIDR as it's already added
                        hcl.append(f'  secondary_cidr_blocks = ["{cidr["cidr_block"]}"]')

            # Add IPv6 settings if present
            if ipv6_cidr := details.get("ipv6_cidr_block"):
                hcl.append(f'  assign_generated_ipv6_cidr_block = true')
                hcl.append(f'  ipv6_cidr_block = "{ipv6_cidr}"')

                if ipv6_association_id := details.get("ipv6_association_id"):
                    hcl.append(f'  # IPv6 association ID: {ipv6_association_id}')

            # Add DHCP options association if present
            dhcp_options_id = details.get("dhcp_options_id")

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Add enable_network_address_usage_metrics if present
            if enable_metrics := details.get("enable_network_address_usage_metrics"):
                hcl.append(f'  enable_network_address_usage_metrics = {str(enable_metrics).lower()}')

            # Add Classic Link settings if present
            if enable_classiclink := details.get("enable_classiclink"):
                hcl.append(f'  enable_classiclink = {str(enable_classiclink).lower()}')
            
            if enable_classiclink_dns := details.get("enable_classiclink_dns_support"):
                hcl.append(f'  enable_classiclink_dns_support = {str(enable_classiclink_dns).lower()}')

            # Close resource block
            hcl.append("}")

            # Add any required VPC-specific configurations
            # For example, VPC Flow Logs if enabled
            if flow_logs := details.get("flow_logs", []):
                for i, log in enumerate(flow_logs):
                    hcl.extend([
                        "",
                        f'resource "aws_flow_log" "{resource_name}_flow_log_{i + 1}" {{',
                        f'  vpc_id = aws_vpc.{resource_name}.id',
                        f'  traffic_type = "{log.get("traffic_type", "ALL")}"',
                        f'  log_destination_type = "{log.get("log_destination_type", "cloud-watch-logs")}"'
                    ])

                    if destination := log.get("log_destination"):
                        hcl.append(f'  log_destination = "{destination}"')

                    if format := log.get("log_format"):
                        hcl.append(f'  log_format = "{format}"')

                    hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for VPC: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            vpc_id = resource.get("id")
            if not vpc_id:
                logger.error("Missing VPC ID for import command")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()
            commands = [
                f"terraform import {prefix + '.' if prefix else ''}aws_vpc.{resource_name} {vpc_id}"
            ]

            # Add import commands for VPC Flow Logs if present
            details = resource.get("details", {})
            if flow_logs := details.get("flow_logs", []):
                for i, log in enumerate(flow_logs):
                    if log_id := log.get("id"):
                        commands.append(
                            f"terraform import {prefix + '.' if prefix else ''}"
                            f"aws_flow_log.{resource_name}_flow_log_{i + 1} {log_id}"
                        )

            return "\n".join(commands)

        except Exception as e:
            logger.error(f"Error generating import command for VPC: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/route_table.py</source>
<document_content>from typing import Dict, Any, Optional
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class RouteTableGenerator(HCLGenerator):
    """Generator for aws_route_table resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_route_table"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from tags or ID"""
        route_table_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = next((tag["Value"] for tag in tags if tag["Key"] == "Name"), None)

        if name_tag:
            return name_tag.replace("-", "_").replace(" ", "_").replace("/", "_").lower()
        else:
            return f"rt_{route_table_id.replace('-', '_').lower()}"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            route_table_id = resource.get("id")
            details = resource.get("details", {})

            if not route_table_id or not details:
                logger.error(f"Missing required Route Table details for Route Table {route_table_id}")
                return None

            resource_name = self._generate_resource_name(resource)
            vpc_id = details.get("vpc_id")

            # Start building HCL
            hcl = [
                f'resource "aws_route_table" "{resource_name}" {{',
                f'  vpc_id = "{vpc_id}"'
            ]

            # Add routes
            routes = details.get("routes", [])
            for route in routes:
                # Skip the local route as it's automatically created
                if route.get("gateway_id") == "local":
                    continue

                hcl.append("  route {")
                
                # Add destination
                if cidr := route.get("destination_cidr_block"):
                    hcl.append(f'    cidr_block = "{cidr}"')
                elif ipv6_cidr := route.get("destination_ipv6_cidr_block"):
                    hcl.append(f'    ipv6_cidr_block = "{ipv6_cidr}"')

                # Add target
                if gateway_id := route.get("gateway_id"):
                    hcl.append(f'    gateway_id = "{gateway_id}"')
                if instance_id := route.get("instance_id"):
                    hcl.append(f'    instance_id = "{instance_id}"')
                if nat_gateway_id := route.get("nat_gateway_id"):
                    hcl.append(f'    nat_gateway_id = "{nat_gateway_id}"')
                if network_interface_id := route.get("network_interface_id"):
                    hcl.append(f'    network_interface_id = "{network_interface_id}"')
                if transit_gateway_id := route.get("transit_gateway_id"):
                    hcl.append(f'    transit_gateway_id = "{transit_gateway_id}"')
                if vpc_peering_connection_id := route.get("vpc_peering_connection_id"):
                    hcl.append(f'    vpc_peering_connection_id = "{vpc_peering_connection_id}"')
                if vpc_endpoint_id := route.get("vpc_endpoint_id"):
                    hcl.append(f'    vpc_endpoint_id = "{vpc_endpoint_id}"')
                if carrier_gateway_id := route.get("carrier_gateway_id"):
                    hcl.append(f'    carrier_gateway_id = "{carrier_gateway_id}"')
                if egress_only_gateway_id := route.get("egress_only_gateway_id"):
                    hcl.append(f'    egress_only_gateway_id = "{egress_only_gateway_id}"')
                if local_gateway_id := route.get("local_gateway_id"):
                    hcl.append(f'    local_gateway_id = "{local_gateway_id}"')

                hcl.append("  }")

            # Add propagating_vgws if present
            if vgws := details.get("propagating_vgws", []):
                for vgw in vgws:
                    hcl.append(f'  propagating_vgws = ["{vgw}"]')

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for Route Table: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            route_table_id = resource.get("id")
            if not route_table_id:
                logger.error("Missing Route Table ID for import command")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()
            
            return f"terraform import {prefix + '.' if prefix else ''}aws_route_table.{resource_name} {route_table_id}"

        except Exception as e:
            logger.error(f"Error generating import command for Route Table: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/subnet.py</source>
<document_content>from typing import Dict, Any, Optional
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class SubnetGenerator(HCLGenerator):
    """Generator for aws_subnet resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_subnet"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from tags or ID"""
        subnet_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = next((tag["Value"] for tag in tags if tag["Key"] == "Name"), None)

        if name_tag:
            return name_tag.replace("-", "_").replace(" ", "_").replace("/", "_").lower()
        else:
            return f"subnet_{subnet_id.replace('-', '_').lower()}"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            subnet_id = resource.get("id")
            details = resource.get("details", {})

            if not subnet_id or not details:
                logger.error(f"Missing required Subnet details for Subnet {subnet_id}")
                return None

            resource_name = self._generate_resource_name(resource)
            vpc_id = details.get("vpc_id")

            # Start building HCL
            hcl = [
                f'resource "aws_subnet" "{resource_name}" {{',
                f'  vpc_id = "{vpc_id}"',
                f'  cidr_block = "{details["cidr_block"]}"',
                f'  availability_zone = "{details.get("availability_zone", "")}"'
            ]

            # Add map_public_ip setting
            map_public_ip = details.get("map_public_ip_on_launch", False)
            hcl.append(f'  map_public_ip_on_launch = {str(map_public_ip).lower()}')

            # Add assign_ipv6_address_on_creation if present
            if ipv6_on_creation := details.get("assign_ipv6_address_on_creation"):
                hcl.append(f'  assign_ipv6_address_on_creation = {str(ipv6_on_creation).lower()}')

            # Add ipv6_cidr_block if present
            if ipv6_cidr := details.get("ipv6_cidr_block"):
                hcl.append(f'  ipv6_cidr_block = "{ipv6_cidr}"')

            # Add enable_dns64 if present
            if enable_dns64 := details.get("enable_dns64"):
                hcl.append(f'  enable_dns64 = {str(enable_dns64).lower()}')

            # Add enable_resource_name_dns_aaaa_record_on_launch if present
            if enable_dns_aaaa := details.get("enable_resource_name_dns_aaaa_record_on_launch"):
                hcl.append(f'  enable_resource_name_dns_aaaa_record_on_launch = {str(enable_dns_aaaa).lower()}')

            # Add enable_resource_name_dns_a_record_on_launch if present
            if enable_dns_a := details.get("enable_resource_name_dns_a_record_on_launch"):
                hcl.append(f'  enable_resource_name_dns_a_record_on_launch = {str(enable_dns_a).lower()}')

            # Add private_dns_hostname_type_on_launch if present
            if hostname_type := details.get("private_dns_hostname_type_on_launch"):
                hcl.append(f'  private_dns_hostname_type_on_launch = "{hostname_type}"')

            # Add customer_owned_ipv4_pool if present
            if ipv4_pool := details.get("customer_owned_ipv4_pool"):
                hcl.append(f'  customer_owned_ipv4_pool = "{ipv4_pool}"')

            # Add outpost_arn if present
            if outpost_arn := details.get("outpost_arn"):
                hcl.append(f'  outpost_arn = "{outpost_arn}"')

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for Subnet: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            subnet_id = resource.get("id")
            if not subnet_id:
                logger.error("Missing Subnet ID for import command")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()
            
            return f"terraform import {prefix + '.' if prefix else ''}aws_subnet.{resource_name} {subnet_id}"

        except Exception as e:
            logger.error(f"Error generating import command for Subnet: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/route.py</source>
<document_content>from typing import Dict, Any, Optional
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

@register_generator
class RouteGenerator(HCLGenerator):
    """Generator for aws_route resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_route"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from route details"""
        route_id = resource.get("id", "")
        return f"route_{route_id.lower()}"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            # Skip if resource is managed
            if resource.get("managed", False):
                logger.debug(f"Skipping managed route resource: {resource.get('id')}")
                return None

            details = resource.get("details", {})
            if not details:
                logger.error("Missing required Route details")
                return None

            resource_name = self._generate_resource_name(resource)
            route_table_id = details.get("route_table_id")

            if not route_table_id:
                logger.error("Missing required route_table_id")
                return None

            hcl_blocks = []

            # Generate aws_route resource
            route_hcl = [
                f'resource "aws_route" "{resource_name}" {{',
                f'  route_table_id = "{route_table_id}"'
            ]

            # Add destination (one of these must be specified)
            if cidr := details.get("destination_cidr_block"):
                route_hcl.append(f'  destination_cidr_block = "{cidr}"')
                logger.debug(f"Added CIDR destination: {cidr}")
            elif ipv6_cidr := details.get("destination_ipv6_cidr_block"):
                route_hcl.append(f'  destination_ipv6_cidr_block = "{ipv6_cidr}"')
                logger.debug(f"Added IPv6 CIDR destination: {ipv6_cidr}")
            elif prefix_list_id := details.get("destination_prefix_list_id"):
                route_hcl.append(f'  destination_prefix_list_id = "{prefix_list_id}"')
                logger.debug(f"Added Prefix List destination: {prefix_list_id}")
            else:
                logger.error(f"Missing destination for route in {route_table_id}")
                return None

            # Handle gateway types
            if vpc_endpoint_id := details.get("vpc_endpoint_id"):
                if details.get("is_vpc_endpoint_association"):
                    # Generate aws_vpc_endpoint_route_table_association resource
                    assoc_name = f"vpce_rtb_assoc_{resource_name}"
                    assoc_hcl = [
                        f'resource "aws_vpc_endpoint_route_table_association" "{assoc_name}" {{',
                        f'  route_table_id = "{route_table_id}"',
                        f'  vpc_endpoint_id = "{vpc_endpoint_id}"',
                        "}"
                    ]
                    hcl_blocks.append("\n".join(assoc_hcl))
                    logger.debug(f"Generated VPC endpoint association HCL for {vpc_endpoint_id}")

            if gateway_id := details.get("gateway_id"):
                logger.debug(f"Using gateway_id: {gateway_id}")
                logger.debug(f"Using gateway_id: {gateway_id}")
                route_hcl.append(f'  gateway_id = "{gateway_id}"')
            elif instance_id := details.get("instance_id"):
                route_hcl.append(f'  instance_id = "{instance_id}"')
            elif nat_gateway_id := details.get("nat_gateway_id"):
                route_hcl.append(f'  nat_gateway_id = "{nat_gateway_id}"')
            elif network_interface_id := details.get("network_interface_id"):
                route_hcl.append(f'  network_interface_id = "{network_interface_id}"')
            elif transit_gateway_id := details.get("transit_gateway_id"):
                route_hcl.append(f'  transit_gateway_id = "{transit_gateway_id}"')
            elif vpc_peering_connection_id := details.get("vpc_peering_connection_id"):
                route_hcl.append(f'  vpc_peering_connection_id = "{vpc_peering_connection_id}"')
            elif carrier_gateway_id := details.get("carrier_gateway_id"):
                route_hcl.append(f'  carrier_gateway_id = "{carrier_gateway_id}"')
            elif egress_only_gateway_id := details.get("egress_only_gateway_id"):
                route_hcl.append(f'  egress_only_gateway_id = "{egress_only_gateway_id}"')
            elif local_gateway_id := details.get("local_gateway_id"):
                route_hcl.append(f'  local_gateway_id = "{local_gateway_id}"')

            # Close route resource block
            route_hcl.append("}")
            hcl_blocks.append("\n".join(route_hcl))

            # Return all HCL blocks
            return "\n\n".join(hcl_blocks)

        except Exception as e:
            logger.error(f"Error generating HCL for Route: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate Terraform import command for the route resource"""
        try:
            import_id = resource.get("import_id")
            if not import_id:
                logger.error("Missing Route import_id for import command")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()
            import_commands = []

            # Generate import command for aws_route
            import_commands.append(
                f"terraform import {prefix + '.' if prefix else ''}aws_route.{resource_name} {import_id}"
            )

            # Generate import command for aws_vpc_endpoint_route_table_association if needed
            details = resource.get("details", {})
            if details.get("is_vpc_endpoint_association"):
                assoc_name = f"vpce_rtb_assoc_{resource_name}"
                route_table_id = details.get("route_table_id")
                vpc_endpoint_id = details.get("vpc_endpoint_id")
                if route_table_id and vpc_endpoint_id:
                    assoc_import_id = f"{vpc_endpoint_id}/{route_table_id}"
                    import_commands.append(
                        f"terraform import {prefix + '.' if prefix else ''}aws_vpc_endpoint_route_table_association.{assoc_name} {assoc_import_id}"
                    )
                    logger.debug(f"Generated VPC endpoint association import command for {vpc_endpoint_id}")

            return "\n".join(import_commands)

        except Exception as e:
            logger.error(f"Error generating import command for Route: {str(e)}")
            return None


@register_generator
class VPCEndpointRouteTableAssociationGenerator(HCLGenerator):
    """Generator for aws_vpc_endpoint_route_table_association resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_vpc_endpoint_route_table_association"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from association details"""
        route_id = resource.get("id", "")
        return f"vpce_rtb_assoc_{route_id.lower()}"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            # Skip if resource is managed
            if resource.get("managed", False):
                logger.debug(f"Skipping managed VPC endpoint route table association: {resource.get('id')}")
                return None

            details = resource.get("details", {})
            if not details:
                logger.error("Missing required association details")
                return None

            resource_name = self._generate_resource_name(resource)
            route_table_id = details.get("route_table_id")
            vpc_endpoint_id = details.get("vpc_endpoint_id")

            if not route_table_id or not vpc_endpoint_id:
                logger.error("Missing required route_table_id or vpc_endpoint_id")
                return None

            # Generate HCL
            hcl = [
                f'resource "aws_vpc_endpoint_route_table_association" "{resource_name}" {{',
                f'  route_table_id = "{route_table_id}"',
                f'  vpc_endpoint_id = "{vpc_endpoint_id}"',
                "}"
            ]

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for VPC Endpoint Route Table Association: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate Terraform import command for the association resource"""
        try:
            details = resource.get("details", {})
            route_table_id = details.get("route_table_id")
            vpc_endpoint_id = details.get("vpc_endpoint_id")
            
            if not route_table_id or not vpc_endpoint_id:
                logger.error("Missing required route_table_id or vpc_endpoint_id for import")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()
            import_id = f"{vpc_endpoint_id}/{route_table_id}"
            
            return f"terraform import {prefix + '.' if prefix else ''}aws_vpc_endpoint_route_table_association.{resource_name} {import_id}"

        except Exception as e:
            logger.error(f"Error generating import command: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/__init__.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/__init__.py

import os
import importlib
import logging
from typing import List

logger = logging.getLogger(__name__)

def _load_modules() -> List[str]:
    """
    Dynamically load all Python modules in the current directory.
    Skips __init__.py and files starting with _.
    """
    current_dir = os.path.dirname(__file__)
    loaded_modules = []

    for filename in os.listdir(current_dir):
        if (filename.startswith("_") or 
            not filename.endswith(".py") or 
            filename == "__init__.py"):
            continue

        module_name = filename[:-3]
        full_module_path = f"{__package__}.{module_name}"

        try:
            importlib.import_module(full_module_path)
            loaded_modules.append(module_name)
            logger.debug(f"Successfully loaded module: {full_module_path}")
        except Exception as e:
            logger.error(f"Failed to load module {full_module_path}: {str(e)}")

    return loaded_modules

# Load all modules when this package is imported
loaded_modules = _load_modules()

# Export the names of all loaded modules
__all__ = loaded_modules
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/target_group.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/target_group.py

from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class ALBTargetGroupGenerator(HCLGenerator):
    """Generator for aws_lb_target_group resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lb_target_group"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for target group based on existing resource"""
        try:
            name = resource.get("id")
            if not name:
                return None

            # Start building HCL
            hcl = [
                f'resource "aws_lb_target_group" "{name}" {{',
                f'  name = "{name}"',
            ]

            # Add basic settings
            protocol = resource.get("protocol")
            if protocol:
                hcl.append(f'  protocol = "{protocol}"')

            port = resource.get("port")
            if port:
                hcl.append(f"  port = {port}")

            vpc_id = resource.get("vpc_id")
            if vpc_id:
                hcl.append(f'  vpc_id = "{vpc_id}"')

            target_type = resource.get("target_type")
            if target_type:
                hcl.append(f'  target_type = "{target_type}"')

            # Add health check if enabled
            health_check = resource.get("health_check")
            if health_check:  # health_check exists means it's enabled
                hcl.append("  health_check {")
                for key, value in health_check.items():
                    if value is not None:
                        if isinstance(value, bool):
                            hcl.append(f"    {key} = {str(value).lower()}")
                        elif isinstance(value, (int, float)):
                            hcl.append(f"    {key} = {value}")
                        else:
                            hcl.append(f'    {key} = "{value}"')
                hcl.append("  }")

            # Add target group attributes
            deregistration_delay = resource.get("deregistration_delay")
            if deregistration_delay:
                hcl.append(f"  deregistration_delay = {deregistration_delay}")

            lambda_multi_value_headers = resource.get(
                "lambda_multi_value_headers_enabled", False
            )
            hcl.append(
                f"  lambda_multi_value_headers_enabled = {str(lambda_multi_value_headers).lower()}"
            )

            proxy_protocol_v2 = resource.get("proxy_protocol_v2", False)
            hcl.append(f"  proxy_protocol_v2 = {str(proxy_protocol_v2).lower()}")

            slow_start = resource.get("slow_start", 0)
            hcl.append(f"  slow_start = {slow_start}")

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for target group: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for target group"""
        try:
            tg_arn = resource.get("arn")
            tg_name = resource.get("id")

            if not tg_arn or not tg_name:
                logger.error("Missing ARN or name for target group import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lb_target_group.{tg_name} {tg_arn}"

        except Exception as e:
            logger.error(f"Error generating import command for target group: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/nat_gateway.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/gateways.py

from typing import Dict, Any, Optional, List
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class InternetGatewayGenerator(HCLGenerator):
    """Generator for aws_internet_gateway resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_internet_gateway"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from tags or ID"""
        igw_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = next((tag["Value"] for tag in tags if tag["Key"] == "Name"), None)

        if name_tag:
            return name_tag.replace("-", "_").replace(" ", "_").lower()
        else:
            return f"igw_{igw_id.replace('-', '_').lower()}"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            igw_id = resource.get("id")
            details = resource.get("details", {})

            if not igw_id:
                logger.error("Missing required internet gateway ID")
                return None

            resource_name = self._generate_resource_name(resource)
            
            # Start building HCL
            hcl = [
                f'resource "aws_internet_gateway" "{resource_name}" {{',
            ]

            # Add VPC ID if attached
            vpc_attachments = details.get("vpc_attachments", [])
            if vpc_attachments and vpc_attachments[0].get("state") == "available":
                hcl.append(f'  vpc_id = "{vpc_attachments[0]["vpc_id"]}"')

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for internet gateway: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            igw_id = resource.get("id")
            if not igw_id:
                logger.error("Missing internet gateway ID for import command")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()

            return f"terraform import {prefix + '.' if prefix else ''}aws_internet_gateway.{resource_name} {igw_id}"

        except Exception as e:
            logger.error(f"Error generating import command for internet gateway: {str(e)}")
            return None


@register_generator
class NATGatewayGenerator(HCLGenerator):
    """Generator for aws_nat_gateway resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_nat_gateway"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from tags or ID"""
        nat_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = next((tag["Value"] for tag in tags if tag["Key"] == "Name"), None)

        if name_tag:
            return name_tag.replace("-", "_").replace(" ", "_").lower()
        else:
            return f"nat_{nat_id.replace('-', '_').lower()}"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            nat_id = resource.get("id")
            details = resource.get("details", {})

            if not nat_id or not details:
                logger.error("Missing required NAT gateway details")
                return None

            resource_name = self._generate_resource_name(resource)

            # Start building HCL
            hcl = [
                f'resource "aws_nat_gateway" "{resource_name}" {{',
                f'  subnet_id = "{details["subnet_id"]}"',
            ]

            # Add connectivity type
            connectivity_type = details.get("connectivity_type", "public")
            if connectivity_type != "public":
                hcl.append(f'  connectivity_type = "{connectivity_type}"')

            # Add elastic IP allocation for public NAT gateways
            if connectivity_type == "public":
                # Since we can't determine if the EIP was created by Terraform or not,
                # we'll add a comment suggesting manual verification
                hcl.append("  # Note: You may want to manage the allocation_id separately using aws_eip")
                hcl.append(f'  allocation_id = "{details.get("elastic_ip_allocation_id", "")}"')

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for NAT gateway: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            nat_id = resource.get("id")
            if not nat_id:
                logger.error("Missing NAT gateway ID for import command")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()

            return f"terraform import {prefix + '.' if prefix else ''}aws_nat_gateway.{resource_name} {nat_id}"

        except Exception as e:
            logger.error(f"Error generating import command for NAT gateway: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/listener_rule.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/listener_rule.py

from typing import Dict, Any, Optional, List
import json
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class ALBListenerRuleGenerator(HCLGenerator):
    """Generator for aws_lb_listener_rule resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lb_listener_rule"

    def _format_forward_action(self, action: Dict[str, Any]) -> str:
        """Format forward action with target groups and stickiness"""
        target_groups = action.get("ForwardConfig", {}).get("TargetGroups", [])
        stickiness = action.get("ForwardConfig", {}).get("TargetGroupStickinessConfig", {})
        if not target_groups:
            return ""

        blocks = ['    forward {']
        
        # Add target groups even with weight 0
        for tg in target_groups:
            blocks.extend([
                '      target_group {',
                f'        arn = "{tg.get("TargetGroupArn")}"',
                f'        weight = {tg.get("Weight", 0)}',
                '      }'
            ])

        # Add stickiness if present
        stickiness = action.get("ForwardConfig", {}).get("TargetGroupStickinessConfig", {})
        blocks.extend([
            '      stickiness {',
            f'        enabled = {str(stickiness.get("Enabled", True)).lower()}',
            f'        duration = {stickiness.get("DurationSeconds", stickiness.get("duration", 3600))}',  # Try to get DurationSeconds first, then duration, finally fallback to 3600
            '      }'
        ])

        blocks.append('    }')
        return '\n'.join(blocks)

    def _format_conditions(self, conditions: List[Dict[str, Any]]) -> str:
        """Format conditions block including http_header"""
        condition_blocks = []
        
        for condition in conditions:
            # HTTP Header condition
            if 'HttpHeaderConfig' in condition:
                config = condition['HttpHeaderConfig']
                condition_blocks.extend([
                    '  condition {',
                    '    http_header {',
                    f'      http_header_name = "{config["HttpHeaderName"]}"',
                    f'      values = {json.dumps(config["Values"])}',
                    '    }',
                    '  }'
                ])
            
            # Path Pattern condition
            elif 'PathPatternConfig' in condition:
                config = condition['PathPatternConfig']
                condition_blocks.extend([
                    '  condition {',
                    '    path_pattern {',
                    f'      values = {json.dumps(config["Values"])}',
                    '    }',
                    '  }'
                ])

            # Host Header condition
            elif 'HostHeaderConfig' in condition:
                config = condition['HostHeaderConfig']
                condition_blocks.extend([
                    '  condition {',
                    '    host_header {',
                    f'      values = {json.dumps(config["Values"])}',
                    '    }',
                    '  }'
                ])

        return '\n'.join(condition_blocks)

    def _format_tags(self, tags: List[Dict[str, str]]) -> Optional[str]:
        """Format resource tags"""
        if not tags:
            return None

        tag_blocks = ['  tags = {']
        for tag in tags:
            if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                key = tag["Key"].replace('"', '\\"')
                value = tag["Value"].replace('"', '\\"')
                tag_blocks.append(f'    "{key}" = "{value}"')
        tag_blocks.append('  }')
        return '\n'.join(tag_blocks)

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            rule_id = resource.get('id')
            details = resource.get('details', {})
            tags = resource.get('tags', [])
            listener_arn = details.get('listener_arn')
            priority = details.get('priority')
            actions = details.get('actions', [])
            conditions = details.get('conditions', [])

            if not all([rule_id, listener_arn, priority]):
                logger.error("Missing required fields for listener rule")
                return None

            # Start building HCL
            hcl_blocks = [
                f'resource "aws_lb_listener_rule" "rule_{rule_id}" {{',
                f'  listener_arn = "{listener_arn}"',
                f'  priority     = {priority}'
            ]

            # Add actions
            for action in actions:
                action_type = action.get('Type', '').lower()
                hcl_blocks.append('  action {')
                hcl_blocks.append(f'    type = "{action_type}"')
                
                if action_type == 'forward':
                    forward_config = self._format_forward_action(action)
                    if forward_config:
                        hcl_blocks.append(forward_config)
                
                hcl_blocks.append('  }')

            # Add conditions
            if conditions:
                condition_blocks = self._format_conditions(conditions)
                if condition_blocks:
                    hcl_blocks.append(condition_blocks)

            # Add tags if present
            tags_block = self._format_tags(tags)
            if tags_block:
                hcl_blocks.append(tags_block)

            hcl_blocks.append('}')
            return '\n'.join(hcl_blocks)

        except Exception as e:
            logger.error(f"Error generating HCL for listener rule: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for listener rule"""
        try:
            rule_arn = resource.get('arn')
            rule_id = resource.get('id')

            if not rule_arn or not rule_id:
                logger.error("Missing ARN or ID for listener rule import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lb_listener_rule.rule_{rule_id} {rule_arn}"

        except Exception as e:
            logger.error(f"Error generating import command for listener rule: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/nacl_dhcp.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/nacl_dhcp.py

from typing import Dict, Any, Optional, List
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class NetworkACLGenerator(HCLGenerator):
    """Generator for aws_network_acl resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_network_acl"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from tags or ID"""
        nacl_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = next((tag["Value"] for tag in tags if tag["Key"] == "Name"), None)

        if name_tag:
            return name_tag.replace("-", "_").replace(" ", "_").lower()
        else:
            return f"nacl_{nacl_id.replace('-', '_').lower()}"

    def _format_rule(self, rule: Dict[str, Any], is_egress: bool) -> List[str]:
        """Format a single NACL rule"""
        rule_lines = []
        rule_type = "egress" if is_egress else "ingress"

        rule_lines.append(f"  {rule_type} {{")
        rule_lines.append(f"    protocol = {rule['Protocol']}")
        rule_lines.append(f"    rule_no = {rule['RuleNumber']}")
        rule_lines.append(f"    action = \"{rule['RuleAction'].lower()}\"")

        if "CidrBlock" in rule:
            rule_lines.append(f"    cidr_block = \"{rule['CidrBlock']}\"")
        elif "Ipv6CidrBlock" in rule:
            rule_lines.append(f"    ipv6_cidr_block = \"{rule['Ipv6CidrBlock']}\"")

        if rule["Protocol"] not in ["-1", "all"]:
            from_port = rule.get("PortRange", {}).get("From", 0)
            to_port = rule.get("PortRange", {}).get("To", 0)
            rule_lines.append(f"    from_port = {from_port}")
            rule_lines.append(f"    to_port = {to_port}")

        rule_lines.append("  }")
        return rule_lines

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            nacl_id = resource.get("id")
            details = resource.get("details", {})

            if not nacl_id or not details:
                logger.error("Missing required Network ACL details")
                return None

            # Skip default Network ACLs as they're managed by AWS
            if details.get("is_default", False):
                logger.info(f"Skipping default Network ACL: {nacl_id}")
                return None

            resource_name = self._generate_resource_name(resource)

            # Start building HCL
            hcl = [
                f'resource "aws_network_acl" "{resource_name}" {{',
                f'  vpc_id = "{details["vpc_id"]}"'
            ]

            # Add subnet associations
            if associations := details.get("associations", []):
                subnet_ids = [assoc["SubnetId"] for assoc in associations if "SubnetId" in assoc]
                if subnet_ids:
                    subnet_ids_str = '", "'.join(subnet_ids)
                    hcl.append(f'  subnet_ids = ["{subnet_ids_str}"]')

            # Add ingress rules
            for rule in details.get("ingress_rules", []):
                hcl.extend(self._format_rule(rule, is_egress=False))

            # Add egress rules
            for rule in details.get("egress_rules", []):
                hcl.extend(self._format_rule(rule, is_egress=True))

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for Network ACL: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            nacl_id = resource.get("id")
            if not nacl_id:
                logger.error("Missing Network ACL ID for import command")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()

            return f"terraform import {prefix + '.' if prefix else ''}aws_network_acl.{resource_name} {nacl_id}"

        except Exception as e:
            logger.error(f"Error generating import command for Network ACL: {str(e)}")
            return None


@register_generator
class DHCPOptionsGenerator(HCLGenerator):
    """Generator for aws_vpc_dhcp_options resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_vpc_dhcp_options"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from tags or ID"""
        dhcp_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = next((tag["Value"] for tag in tags if tag["Key"] == "Name"), None)

        if name_tag:
            return name_tag.replace("-", "_").replace(" ", "_").lower()
        else:
            return f"dhcp_{dhcp_id.replace('-', '_').lower()}"

    def _format_list_values(self, values: List[str]) -> str:
        """Format a list of values for HCL"""
        if not values:
            return "[]"
        return '[' + ', '.join(f'"{v}"' for v in values) + ']'

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            dhcp_id = resource.get("id")
            details = resource.get("details", {})

            if not dhcp_id or not details:
                logger.error("Missing required DHCP options details")
                return None

            resource_name = self._generate_resource_name(resource)

            # Start building HCL
            hcl = [
                f'resource "aws_vpc_dhcp_options" "{resource_name}" {{'
            ]

            # Add DHCP options
            if domain_name := details.get("domain_name"):
                hcl.append(f'  domain_name = "{domain_name[0]}"')

            if domain_name_servers := details.get("domain_name_servers"):
                hcl.append(f"  domain_name_servers = {self._format_list_values(domain_name_servers)}")

            if ntp_servers := details.get("ntp_servers"):
                hcl.append(f"  ntp_servers = {self._format_list_values(ntp_servers)}")

            if netbios_name_servers := details.get("netbios_name_servers"):
                hcl.append(f"  netbios_name_servers = {self._format_list_values(netbios_name_servers)}")

            if netbios_node_type := details.get("netbios_node_type"):
                hcl.append(f'  netbios_node_type = "{netbios_node_type[0]}"')

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            # Add VPC association if this DHCP options set is associated with a VPC
            vpc_id = details.get("vpc_id")
            if vpc_id:
                hcl.extend([
                    "",
                    f'resource "aws_vpc_dhcp_options_association" "{resource_name}_association" {{',
                    f'  vpc_id = "{vpc_id}"',
                    f'  dhcp_options_id = aws_vpc_dhcp_options.{resource_name}.id',
                    "}"
                ])

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for DHCP options: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            dhcp_id = resource.get("id")
            if not dhcp_id:
                logger.error("Missing DHCP options ID for import command")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()
            commands = [
                f"terraform import {prefix + '.' if prefix else ''}aws_vpc_dhcp_options.{resource_name} {dhcp_id}"
            ]

            # Add import command for VPC association if present
            if vpc_id := resource.get("details", {}).get("vpc_id"):
                commands.append(
                    f"terraform import {prefix + '.' if prefix else ''}aws_vpc_dhcp_options_association.{resource_name}_association {vpc_id}:{dhcp_id}"
                )

            return "\n".join(commands)

        except Exception as e:
            logger.error(f"Error generating import command for DHCP options: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/vpc_endpoint.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/vpc_endpoint.py

from typing import Dict, Any, Optional, List
import json
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class VPCEndpointGenerator(HCLGenerator):
    """Generator for aws_vpc_endpoint resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_vpc_endpoint"

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from tags or ID"""
        endpoint_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = next((tag["Value"] for tag in tags if tag["Key"] == "Name"), None)

        if name_tag:
            return name_tag.replace("-", "_").replace(" ", "_").lower()
        else:
            # Get service name from endpoint ID and use it in resource name
            service_name = endpoint_id.split(".")[-1] if "." in endpoint_id else "vpce"
            return f"vpce_{service_name}_{endpoint_id[-8:].lower()}"

    def _format_policy(self, policy: Any) -> Optional[str]:
        """Format the endpoint policy as HCL"""
        if not policy:
            return None

        try:
            if isinstance(policy, str):
                policy_json = json.loads(policy)
            else:
                policy_json = policy
            return f'jsonencode({json.dumps(policy_json, indent=2)})'
        except (json.JSONDecodeError, TypeError) as e:
            logger.warning(f"Failed to format endpoint policy: {e}")
            return None

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            endpoint_id = resource.get("id")
            details = resource.get("details", {})

            if not endpoint_id or not details:
                logger.error("Missing required VPC endpoint details")
                return None

            resource_name = self._generate_resource_name(resource)

            # Start building HCL
            hcl = [
                f'resource "aws_vpc_endpoint" "{resource_name}" {{',
                f'  vpc_id = "{details["vpc_id"]}"',
                f'  service_name = "{details["service_name"]}"',
                f'  vpc_endpoint_type = "{details["vpc_endpoint_type"]}"'
            ]

            # Add auto_accept if present
            auto_accept = details.get("auto_accept")
            if auto_accept is not None:
                hcl.append(f'  auto_accept = {str(auto_accept).lower()}')

            # Add type-specific configurations
            endpoint_type = details["vpc_endpoint_type"]
            if endpoint_type == "Interface":
                # Add subnet IDs for Interface endpoints
                if subnet_ids := details.get("subnet_ids"):
                    subnet_ids_str = '", "'.join(subnet_ids)
                    hcl.append(f'  subnet_ids = ["{subnet_ids_str}"]')

                # Add security group IDs for Interface endpoints
                if security_group_ids := details.get("security_group_ids"):
                    sg_ids_str = '", "'.join(security_group_ids)
                    hcl.append(f'  security_group_ids = ["{sg_ids_str}"]')

                # Add private DNS settings
                private_dns_enabled = details.get("private_dns_enabled", False)
                hcl.append(f'  private_dns_enabled = {str(private_dns_enabled).lower()}')

            elif endpoint_type == "Gateway":
                # Add route table IDs for Gateway endpoints
                if route_table_ids := details.get("route_table_ids"):
                    rt_ids_str = '", "'.join(route_table_ids)
                    hcl.append(f'  route_table_ids = ["{rt_ids_str}"]')

            # Add policy if present
            if policy := self._format_policy(details.get("policy")):
                hcl.append(f'  policy = {policy}')

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Add timeouts if needed
            hcl.extend([
                "  timeouts {",
                "    create = \"10m\"",
                "    update = \"10m\"",
                "    delete = \"10m\"",
                "  }"
            ])

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for VPC endpoint: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            endpoint_id = resource.get("id")
            if not endpoint_id:
                logger.error("Missing VPC endpoint ID for import command")
                return None

            resource_name = self._generate_resource_name(resource)
            prefix = self.get_import_prefix()

            return f"terraform import {prefix + '.' if prefix else ''}aws_vpc_endpoint.{resource_name} {endpoint_id}"

        except Exception as e:
            logger.error(f"Error generating import command for VPC endpoint: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/lb.py</source>
<document_content>from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class LoadBalancerGenerator(HCLGenerator):
    """Generator for aws_lb resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lb"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for an Application Load Balancer"""
        try:
            lb_name = resource.get("id")
            details = resource.get("details", {})

            if not lb_name:
                logger.error("Missing required load balancer name")
                return None

            # Start building HCL
            hcl = [
                f'resource "aws_lb" "{lb_name}" {{',
                f'  name = "{lb_name}"',
            ]

            # Add load balancer type
            hcl.append('  load_balancer_type = "application"')

            # Add internal/external scheme
            scheme = details.get("scheme")
            if scheme == "internal":
                hcl.append('  internal = true')
            else:
                hcl.append('  internal = false')

            # Add security groups
            security_groups = details.get("security_groups", [])
            if security_groups:
                groups_str = '", "'.join(security_groups)
                hcl.append(f'  security_groups = ["{groups_str}"]')

            # Add subnets
            subnets = details.get("subnets", [])
            if subnets:
                subnets_str = '", "'.join(subnets)
                hcl.append(f'  subnets = ["{subnets_str}"]')

            # Add IP address type
            ip_address_type = details.get("ip_address_type")
            if ip_address_type:
                hcl.append(f'  ip_address_type = "{ip_address_type.lower()}"')

            # Add idle timeout
            idle_timeout = details.get("idle_timeout")
            if isinstance(idle_timeout, (int, str)) and str(idle_timeout).isdigit():
                hcl.append(f'  idle_timeout = {int(idle_timeout)}')

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close the resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for Load Balancer: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for Load Balancer"""
        try:
            lb_arn = resource.get("arn")
            lb_name = resource.get("id")

            if not lb_arn or not lb_name:
                logger.error("Missing required ARN or name for Load Balancer import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lb.{lb_name} {lb_arn}"

        except Exception as e:
            logger.error(f"Error generating import command for Load Balancer: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/load_balancer_resources.tf</source>
<document_content>resource "aws_lb_listener" "internal_listener" {
  load_balancer_arn = "arn:aws:elasticloadbalancing:ap-northeast-1:123456789012:loadbalancer/app/lambda-whisperdecode-internal/mnopqr345678"
  port              = 80
  protocol          = "HTTP"

  default_action {
    type = "fixed-response"
    fixed_response {
      content_type = "text/plain"
      message_body = "OK"
      status_code  = "200"
    }
  }
}

resource "aws_lb_target_group" "internal_target_group" {
  name     = "internal-target-group"
  port     = 80
  protocol = "HTTP"
  vpc_id   = "vpc-87654321"

  health_check {
    path                = "/health"
    protocol            = "HTTP"
    interval            = 30
    timeout             = 5
    healthy_threshold   = 3
    unhealthy_threshold = 3
    matcher             = "200-299"
  }
}
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/listener.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/listener.py

from typing import Dict, Any, List, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

class ListenerConfigError(Exception):
    """Exception raised for missing listener configuration"""
    pass

@register_generator
class ALBListenerGenerator(HCLGenerator):
    """Generator for aws_lb_listener resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lb_listener"

    def _format_certificates(self, certificates: list) -> str:
        """Format SSL certificate configuration block"""
        if not certificates:
            return ""
        
        cert_blocks = []
        for cert in certificates:
            if not cert.get("CertificateArn"):
                continue
            cert_block = [
                "  certificate {",
                f'    certificate_arn = "{cert["CertificateArn"]}"'
            ]
            if cert.get("IsDefault"):
                cert_block.append("    is_default = true")
            cert_block.append("  }")
            cert_blocks.append("\n".join(cert_block))
        
        return "\n".join(cert_blocks)

    def _format_forward_config(self, action: Dict[str, Any]) -> List[str]:
        """Format forward action configuration"""
        config = []
        target_group_arn = None
        
        # Get target group ARN and prepare target groups
        if "TargetGroupArn" in action:
            target_group_arn = action["TargetGroupArn"]
            target_groups = [{
                "TargetGroupArn": target_group_arn,
                "Weight": action.get("Weight", 1)
            }]
        else:
            forward_config = action.get("ForwardConfig", {})
            target_groups = forward_config.get("TargetGroups", [])
            if target_groups and len(target_groups) == 1:
                target_group_arn = target_groups[0]["TargetGroupArn"]

        # Add target_group_arn setting
        if target_group_arn:
            config.append(f'    target_group_arn = "{target_group_arn}"')

        # Add forward block
        lines = ["forward {"]
        
        # Format target groups
        for tg in target_groups:
            target_group_config = [
                "  target_group {",
                f'    arn    = "{tg["TargetGroupArn"]}"'
            ]
            
            # Use weight from the configuration if available
            if "Weight" in tg:
                target_group_config.append(f'    weight = {tg["Weight"]}')
            
            target_group_config.append("  }")
            lines.extend(target_group_config)

        # Add stickiness configuration from actual settings
        stickiness = action.get("ForwardConfig", {}).get("TargetGroupStickinessConfig", {})
        duration_seconds = stickiness.get("DurationSeconds", 1)  # Fallback to 1 if not set
        
        # Stickiness block is always required with both enabled and duration
        lines.extend([
            "  stickiness {",
            f'    enabled  = {str(stickiness.get("Enabled", False)).lower()}',
            f'    duration = {duration_seconds}',
            "  }"
        ])

        lines.append("}")
        config.append("    " + "\n    ".join(lines))
        
        return config

    def _format_fixed_response_config(self, action: Dict[str, Any]) -> List[str]:
        """Format fixed response action configuration"""
        fixed_response_config = action.get("FixedResponseConfig", {})
        if not fixed_response_config:
            return []

        lines = ["    fixed_response {"]
        
        if "ContentType" in fixed_response_config:
            lines.append(f'      content_type = "{fixed_response_config["ContentType"]}"')
        if "MessageBody" in fixed_response_config:
            lines.append(f'      message_body = "{fixed_response_config["MessageBody"]}"')
        if "StatusCode" in fixed_response_config:
            lines.append(f'      status_code  = "{fixed_response_config["StatusCode"]}"')

        lines.append("    }")
        return lines

    def _format_redirect_config(self, action: Dict[str, Any]) -> List[str]:
        """Format redirect action configuration"""
        redirect_config = action.get("RedirectConfig", {})
        if not redirect_config:
            return []

        lines = ["    redirect {"]
        
        param_mapping = {
            "Host": "host",
            "Path": "path",
            "Port": "port",
            "Protocol": "protocol",
            "Query": "query",
            "StatusCode": "status_code"
        }

        for aws_param, tf_param in param_mapping.items():
            if aws_param in redirect_config:
                value = redirect_config[aws_param]
                lines.append(f'      {tf_param} = "{value}"')

        lines.append("    }")
        return lines

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for an ALB listener"""
        try:
            listener_id = resource.get("id")
            details = resource.get("details", {})
            
            if not listener_id or not details:
                raise ListenerConfigError("Missing required listener details")

            # Start building HCL
            hcl = [
                f'resource "aws_lb_listener" "listener_{listener_id}" {{',
                f'  load_balancer_arn = "{details.get("load_balancer_arn")}"',
                f'  port              = {details.get("port", 80)}',
                f'  protocol          = "{details.get("protocol", "HTTP")}"'
            ]

            # Add SSL policy for HTTPS
            if details.get("protocol") == "HTTPS":
                ssl_policy = details.get("ssl_policy")
                if ssl_policy:
                    hcl.append(f'  ssl_policy = "{ssl_policy}"')

                # Add certificate configuration
                certificates = details.get("certificates", [])
                cert_blocks = self._format_certificates(certificates)
                if cert_blocks:
                    hcl.append(cert_blocks)

            # Handle default action
            actions = details.get("actions", [])
            if not actions:
                raise ListenerConfigError(f"No default_action found for listener {listener_id}")

            default_action = actions[0]  # First action is the default action
            action_type = default_action.get("Type", "").lower()
            
            # Add default action block
            hcl.append("  default_action {")
            hcl.append(f'    type = "{action_type}"')
            
            # Format action configuration based on type
            if action_type == "forward":
                config = self._format_forward_config(default_action)
            elif action_type == "fixed-response":
                config = self._format_fixed_response_config(default_action)
            elif action_type == "redirect":
                config = self._format_redirect_config(default_action)
            else:
                config = []

            hcl.extend(config)
            
            # Close default_action block
            hcl.append("  }")

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for ALB listener: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for ALB listener"""
        try:
            listener_arn = resource.get("arn")
            listener_id = resource.get("id")

            if not listener_arn or not listener_id:
                raise ListenerConfigError("Missing ARN or ID for listener import command")

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lb_listener.listener_{listener_id} {listener_arn}"

        except Exception as e:
            logger.error(f"Error generating import command for ALB listener: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/user.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/user.py

from typing import Dict, Any, Optional, List
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMUserGenerator(HCLGenerator):
    """Generator for aws_iam_user resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_user"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("id")
            details = resource.get("details", {})

            # Start building HCL
            hcl = [
                f'resource "aws_iam_user" "{user_name}" {{',
                f'  name = "{user_name}"',
            ]

            # Add path if not default
            path = details.get("path", "/")
            if path != "/":
                hcl.append(f'  path = "{path}"')

            # Add permissions boundary if present
            permissions_boundary = details.get("permissions_boundary")
            if permissions_boundary:
                hcl.append(f'  permissions_boundary = "{permissions_boundary}"')

            # Add force_destroy if specified
            force_destroy = details.get("force_destroy", False)
            if force_destroy:
                hcl.append("  force_destroy = true")

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM user: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for IAM user"""
        try:
            user_name = resource.get("id")
            if not user_name:
                logger.error("Missing user name for import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_iam_user.{user_name} {user_name}"

        except Exception as e:
            logger.error(f"Error generating import command for IAM user: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/instance_profile.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/instance_profile.py

from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class IAMInstanceProfileGenerator(HCLGenerator):
    """Generator for aws_iam_instance_profile resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_instance_profile"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for an IAM Instance Profile"""
        try:
            profile_name = resource.get("id")
            details = resource.get("details", {})

            if not profile_name:
                logger.error("Missing required instance profile name")
                return None

            # Start building HCL
            hcl = [
                f'resource "aws_iam_instance_profile" "{profile_name}" {{',
                f'  name = "{profile_name}"'
            ]

            # Add path if not default
            path = details.get("path")
            if path and path != "/":
                hcl.append(f'  path = "{path}"')

            # Add role if present
            role_name = details.get("role_name")
            if role_name:
                hcl.append(f'  role = "{role_name}"')

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM instance profile: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for IAM Instance Profile"""
        try:
            profile_name = resource.get("id")
            if not profile_name:
                logger.error("Missing instance profile name for import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_iam_instance_profile.{profile_name} {profile_name}"

        except Exception as e:
            logger.error(f"Error generating import command for IAM instance profile: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/policy.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/policy.py

from typing import Dict, Any, Optional
import json
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMPolicyGenerator(HCLGenerator):
    """Generator for aws_iam_policy resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_policy"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            policy_name = resource.get("id")
            details = resource.get("details", {})
            policy_document = details.get("policy_document", {})

            if not policy_name or not policy_document:
                logger.error("Missing required fields for IAM policy generation")
                return None

            # Start building HCL
            hcl = [
                f'resource "aws_iam_policy" "{policy_name}" {{',
                f'  name = "{policy_name}"',
            ]

            # Add description if present
            description = details.get("description")
            if description:
                hcl.append(f'  description = "{description}"')

            # Add path if not default
            path = details.get("path", "/")
            if path != "/":
                hcl.append(f'  path = "{path}"')

            # Add policy document
            hcl.append(
                f"  policy = jsonencode({json.dumps(policy_document, indent=2)})"
            )

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM policy: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate terraform import command for IAM policy"""
        try:
            arn = resource.get("arn")
            policy_name = resource.get("id")

            if not arn or not policy_name:
                logger.error("Missing ARN or policy name for import command generation")
                return None

            prefix = self.get_import_prefix()
            return (
                f"terraform import {prefix + '.' if prefix else ''}"
                f"aws_iam_policy.{policy_name} {arn}"
            )

        except Exception as e:
            logger.error(f"Error generating import command for IAM policy: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/__init__.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/__init__.py

import os
import importlib
import logging
from typing import List

logger = logging.getLogger(__name__)


def _load_modules() -> List[str]:
    """Dynamically load all Python modules in the current directory"""
    current_dir = os.path.dirname(__file__)
    loaded_modules = []

    for filename in os.listdir(current_dir):
        if (
            filename.startswith("_")
            or not filename.endswith(".py")
            or filename == "__init__.py"
        ):
            continue

        module_name = filename[:-3]
        full_module_path = f"{__package__}.{module_name}"

        try:
            importlib.import_module(full_module_path)
            loaded_modules.append(module_name)
        except Exception as e:
            logger.error(
                f"Failed to load IAM generator module {full_module_path}: {str(e)}"
            )

    return loaded_modules


# Load all modules when this package is imported
loaded_modules = _load_modules()

# Export loaded module names
__all__ = loaded_modules
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/user_policy_attachment.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/user_policy_attachment.py

from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMUserPolicyAttachmentGenerator(HCLGenerator):
    """Generator for aws_iam_user_policy_attachment resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_user_policy_attachment"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("user_name")
            policy_arn = resource.get("policy_arn")

            if not all([user_name, policy_arn]):
                logger.error(
                    "Missing required fields for user policy attachment generation"
                )
                return None

            # Create unique resource name from user name and policy name
            policy_name = policy_arn.split("/")[-1].replace("-", "_")
            resource_name = f"{user_name}_{policy_name}"

            # Generate HCL
            hcl = [
                f'resource "aws_iam_user_policy_attachment" "{resource_name}" {{',
                f'  user       = "{user_name}"',
                f'  policy_arn = "{policy_arn}"',
                "}",
            ]

            return "\n".join(hcl)

        except Exception as e:
            logger.error(
                f"Error generating HCL for IAM user policy attachment: {str(e)}"
            )
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("user_name")
            policy_arn = resource.get("policy_arn")

            if not all([user_name, policy_arn]):
                logger.error(
                    "Missing required fields for user policy attachment import command"
                )
                return None

            # Create the same resource name used in generate()
            policy_name = policy_arn.split("/")[-1].replace("-", "_")
            resource_name = f"{user_name}_{policy_name}"

            # Import identifier format: username/policy_arn
            import_id = f"{user_name}/{policy_arn}"

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_iam_user_policy_attachment.{resource_name} {import_id}"

        except Exception as e:
            logger.error(
                f"Error generating import command for IAM user policy attachment: {str(e)}"
            )
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/role.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/role.py

from typing import Dict, Any, Optional
import json
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMRoleGenerator(HCLGenerator):
    """Generator for aws_iam_role resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_role"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            role_name = resource.get("id")
            details = resource.get("details", {})

            # Buffer to store all HCL blocks
            hcl_blocks = []

            # Generate main role HCL
            if resource["type"] == "aws_iam_role":
                assume_role_policy = details.get("assume_role_policy", {})
                description = details.get("description", "")
                path = details.get("path", "/")

                role_hcl = [
                    f'resource "aws_iam_role" "{role_name}" {{',
                    f'  name = "{role_name}"',
                ]

                if description:
                    role_hcl.append(f'  description = "{description}"')

                if path != "/":
                    role_hcl.append(f'  path = "{path}"')

                role_hcl.append(
                    f"  assume_role_policy = jsonencode({json.dumps(assume_role_policy, indent=2)})"
                )

                # Add tags if present
                tags = resource.get("tags", [])
                if tags:
                    role_hcl.append("  tags = {")
                    for tag in tags:
                        key = tag.get("Key", "").replace('"', '\\"')
                        value = tag.get("Value", "").replace('"', '\\"')
                        role_hcl.append(f'    "{key}" = "{value}"')
                    role_hcl.append("  }")

                role_hcl.append("}")
                hcl_blocks.append("\n".join(role_hcl))

            return "\n\n".join(hcl_blocks)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM role: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            role_name = resource.get("id")
            prefix = self.get_import_prefix()
            if not role_name:
                logger.error("Missing role name for import command generation")
                return None

            return f"terraform import {prefix + '.' if prefix else ''}" \
                f"aws_iam_role.{role_name} {role_name}"

        except Exception as e:
            logger.error(f"Error generating import command for IAM role: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/role_policy_attachment.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/role_policy_attachment.py

from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMRolePolicyAttachmentGenerator(HCLGenerator):
    """Generator for aws_iam_role_policy_attachment resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_role_policy_attachment"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            role_name = resource.get("role_name")
            policy_arn = resource.get("policy_arn")

            if not role_name or not policy_arn:
                logger.error("Missing required fields for role policy attachment")
                return None

            # Create unique resource name
            policy_name = policy_arn.split("/")[-1].replace("-", "_")
            resource_name = f"{role_name}_{policy_name}"

            # Generate HCL
            hcl = [
                f'resource "aws_iam_role_policy_attachment" "{resource_name}" {{',
                f'  role       = "{role_name}"',
                f'  policy_arn = "{policy_arn}"',
                "}",
            ]

            return "\n".join(hcl)

        except Exception as e:
            logger.error(
                f"Error generating HCL for IAM role policy attachment: {str(e)}"
            )
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            role_name = resource.get("role_name")
            policy_arn = resource.get("policy_arn")

            if not role_name or not policy_arn:
                logger.error(
                    "Missing required fields for role policy attachment import command"
                )
                return None

            policy_name = policy_arn.split("/")[-1].replace("-", "_")
            resource_name = f"{role_name}_{policy_name}"

            import_id = f"{role_name}/{policy_arn}"
            prefix = ""
            return f"terraform import {prefix}.aws_iam_role_policy_attachment.{resource_name} {import_id}"

        except Exception as e:
            logger.error(
                f"Error generating import command for IAM role policy attachment: {str(e)}"
            )
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/user_policy.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/user_policy.py

from typing import Dict, Any, Optional
import json
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMUserPolicyGenerator(HCLGenerator):
    """Generator for aws_iam_user_policy resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_user_policy"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("user_name")
            policy_name = resource.get("policy_name")
            policy_document = resource.get("policy_document", {})

            if not all([user_name, policy_name, policy_document]):
                logger.error("Missing required fields for user policy generation")
                return None

            # Create unique resource identifier
            resource_id = f"{user_name}_{policy_name}".replace("-", "_")

            # Generate HCL
            hcl = [
                f'resource "aws_iam_user_policy" "{resource_id}" {{',
                f'  name = "{policy_name}"',
                f'  user = "{user_name}"',
                "",
                f"  policy = jsonencode({json.dumps(policy_document, indent=2)})",
                "}",
            ]

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM user policy: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("user_name")
            policy_name = resource.get("policy_name")

            if not all([user_name, policy_name]):
                logger.error("Missing required fields for user policy import command")
                return None

            # Create resource identifier matching the one in generate()
            resource_id = f"{user_name}_{policy_name}".replace("-", "_")

            # Import identifier format: username:policyname
            import_id = f"{user_name}:{policy_name}"

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_iam_user_policy.{resource_id} {import_id}"

        except Exception as e:
            logger.error(
                f"Error generating import command for IAM user policy: {str(e)}"
            )
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_compute/lambda.py</source>
<document_content># terraform_aws_migrator/generators/aws_compute/lambda.py

from typing import Dict, Any, Optional, List, Tuple, Union
import logging
import json
import base64
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class LambdaFunctionGenerator(HCLGenerator):
    """Generator for aws_lambda_function resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lambda_function"

    def _get_name_from_tags(self, tags: Dict[str, str]) -> Optional[str]:
        """Get Name tag value from tags dictionary"""
        if isinstance(tags, dict):
            return tags.get("Name")
        return None

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from function name"""
        function_name = resource.get("id", "")
        return function_name.replace("-", "_").replace(".", "_")

    def _get_lambda_code(self, code_location: Dict[str, Any]) -> Optional[str]:
        """Retrieve Lambda function code from code location"""
        if not code_location:
            return None

        try:
            if "ZipFile" in code_location:
                return base64.b64decode(code_location["ZipFile"]).decode('utf-8')
            return None
        except Exception as e:
            logger.error(f"Error retrieving Lambda code: {str(e)}")
            return None

    def _get_source_config(self, details: Dict[str, Any], resource_name: str) -> Tuple[List[str], Optional[Dict[str, str]]]:
        """Determine and format the appropriate source configuration"""
        package_type = details.get("package_type", "Zip")
        lines = []
        file_content = None

        if package_type == "Image":
            # For container images
            if image_uri := details.get("image_uri"):
                lines.append(f'  image_uri = "{image_uri}"')
            lines.append('  package_type = "Image"')
            return lines, None

        # For zip packages
        code_location = details.get("code", {})
        if s3_bucket := code_location.get("s3_bucket"):
            # S3 source
            lines.append(f'  s3_bucket = "{s3_bucket}"')
            if s3_key := code_location.get("s3_key"):
                lines.append(f'  s3_key = "{s3_key}"')
            if s3_object_version := code_location.get("s3_object_version"):
                lines.append(f'  s3_object_version = "{s3_object_version}"')
        else:
            # Try to get inline code
            if code := self._get_lambda_code(code_location):
                file_content = {
                    "index.py": code  # or index.js, depending on runtime
                }
                # Add archive configuration
                lines.extend([
                    '  filename = "${' + f'data.archive_file.{resource_name}_lambda.output_path' + '}"',
                    '  source_code_hash = "${' + f'data.archive_file.{resource_name}_lambda.output_base64sha256' + '}"'
                    f'  source_code_hash = data.archive_file.{resource_name}.output_base64sha256'
                ])

        return lines, file_content

    def _format_function_layers(self, layers: List[str]) -> str:
        """Format Lambda layers configuration"""
        if not layers:
            return ""
        layer_arns = '", "'.join(layers)
        return f'  layers = ["{layer_arns}"]'

    def _format_image_config(self, image_config: Dict[str, Any]) -> List[str]:
        """Format container image configuration"""
        if not image_config:
            return []

        lines = ["  image_config {"]
        
        if command := image_config.get("command"):
            commands = ", ".join(f'"{cmd}"' for cmd in command)
            lines.append(f"    command = [{commands}]")
            
        if entry_point := image_config.get("entry_point"):
            entry_points = ", ".join(f'"{ep}"' for ep in entry_point)
            lines.append(f"    entry_point = [{entry_points}]")
            
        if working_directory := image_config.get("working_directory"):
            lines.append(f'    working_directory = "{working_directory}"')

        lines.append("  }")
        return lines

    def _get_main_file_name(self, handler: Optional[str], runtime: str) -> str:
        """Get the main file name from handler and runtime"""
        # ハンドラーが有効な場合、そこからファイル名を抽出
        if handler and '.' in handler:
            file_base = handler.split('.')[0]
        else:
            # ハンドラーが無効な場合はデフォルトのファイル名を使用
            file_base = "index"
        
        # ランタイムに基づいて適切な拡張子を追加
        if runtime and "python" in runtime.lower():
            return f"{file_base}.py"
        elif runtime and "node" in runtime.lower():
            return f"{file_base}.js"
        else:
            # デフォルトはPython
            return f"{file_base}.py"

    def _generate_archive_file(self, resource_name: str, files: Dict[str, str], runtime: str, handler: Optional[str] = None) -> List[str]:
        """Generate archive_file data source configuration"""
        main_file = self._get_main_file_name(handler, runtime)

        lines = [
            f'data "archive_file" "{resource_name}_lambda" {{',
            '  type        = "zip"',
            f'  output_path = "${{path.module}}/files/{resource_name}.zip"',
            "",
            "  source {",
            f'    content  = <<EOF',
            files.get(main_file, "# Empty function"),
            'EOF',
            f'    filename = "{main_file}"',
            "  }",
            "}"
        ]
        return lines

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for a Lambda function"""
        try:
            function_name = resource.get("id")
            details = resource.get("details", {})

            if not function_name or not details:
                logger.error("Missing required Lambda function details")
                return None

            # Generate resource name
            resource_name = self._generate_resource_name(resource)

            # Get source configuration and potential inline code
            source_config, file_content = self._get_source_config(details, resource_name)

            # Start building HCL blocks
            hcl_blocks = []

            # Add archive_file data source if we have inline code
            if file_content:
                archive_block = self._generate_archive_file(
                    resource_name,
                    file_content,
                    details.get("runtime", ""),
                    details.get("handler", "")
                )
                hcl_blocks.extend(archive_block)
                hcl_blocks.append("")  # Add spacing

            # Start main Lambda resource
            hcl = [
                f'resource "aws_lambda_function" "{resource_name}" {{',
                f'  function_name = "{function_name}"',
                f'  role          = "{details.get("role")}"',
            ]

            # Add package type
            if package_type := details.get("package_type"):
                hcl.append(f'  package_type = "{package_type}"')

            # Add source configuration based on package type
            if package_type == "Image":
                if image_uri := details.get("image_uri"):
                    hcl.append(f'  image_uri = "{image_uri}"')
            else:
                # Add source configuration for Zip packages
                hcl.extend(source_config)
                # Add handler and runtime for zip packages
                if handler := details.get("handler"):
                    hcl.append(f'  handler = "{handler}"')
                if runtime := details.get("runtime"):
                    hcl.append(f'  runtime = "{runtime}"')

            # Add optional fields
            if description := details.get("description"):
                hcl.append(f'  description = "{description}"')

            if memory_size := details.get("memory_size"):
                hcl.append(f'  memory_size = {memory_size}')

            if timeout := details.get("timeout"):
                hcl.append(f'  timeout = {timeout}')

            # Always add publish parameter with default value false
            publish = details.get("publish", False)
            hcl.append(f'  publish = {str(publish).lower()}')

            # Add layers if present
            if layers := details.get("layers", []):
                layer_config = self._format_function_layers(layers)
                if layer_config:
                    hcl.append(layer_config)

            # Add environment variables
            environment = details.get("environment", {})
            if environment and environment.get("variables"):
                hcl.append("  environment {")
                hcl.append("    variables = {")
                for key, value in environment["variables"].items():
                    hcl.append(f'      {key} = "{value}"')
                hcl.append("    }")
                hcl.append("  }")

            # Add VPC configuration
            vpc_config = details.get("vpc_config", {})
            if vpc_config:
                hcl.append("  vpc_config {")
                if subnet_ids := vpc_config.get("subnet_ids", []):
                    subnet_ids_str = '", "'.join(subnet_ids)
                    hcl.append(f'    subnet_ids = ["{subnet_ids_str}"]')
                if security_group_ids := vpc_config.get("security_group_ids", []):
                    sg_ids_str = '", "'.join(security_group_ids)
                    hcl.append(f'    security_group_ids = ["{sg_ids_str}"]')
                hcl.append("  }")

            # Add dead letter config
            dead_letter_config = details.get("dead_letter_config", {})
            if dead_letter_config and (target_arn := dead_letter_config.get("target_arn")):
                hcl.append("  dead_letter_config {")
                hcl.append(f'    target_arn = "{target_arn}"')
                hcl.append("  }")

            # Add tracing config
            tracing_config = details.get("tracing_config", {})
            if tracing_config and (mode := tracing_config.get("mode")):
                hcl.append("  tracing_config {")
                hcl.append(f'    mode = "{mode}"')
                hcl.append("  }")

            # Add file system config if present
            file_system_config = details.get("file_system_config", {})
            if file_system_config:
                hcl.append("  file_system_config {")
                if arn := file_system_config.get("arn"):
                    hcl.append(f'    arn = "{arn}"')
                if local_mount_path := file_system_config.get("local_mount_path"):
                    hcl.append(f'    local_mount_path = "{local_mount_path}"')
                hcl.append("  }")

            # Add tags
            tags = resource.get("tags", {})
            if tags:
                hcl.append("  tags = {")
                for key, value in tags.items():
                    key = key.replace('"', '\\"')
                    value = value.replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            # Add the main resource block to our blocks
            hcl_blocks.extend(hcl)

            return "\n".join(hcl_blocks)

        except Exception as e:
            logger.error(f"Error generating HCL for Lambda function: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for Lambda function"""
        try:
            function_name = resource.get("id")
            if not function_name:
                logger.error("Missing function name for import command")
                return None

            # Generate resource name matching the one in generate()
            resource_name = self._generate_resource_name(resource)

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lambda_function.{resource_name} {function_name}"

        except Exception as e:
            logger.error(f"Error generating import command for Lambda function: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_compute/ec2.py</source>
<document_content># terraform_aws_migrator/generators/aws_compute/ec2.py

from typing import Dict, List, Any, Optional
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class EC2InstanceGenerator(HCLGenerator):
    """Generator for aws_instance resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_instance"

    def _get_name_from_tags(self, tags: List[Dict[str, str]]) -> Optional[str]:
        """Get Name tag value from tags list"""
        for tag in tags:
            if isinstance(tag, dict) and tag.get("Key") == "Name":
                return tag.get("Value")
        return None

    def _get_short_instance_id(self, instance_id: str) -> str:
        """Get shortened version of instance ID (last 4 characters)"""
        return instance_id[-4:] if instance_id else ""

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from Name tag or instance ID"""
        instance_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = self._get_name_from_tags(tags)
        short_id = self._get_short_instance_id(instance_id)

        if name_tag:
            # If there's a Name tag, use it with the short instance ID as suffix
            base_name = name_tag.replace("-", "_").replace(" ", "_")
            return f"{base_name}_{short_id}"
        else:
            # Fallback to instance ID if no Name tag
            return instance_id.replace("-", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for an EC2 instance"""
        try:
            instance_id = resource.get("id")
            if not instance_id:
                logger.error("Missing required instance ID")
                return None

            # Generate resource name based on Name tag or instance ID
            resource_name = self._generate_resource_name(resource)

            # Start building HCL
            hcl = [
                f'resource "aws_instance" "{resource_name}" {{',
            ]

            # Add instance details if available
            details = resource.get("details", {})

            # Add required fields with fallback values
            if details.get("ami"):
                hcl.append(f'  ami = "{details["ami"]}"')

            if details.get("instance_type"):
                hcl.append(f'  instance_type = "{details["instance_type"]}"')

            # Add optional fields if present
            if details.get("availability_zone"):
                hcl.append(f'  availability_zone = "{details["availability_zone"]}"')

            if details.get("subnet_id"):
                hcl.append(f'  subnet_id = "{details["subnet_id"]}"')

            if details.get("key_name"):
                hcl.append(f'  key_name = "{details["key_name"]}"')

            # Add VPC security groups if present
            vpc_security_groups = details.get("vpc_security_group_ids", [])
            if vpc_security_groups:
                security_groups_str = '", "'.join(vpc_security_groups)
                hcl.append(f'  vpc_security_group_ids = ["{security_groups_str}"]')

            # Add IAM instance profile if present
            if details.get("iam_instance_profile"):
                hcl.append(f'  iam_instance_profile = "{details["iam_instance_profile"]}"')

            # Add monitoring configuration
            monitoring = details.get("monitoring", False)
            hcl.append(f"  monitoring = {str(monitoring).lower()}")

            # Add root block device if present
            root_block_device = details.get("root_block_device")
            if root_block_device:
                hcl.extend([
                    "  root_block_device {",
                    f'    volume_size = {root_block_device.get("volume_size", 8)}',
                    f'    volume_type = "{root_block_device.get("volume_type", "gp2")}"',
                    f'    encrypted = {str(root_block_device.get("encrypted", False)).lower()}',
                    "  }",
                ])

            # Add EBS block devices if present
            ebs_block_devices = details.get("ebs_block_device", [])
            for device in ebs_block_devices:
                hcl.extend([
                    "  ebs_block_device {",
                    f'    device_name = "{device.get("device_name")}"',
                    f'    volume_size = {device.get("volume_size", 8)}',
                    f'    volume_type = "{device.get("volume_type", "gp2")}"',
                    f'    encrypted = {str(device.get("encrypted", False)).lower()}',
                    "  }",
                ])

            # Add user data if present
            user_data = details.get("user_data")
            if user_data:
                hcl.append(f'  user_data = "{user_data}"')

            # Set user_data_replace_on_change to false (Terraform default)
            # This setting determines whether changes to user_data should trigger instance replacement
            hcl.append("  user_data_replace_on_change = false")

            # Add instance tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for EC2 instance: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for EC2 instance"""
        try:
            instance_id = resource.get("id")
            if not instance_id:
                logger.error("Missing instance ID for import command")
                return None

            # Generate resource name matching the one in generate()
            resource_name = self._generate_resource_name(resource)

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_instance.{resource_name} {instance_id}"

        except Exception as e:
            logger.error(f"Error generating import command for EC2 instance: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_compute/security_group.py</source>
<document_content># terraform_aws_migrator/generators/aws_compute/security_group.py

from typing import Dict, List, Any, Optional
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class SecurityGroupGenerator(HCLGenerator):
    """Generator for aws_security_group resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_security_group"

    def _get_name_from_tags(self, tags: List[Dict[str, str]]) -> Optional[str]:
        """Get Name tag value from tags list"""
        for tag in tags:
            if isinstance(tag, dict) and tag.get("Key") == "Name":
                return tag.get("Value")
        return None

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from Name tag or security group ID"""
        sg_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = self._get_name_from_tags(tags)

        if name_tag:
            # Use Name tag value, sanitized for Terraform
            return name_tag.replace("-", "_").replace(" ", "_").lower()
        else:
            # Fallback to security group ID
            return sg_id.replace("-", "_").lower()

    def _format_rule(self, rule: Dict[str, Any], rule_type: str) -> List[str]:
        """Format a single security group rule (ingress or egress)"""
        lines = []

        # Start rule block
        lines.append(f"  {rule_type} {{")

        # Add from_port and to_port
        from_port = rule.get("from_port")
        to_port = rule.get("to_port")
        
        # For egress rules, use default values if not specified
        if rule_type == "egress" and (from_port is None or to_port is None):
            from_port = 0
            to_port = 0
            
        lines.append(f"    from_port = {from_port}")
        lines.append(f"    to_port = {to_port}")

        # Add protocol
        protocol = rule.get("protocol")
        if protocol == "-1":
            protocol = "all"
        lines.append(f'    protocol = "{protocol}"')

        # Add CIDR blocks
        cidr_blocks = rule.get("cidr_blocks", [])
        if cidr_blocks:
            cidr_blocks_str = '", "'.join(cidr_blocks)
            lines.append(f'    cidr_blocks = ["{cidr_blocks_str}"]')

        # Add IPv6 CIDR blocks
        ipv6_cidr_blocks = rule.get("ipv6_cidr_blocks", [])
        if ipv6_cidr_blocks:
            ipv6_blocks_str = '", "'.join(ipv6_cidr_blocks)
            lines.append(f'    ipv6_cidr_blocks = ["{ipv6_blocks_str}"]')

        # Add security group references
        security_groups = rule.get("security_groups", [])
        if security_groups:
            sg_str = '", "'.join(security_groups)
            lines.append(f'    security_groups = ["{sg_str}"]')

        # Close rule block
        lines.append("  }")

        return lines

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            sg_id = resource.get("id")
            details = resource.get("details", {})

            if not sg_id or not details:
                logger.error("Missing required security group details")
                return None

            # Generate resource name
            resource_name = self._generate_resource_name(resource)

            # Start building HCL
            hcl = [
                f'resource "aws_security_group" "{resource_name}" {{',
                f'  name                   = "{details.get("name")}"',
                f'  description            = "{details.get("description", "Managed by Terraform")}"',
                f'  revoke_rules_on_delete = {str(details.get("revoke_rules_on_delete", False)).lower()}',
            ]

            # Add VPC ID if present
            vpc_id = details.get("vpc_id")
            if vpc_id:
                hcl.append(f'  vpc_id = "{vpc_id}"')

            # Add ingress rules
            ingress_rules = details.get("ingress_rules", [])
            for rule in ingress_rules:
                hcl.extend(self._format_rule(rule, "ingress"))

            # Add egress rules
            egress_rules = details.get("egress_rules", [])
            for rule in egress_rules:
                hcl.extend(self._format_rule(rule, "egress"))

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for security group: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for security group"""
        try:
            sg_id = resource.get("id")
            if not sg_id:
                logger.error("Missing security group ID for import command")
                return None

            # Generate resource name matching the one in generate()
            resource_name = self._generate_resource_name(resource)

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_security_group.{resource_name} {sg_id}"

        except Exception as e:
            logger.error(
                f"Error generating import command for security group: {str(e)}"
            )
            return None
</document_content>
</document>

<document>
<source>tests/conftest.py</source>
<document_content>import pytest
from unittest.mock import MagicMock
from pathlib import Path
from terraform_aws_migrator.collectors.aws_network.network import LoadBalancerV2Collector

@pytest.fixture
def mock_session():
    # モックされたセッションオブジェクトを作成
    mock = MagicMock()
    mock.region_name = "ap-northeast-1"
    mock.account_id = "123456789012"  # 例として固定のアカウントIDを設定

    # STSクライアントのモックを作成
    sts_client = MagicMock()
    sts_client.get_caller_identity.return_value = {"Account": mock.account_id}

    # セッションのクライアントメソッドが適切なクライアントを返すように設定
    def get_client(service_name, *args, **kwargs):
        if service_name == 'sts':
            return sts_client
        return MagicMock()
    
    mock.client.side_effect = get_client

    return mock

@pytest.fixture
def sample_state_data():
    # サンプルの状態データを提供
    sample_data = {
        "version": 4,
        "terraform_version": "1.5.7",
        "serial": 2,
        "lineage": "sample-lineage",
        "outputs": {},
        "resources": [
            {
                "module": "module.sample_module",
                "mode": "managed",
                "type": "aws_vpc",
                "name": "sample_vpc",
                "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
                "instances": [
                    {
                        "schema_version": 0,
                        "attributes": {
                            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
                            "cidr_block": "10.0.0.0/16",
                            "is_default": False,
                            "id": "vpc-12345678",
                            "tags": {},
                            "tags_all": {}
                        }
                    }
                ]
            },
            {
                "module": "module.sample_module",
                "mode": "managed",
                "type": "aws_lb",
                "name": "sample_lb",
                "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
                "instances": [
                    {
                        "schema_version": 0,
                        "attributes": {
                            "arn": "arn:aws:elasticloadbalancing:ap-northeast-1:123456789012:loadbalancer/app/sample-lb/abcdef123456",
                            "id": "arn:aws:elasticloadbalancing:ap-northeast-1:123456789012:loadbalancer/app/sample-lb/abcdef123456",
                            "name": "sample-lb",
                            "load_balancer_type": "application",
                            "subnets": ["subnet-aaaa1111", "subnet-bbbb2222"],
                            "security_groups": ["sg-aaaa1111"],
                            "vpc_id": "vpc-12345678",
                            "tags": {},
                            "tags_all": {}
                        }
                    }
                ]
            },
            {
                "module": "module.sample_module",
                "mode": "managed",
                "type": "aws_lb",
                "name": "test-lb",
                "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
                "instances": [
                    {
                        "schema_version": 0,
                        "attributes": {
                            "arn": "arn:aws:elasticloadbalancing:ap-northeast-1:123456789012:loadbalancer/app/test-lb/1234567890",
                            "id": "arn:aws:elasticloadbalancing:ap-northeast-1:123456789012:loadbalancer/app/test-lb/1234567890",
                            "name": "test-lb",
                            "load_balancer_type": "application",
                            "subnets": ["subnet-33333333", "subnet-44444444"],
                            "security_groups": ["sg-87654321"],
                            "vpc_id": "vpc-87654321",
                            "tags": {},
                            "tags_all": {}
                        }
                    }
                ]
            },
            {
                "module": "module.sample_module",
                "mode": "managed",
                "type": "aws_subnet",
                "name": "sample_subnet",
                "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
                "instances": [
                    {
                        "schema_version": 0,
                        "attributes": {
                            "id": "subnet-12345678",
                            "vpc_id": "vpc-12345678",
                            "cidr_block": "10.0.1.0/24",
                            "availability_zone": "ap-northeast-1a",
                            "tags": {},
                            "tags_all": {}
                        }
                    }
                ]
            }
        ]
    }
    return sample_data

@pytest.fixture
def mock_collector(mock_session):
    """LoadBalancerV2Collectorのモックを提供するフィクスチャ"""
    collector = LoadBalancerV2Collector(mock_session)
    
    # ELBv2クライアントのモックを取得
    elbv2_client = mock_session.client('elbv2')
    
    # describe_load_balancersのモックレスポンスを設定
    paginator = MagicMock()
    paginator.paginate.return_value = [{
        "LoadBalancers": [
            {
                "LoadBalancerArn": "arn:aws:elasticloadbalancing:ap-northeast-1:123456789012:loadbalancer/app/test-lb/1234567890",
                "LoadBalancerName": "test-lb",
                "Type": "application",  # ALBのみを収集
                "Scheme": "internet-facing",
                "VpcId": "vpc-12345678",
                "State": {"Code": "active"},
                "AvailabilityZones": [
                    {"SubnetId": "subnet-11111111"},
                    {"SubnetId": "subnet-22222222"}
                ],
                "SecurityGroups": ["sg-12345678"],
                "DNSName": "test-lb.ap-northeast-1.elb.amazonaws.com"
            }
        ]
    }]
    elbv2_client.get_paginator.return_value = paginator

    # describe_load_balancer_attributesのモックレスポンスを設定
    elbv2_client.describe_load_balancer_attributes.return_value = {
        "Attributes": [
            {
                "Key": "idle_timeout.timeout_seconds",
                "Value": "60"
            }
        ]
    }

    # describe_tagsのモックレスポンスを設定
    def describe_tags_side_effect(**kwargs):
        return {
            "TagDescriptions": [
                {
                    "ResourceArn": kwargs["ResourceArns"][0],
                    "Tags": []
                }
            ]
        }
    elbv2_client.describe_tags.side_effect = describe_tags_side_effect
    return collector
</document_content>
</document>

<document>
<source>tests/__init__.py</source>
<document_content># tests/__init__.py
</document_content>
</document>

<document>
<source>tests/unit/test_vpc_endpoint.py</source>
<document_content>import pytest
import sys
import os

# プロジェクトルートをPythonパスに追加
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from terraform_aws_migrator.auditor import AWSResourceAuditor
from terraform_aws_migrator.collectors.base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)

def test_vpc_endpoint_identifier_generation_with_route_tables(mock_session):
    """VPCエンドポイントの識別子生成テスト（route_table_idsを含むケース）"""
    @register_collector
    class TestVpcEndpointCollector(ResourceCollector):
        def __init__(self, session=None):
            super().__init__(session)
            self._test_resources = [
                {
                    "type": "aws_vpc_endpoint",
                    "id": "dummy-vpce-1",
                    "arn": "",
                    "tags": [
                        {"Key": "Name", "Value": "dummy-front"}
                    ],
                    "details": {
                        "vpc_id": "dummy-vpc-1",
                        "service_name": "dummy-service",
                        "state": "available",
                        "vpc_endpoint_type": "Gateway",
                        "subnet_ids": [],
                        "route_table_ids": [],
                        "private_dns_enabled": False,
                        "network_interface_ids": [],
                        "dns_entries": [],
                        "policy": '{"Statement": [{"Action": "*", "Effect": "Allow", "Principal": "*", "Resource": "*"}], "Version": "2008-10-17"}',
                        "name": "dummy-front"
                    }
                },
                {
                    "type": "aws_vpc_endpoint",
                    "id": "dummy-vpce-2",
                    "arn": "",
                    "tags": [
                        {"Key": "Name", "Value": "dummy-back"}
                    ],
                    "details": {
                        "vpc_id": "dummy-vpc-2",
                        "service_name": "dummy-service",
                        "state": "available",
                        "vpc_endpoint_type": "Gateway",
                        "subnet_ids": [],
                        "route_table_ids": ["dummy-rtb"],
                        "private_dns_enabled": False,
                        "network_interface_ids": [],
                        "dns_entries": [],
                        "policy": '{"Statement": [{"Action": "*", "Effect": "Allow", "Principal": "*", "Resource": "*"}], "Version": "2008-10-17"}',
                        "name": "dummy-back"
                    }
                }
            ]

        def get_service_name(self) -> str:
            return "ec2"

        @classmethod
        def get_resource_types(cls) -> dict:
            return {
                "aws_vpc_endpoint": "VPC Endpoint"
            }

        def collect(self, target_resource_type: str = "") -> list:
            resources = self._test_resources.copy()
            if target_resource_type:
                return [r.copy() for r in resources if r["type"] == target_resource_type]
            return [r.copy() for r in resources]

        def generate_resource_identifier(self, resource: dict) -> str:
            """VPCエンドポイント用の識別子生成"""
            if resource.get("type") != "aws_vpc_endpoint":
                return super().generate_resource_identifier(resource)
            
            try:
                details = resource.get("details", {})
                vpc_id = details.get("vpc_id")
                service_name = details.get("service_name")
                endpoint_id = resource.get("id")
                name = details.get("name")

                # デバッグログを出力
                logger.debug(f"Generating identifier for VPC endpoint:")
                logger.debug(f"  vpc_id: {vpc_id}")
                logger.debug(f"  service_name: {service_name}")
                logger.debug(f"  endpoint_id: {endpoint_id}")
                logger.debug(f"  name: {name}")
                logger.debug(f"  details: {details}")

                # エンドポイントIDがある場合、そのIDを使って識別子生成
                if endpoint_id:
                    # 基本の識別子
                    identifier = f"{resource['type']}:{endpoint_id}"
                    
                    # 追加情報があれば詳細な識別子にする
                    if name and vpc_id and service_name:
                        identifier = f"{resource['type']}:{name}:{vpc_id}:{service_name}:{endpoint_id}"
                    elif vpc_id and service_name:
                        identifier = f"{resource['type']}:{vpc_id}:{service_name}:{endpoint_id}"
                    
                    logger.debug(f"Generated identifier: {identifier}")
                    return identifier

                logger.warning("Missing endpoint_id for VPC endpoint")
                return None

            except Exception as e:
                logger.error(f"Error generating identifier for VPC endpoint: {str(e)}")
                logger.debug(f"Resource: {resource}")
                return None

    from terraform_aws_migrator.collectors.base import registry
    registry.collectors = []
    register_collector(TestVpcEndpointCollector)

    auditor = AWSResourceAuditor()
    auditor.session = mock_session
    
    auditor.state_reader.get_managed_resources = lambda *args: {}
    
    result = auditor.audit_resources("dummy_tf_dir")
    
    ec2_resources = result["all_resources"].get("ec2", [])
    vpc_endpoints = [r for r in ec2_resources if r["type"] == "aws_vpc_endpoint"]
    assert len(vpc_endpoints) == 2  # 2件とも処理される

    front_endpoint = next(e for e in vpc_endpoints if e["id"] == "dummy-vpce-1")
    assert "identifier" in front_endpoint
    expected_front_identifier = "aws_vpc_endpoint:dummy-front:dummy-vpc-1:dummy-service:dummy-vpce-1"
    assert front_endpoint["identifier"] == expected_front_identifier

    back_endpoint = next(e for e in vpc_endpoints if e["id"] == "dummy-vpce-2")
    assert "identifier" in back_endpoint
    expected_back_identifier = "aws_vpc_endpoint:dummy-back:dummy-vpc-2:dummy-service:dummy-vpce-2"
    assert back_endpoint["identifier"] == expected_back_identifier

    assert back_endpoint["details"]["route_table_ids"] == ["dummy-rtb"]
    assert front_endpoint["details"]["route_table_ids"] == []
</document_content>
</document>

<document>
<source>tests/unit/test_auditor.py</source>
<document_content>import pytest
from terraform_aws_migrator.auditor import AWSResourceAuditor
from terraform_aws_migrator.collectors.base import ResourceCollector, register_collector


@pytest.fixture
def mock_collector(mock_session):
    """テスト用のモックコレクター"""
    @register_collector
    class TestCollector(ResourceCollector):
        def __init__(self, session=None):
            super().__init__(session)
            self._current_service = "ec2"
            self._test_resources = [
                {
                    "type": "aws_vpc",
                    "id": "vpc-12345678",
                    "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
                    "tags": [{"Key": "Name", "Value": "MainVPC"}],
                    "details": {"cidr_block": "10.0.0.0/16"}
                },
                {
                    "type": "aws_subnet",
                    "id": "subnet-12345678",
                    "arn": "arn:aws:ec2:ap-northeast-1:123456789012:subnet/subnet-12345678",
                    "tags": [{"Key": "Name", "Value": "PublicSubnet"}],
                    "details": {
                        "vpc_id": "vpc-12345678",
                        "cidr_block": "10.0.1.0/24"
                    }
                }
            ]

        def get_service_name(self) -> str:
            return self._current_service

        def set_service(self, service: str):
            self._current_service = service
            if service == "ec2":
                self._test_resources = [
                    {
                        "type": "aws_vpc",
                        "id": "vpc-12345678",
                        "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
                        "tags": [{"Key": "Name", "Value": "MainVPC"}],
                        "details": {"cidr_block": "10.0.0.0/16"}
                    },
                    {
                        "type": "aws_subnet",
                        "id": "subnet-12345678",
                        "arn": "arn:aws:ec2:ap-northeast-1:123456789012:subnet/subnet-12345678",
                        "tags": [{"Key": "Name", "Value": "PublicSubnet"}],
                        "details": {
                            "vpc_id": "vpc-12345678",
                            "cidr_block": "10.0.1.0/24"
                        }
                    }
                ]
            elif service == "iam":
                self._test_resources = [
                    {
                        "type": "aws_iam_role",
                        "id": "test-role",
                        "arn": "arn:aws:iam::123456789012:role/service-role/test-role",
                        "path": "/service-role/",
                        "tags": [{"Key": "Service", "Value": "Lambda"}]
                    }
                ]
            elif service == "s3":
                self._test_resources = [
                    {
                        "type": "aws_s3_bucket",
                        "id": "my-test-bucket",
                        "arn": "arn:aws:s3:::my-test-bucket",
                        "tags": [{"Key": "Environment", "Value": "Test"}]
                    }
                ]
            
        @classmethod
        def get_resource_types(cls) -> dict:
            return {
                "aws_vpc": "VPC",
                "aws_subnet": "Subnet"
            }
            
        def collect(self, target_resource_type: str = "") -> list:
            resources = self._test_resources.copy()
            if target_resource_type:
                return [r.copy() for r in resources if r["type"] == target_resource_type]
            return [r.copy() for r in resources]

        def generate_resource_identifier(self, resource: dict) -> str:
            """リソース識別子の生成"""
            resource_type = resource.get("type", "")
            resource_id = resource.get("id", "")
            
            # ARNがある場合はそれを使用
            if "arn" in resource:
                return resource["arn"]
                
            # リソースタイプに基づいて識別子を生成
            if resource_type == "aws_vpc":
                return f"aws_vpc:{resource_id}"
            elif resource_type == "aws_subnet":
                return f"aws_subnet:{resource_id}"
            elif resource_type and resource_id:
                return f"{resource_type}:{resource_id}"
                
            # フォールバック：空の文字列を返す
            return ""


def test_resource_matching(mock_session, sample_state_data, mock_collector):
    """リソースの照合と管理状態の判定テスト"""
    auditor = AWSResourceAuditor()
    auditor.session = mock_session
    
    # リソースの監査を実行
    result = auditor.audit_resources("dummy_tf_dir")
    
    # EC2サービスのリソースを確認
    ec2_resources = result["all_resources"].get("ec2", [])
    assert len(ec2_resources) == 2
    
    # VPCリソースの確認
    vpc = next(r for r in ec2_resources if r["type"] == "aws_vpc")
    assert vpc["id"] == "vpc-12345678"
    assert vpc["managed"] is False  # 未管理状態の確認
    
    # サブネットリソースの確認
    subnet = next(r for r in ec2_resources if r["type"] == "aws_subnet")
    assert subnet["id"] == "subnet-12345678"
    assert subnet["managed"] is False


def test_identifier_based_mapping(mock_session, mock_collector):
    """識別子を使用したリソースのマッピングテスト"""
    auditor = AWSResourceAuditor()
    auditor.session = mock_session
    
    # 既存の管理されたリソース
    managed_resources = {
        "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678": {
            "type": "aws_vpc",
            "id": "vpc-12345678",
            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
            "managed": True
        }
    }
    
    # モックのstate_reader
    auditor.state_reader.get_managed_resources = lambda *args: managed_resources
    
    # リソースの監査を実行
    result = auditor.audit_resources("dummy_tf_dir")
    
    # EC2サービスのリソースを確認
    ec2_resources = result["all_resources"].get("ec2", [])
    
    # VPCリソースの確認（管理状態）
    vpc = next(r for r in ec2_resources if r["type"] == "aws_vpc")
    assert vpc["managed"] is True
    
    # サブネットリソースの確認（未管理状態）
    subnet = next(r for r in ec2_resources if r["type"] == "aws_subnet")
    assert subnet["managed"] is False


def test_resource_exclusion(mock_session, mock_collector):
    """リソース除外設定のテスト"""
    # 除外設定付きでAuditorを初期化
    auditor = AWSResourceAuditor(exclusion_file="dummy_ignore_file")
    auditor.session = mock_session
    
    # 除外設定をモック
    auditor.exclusion_config.should_exclude = lambda resource: resource["type"] == "aws_vpc"
    
    # リソースの監査を実行
    result = auditor.audit_resources("dummy_tf_dir")
    
    # EC2サービスのリソースを確認
    ec2_resources = result["all_resources"].get("ec2", [])
    
    # VPCが除外され、サブネットのみが含まれていることを確認
    assert len(ec2_resources) == 1
    assert ec2_resources[0]["type"] == "aws_subnet"


def test_service_type_mapping(mock_session, mock_collector):
    """サービス名とリソースタイプのマッピングテスト"""
    auditor = AWSResourceAuditor(target_resource_type="aws_vpc")
    auditor.session = mock_session
    
    # リソースの監査を実行
    result = auditor.audit_resources("dummy_tf_dir")
    
    # 特定のリソースタイプのみが含まれていることを確認
    ec2_resources = result["all_resources"].get("ec2", [])
    resource_types = {r["type"] for r in ec2_resources}
    assert resource_types == {"aws_vpc"}


def test_resource_details_preservation(mock_session, mock_collector):
    """リソース詳細情報の保持テスト"""
    auditor = AWSResourceAuditor()
    auditor.session = mock_session
    
    # リソースの監査を実行
    result = auditor.audit_resources("dummy_tf_dir")
    
    # EC2サービスのリソースを確認
    ec2_resources = result["all_resources"].get("ec2", [])
    
    # VPCリソースの詳細情報を確認
    vpc = next(r for r in ec2_resources if r["type"] == "aws_vpc")
    assert "details" in vpc
    assert vpc["details"]["cidr_block"] == "10.0.0.0/16"
    
    # サブネットリソースの詳細情報を確認
    subnet = next(r for r in ec2_resources if r["type"] == "aws_subnet")
    assert "details" in subnet
    assert subnet["details"]["vpc_id"] == "vpc-12345678"
    assert subnet["details"]["cidr_block"] == "10.0.1.0/24"


def test_managed_resource_count(mock_session, mock_collector):
    """マネージドリソースのカウントテスト"""
    auditor = AWSResourceAuditor()
    auditor.session = mock_session
    
    # 既存の管理されたリソース（より複雑なケース）
    managed_resources = {
        # 通常のリソース
        "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678": {
            "type": "aws_vpc",
            "id": "vpc-12345678",
            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
            "managed": True
        },
        "aws_subnet:subnet-12345678": {
            "type": "aws_subnet",
            "id": "subnet-12345678",
            "managed": True
        },
        # モジュールリソース
        "arn:aws:dynamodb:ap-northeast-1:123456789012:table/test-table": {
            "type": "module.fa_common_modules_stag.aws_dynamodb_table",
            "id": "test-table",
            "arn": "arn:aws:dynamodb:ap-northeast-1:123456789012:table/test-table",
            "managed": True
        },
        "arn:aws:ec2:ap-northeast-1:123456789012:instance/i-1234567890abcdef0": {
            "type": "module.fa_common_modules_stag.aws_instance",
            "id": "i-1234567890abcdef0",
            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:instance/i-1234567890abcdef0",
            "managed": True
        },
        # IAM Role with path-based identifier
        "arn:aws:iam::123456789012:role/service-role/test-role": {
            "type": "aws_iam_role",
            "id": "test-role",
            "arn": "arn:aws:iam::123456789012:role/service-role/test-role",
            "managed": True
        },
        # S3 Bucket with name-based identifier
        "aws_s3_bucket:my-test-bucket": {
            "type": "aws_s3_bucket",
            "id": "my-test-bucket",
            "managed": True
        }
    }
    
    # モックのstate_reader
    auditor.state_reader.get_managed_resources = lambda *args: managed_resources
    
    # コレクターを作成
    @register_collector
    class TestCollector(ResourceCollector):
        def __init__(self, session=None):
            super().__init__(session)
            self._current_service = "ec2"
            self._test_resources = []
            self.set_service("ec2")

        def get_service_name(self) -> str:
            return self._current_service

        def set_service(self, service: str):
            self._current_service = service
            if service == "ec2":
                self._test_resources = [
                    {
                        "type": "aws_vpc",
                        "id": "vpc-12345678",
                        "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
                        "tags": [{"Key": "Name", "Value": "MainVPC"}],
                        "details": {"cidr_block": "10.0.0.0/16"}
                    },
                    {
                        "type": "aws_subnet",
                        "id": "subnet-12345678",
                        "arn": "arn:aws:ec2:ap-northeast-1:123456789012:subnet/subnet-12345678",
                        "tags": [{"Key": "Name", "Value": "PublicSubnet"}],
                        "details": {
                            "vpc_id": "vpc-12345678",
                            "cidr_block": "10.0.1.0/24"
                        }
                    }
                ]
            elif service == "iam":
                self._test_resources = [
                    {
                        "type": "aws_iam_role",
                        "id": "test-role",
                        "arn": "arn:aws:iam::123456789012:role/service-role/test-role",
                        "path": "/service-role/",
                        "tags": [{"Key": "Service", "Value": "Lambda"}]
                    }
                ]
            elif service == "s3":
                self._test_resources = [
                    {
                        "type": "aws_s3_bucket",
                        "id": "my-test-bucket",
                        "arn": "arn:aws:s3:::my-test-bucket",
                        "tags": [{"Key": "Environment", "Value": "Test"}]
                    }
                ]

        def collect(self, target_resource_type: str = "") -> list:
            resources = self._test_resources.copy()
            if target_resource_type:
                return [r.copy() for r in resources if r["type"] == target_resource_type]
            return [r.copy() for r in resources]

        @classmethod
        def get_resource_types(cls) -> dict:
            return {
                "aws_vpc": "VPC",
                "aws_subnet": "Subnet",
                "aws_iam_role": "IAM Role",
                "aws_s3_bucket": "S3 Bucket"
            }

        @classmethod
        def get_service_for_resource_type(cls, resource_type: str) -> str:
            if resource_type.startswith("aws_vpc") or resource_type.startswith("aws_subnet"):
                return "ec2"
            elif resource_type.startswith("aws_iam"):
                return "iam"
            elif resource_type.startswith("aws_s3"):
                return "s3"
            return ""

    collector = TestCollector(mock_session)
    auditor._get_relevant_collectors = lambda: [collector]

    # EC2リソースの監査
    collector.set_service("ec2")
    result_ec2 = auditor.audit_resources("dummy_tf_dir")
    ec2_resources = result_ec2["all_resources"].get("ec2", [])

    # IAMリソースの監査
    collector.set_service("iam")
    result_iam = auditor.audit_resources("dummy_tf_dir")
    iam_resources = result_iam["all_resources"].get("iam", [])

    # S3リソースの監査
    collector.set_service("s3")
    result_s3 = auditor.audit_resources("dummy_tf_dir")
    s3_resources = result_s3["all_resources"].get("s3", [])

    # マネージドリソースの数を確認
    managed_count = (
        sum(1 for r in ec2_resources if r.get("managed", False)) +
        sum(1 for r in iam_resources if r.get("managed", False)) +
        sum(1 for r in s3_resources if r.get("managed", False))
    )
    assert managed_count == 4
    
    # 各リソースタイプごとのマネージド数を確認
    vpc_managed = sum(1 for r in ec2_resources if r["type"] == "aws_vpc" and r.get("managed", False))
    subnet_managed = sum(1 for r in ec2_resources if r["type"] == "aws_subnet" and r.get("managed", False))
    role_managed = sum(1 for r in iam_resources if r["type"] == "aws_iam_role" and r.get("managed", False))
    bucket_managed = sum(1 for r in s3_resources if r["type"] == "aws_s3_bucket" and r.get("managed", False))
    
    assert vpc_managed == 1
    assert subnet_managed == 1
    assert role_managed == 1
    assert bucket_managed == 1


def test_complex_resource_matching(mock_session):
    """複雑なリソースマッチングのテスト"""
    @register_collector
    class ComplexTestCollector(ResourceCollector):
        def __init__(self, session=None):
            super().__init__(session)
            self._current_service = "ec2"
            self._test_resources = []
            self.set_service("ec2")

        def get_service_name(self) -> str:
            return self._current_service

        def set_service(self, service: str):
            self._current_service = service
            if service == "ec2":
                self._test_resources = [
                    {
                        "type": "aws_vpc",
                        "id": "vpc-12345678",
                        "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
                        "tags": [{"Key": "Name", "Value": "MainVPC"}],
                        "details": {"cidr_block": "10.0.0.0/16"}
                    },
                    {
                        "type": "module.fa_common_modules_stag.aws_instance",
                        "id": "i-1234567890abcdef0",
                        "arn": "arn:aws:ec2:ap-northeast-1:123456789012:instance/i-1234567890abcdef0",
                        "tags": [{"Key": "Name", "Value": "ModuleInstance"}],
                        "details": {"instance_type": "t3.micro"}
                    }
                ]
            elif service == "dynamodb":
                self._test_resources = [{
                    "type": "module.fa_common_modules_stag.aws_dynamodb_table",
                    "id": "test-table",
                    "arn": "arn:aws:dynamodb:ap-northeast-1:123456789012:table/test-table",
                    "tags": [{"Key": "Environment", "Value": "Staging"}],
                    "details": {"billing_mode": "PAY_PER_REQUEST"}
                }]
            elif service == "iam":
                self._test_resources = [{
                    "type": "aws_iam_role",
                    "id": "test-role",
                    "arn": "arn:aws:iam::123456789012:role/service-role/test-role",
                    "path": "/service-role/",
                    "tags": [{"Key": "Service", "Value": "Lambda"}]
                }]
            elif service == "s3":
                self._test_resources = [{
                    "type": "aws_s3_bucket",
                    "id": "my-test-bucket",
                    "arn": "arn:aws:s3:::my-test-bucket",
                    "tags": [{"Key": "Environment", "Value": "Test"}]
                }]

        @classmethod
        def get_resource_types(cls) -> dict:
            return {
                "aws_vpc": "VPC",
                "module.fa_common_modules_stag.aws_instance": "EC2 Instance (Module)",
                "module.fa_common_modules_stag.aws_dynamodb_table": "DynamoDB Table (Module)",
                "aws_iam_role": "IAM Role",
                "aws_s3_bucket": "S3 Bucket"
            }

        def collect(self, target_resource_type: str = "") -> list:
            resources = self._test_resources.copy()
            if target_resource_type:
                return [r.copy() for r in resources if r["type"] == target_resource_type]
            return [r.copy() for r in resources]

        @classmethod
        def get_service_for_resource_type(cls, resource_type: str) -> str:
            if resource_type.startswith("aws_vpc") or "aws_instance" in resource_type:
                return "ec2"
            elif "aws_dynamodb_table" in resource_type:
                return "dynamodb"
            elif resource_type.startswith("aws_iam"):
                return "iam"
            elif resource_type.startswith("aws_s3"):
                return "s3"
            return ""

    auditor = AWSResourceAuditor()
    auditor.session = mock_session

    # 既存の管理されたリソース
    managed_resources = {
        # 通常のリソース
        "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678": {
            "type": "aws_vpc",
            "id": "vpc-12345678",
            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
            "managed": True
        },
        # モジュールリソース
        "arn:aws:ec2:ap-northeast-1:123456789012:instance/i-1234567890abcdef0": {
            "type": "module.fa_common_modules_stag.aws_instance",
            "id": "i-1234567890abcdef0",
            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:instance/i-1234567890abcdef0",
            "managed": True
        },
        "arn:aws:dynamodb:ap-northeast-1:123456789012:table/test-table": {
            "type": "module.fa_common_modules_stag.aws_dynamodb_table",
            "id": "test-table",
            "arn": "arn:aws:dynamodb:ap-northeast-1:123456789012:table/test-table",
            "managed": True
        }
    }

    # モックのstate_reader
    auditor.state_reader.get_managed_resources = lambda *args: managed_resources

    # コレクターを作成
    collector = ComplexTestCollector(mock_session)
    auditor._get_relevant_collectors = lambda: [collector]

    # EC2リソースの監査（通常のリソースとモジュールリソース）
    collector.set_service("ec2")
    result_ec2 = auditor.audit_resources("dummy_tf_dir")
    ec2_resources = result_ec2["all_resources"].get("ec2", [])
    
    # 通常のVPCリソースの確認
    assert any(r["type"] == "aws_vpc" for r in ec2_resources)
    vpc = next(r for r in ec2_resources if r["type"] == "aws_vpc")
    assert vpc["managed"] is True

    # モジュールのEC2インスタンスリソースの確認
    assert any("aws_instance" in r["type"] for r in ec2_resources)
    instance = next(r for r in ec2_resources if "aws_instance" in r["type"])
    assert instance["managed"] is True
    assert instance["type"] == "module.fa_common_modules_stag.aws_instance"

    # DynamoDBリソースの監査（モジュールリソース）
    collector.set_service("dynamodb")
    result_dynamodb = auditor.audit_resources("dummy_tf_dir")
    dynamodb_resources = result_dynamodb["all_resources"].get("dynamodb", [])
    assert any("aws_dynamodb_table" in r["type"] for r in dynamodb_resources)
    table = next(r for r in dynamodb_resources if "aws_dynamodb_table" in r["type"])
    assert table["managed"] is True
    assert table["type"] == "module.fa_common_modules_stag.aws_dynamodb_table"

    # IAMリソースの監査（未管理リソース）
    collector.set_service("iam")
    result_iam = auditor.audit_resources("dummy_tf_dir")
    iam_resources = result_iam["all_resources"].get("iam", [])
    assert any(r["type"] == "aws_iam_role" for r in iam_resources)
    role = next(r for r in iam_resources if r["type"] == "aws_iam_role")
    assert role["managed"] is False

    # S3リソースの監査（未管理リソース）
    collector.set_service("s3")
    result_s3 = auditor.audit_resources("dummy_tf_dir")
    s3_resources = result_s3["all_resources"].get("s3", [])
    assert any(r["type"] == "aws_s3_bucket" for r in s3_resources)
    bucket = next(r for r in s3_resources if r["type"] == "aws_s3_bucket")
    assert bucket["managed"] is False


def test_real_state_file(mock_session):
    """実際のTerraformステートファイルを使用したテスト"""
    auditor = AWSResourceAuditor()
    auditor.session = mock_session

    # テストフィクスチャのディレクトリを使用
    result = auditor.audit_resources("tests/fixtures")

    # 結果の検証
    all_resources = result["all_resources"]
    assert len(all_resources) > 0

    # マネージドリソースの存在を確認
    managed_count = sum(1 for resources in all_resources.values()
                       for r in resources if r.get("managed", False))
    assert managed_count > 0

    # 特定のリソースタイプの確認
    ec2_resources = all_resources.get("ec2", [])
    vpc_resources = [r for r in ec2_resources if r["type"] == "aws_vpc"]
    assert len(vpc_resources) > 0
    assert any(r["managed"] for r in vpc_resources)
    
    # 既存の管理されたリソース
    managed_resources = {
        "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678": {
            "type": "aws_vpc",
            "id": "vpc-12345678",
            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
            "managed": True
        },
        "aws_subnet:subnet-12345678": {
            "type": "aws_subnet",
            "id": "subnet-12345678",
            "managed": True
        }
    }
    
    # モックのstate_reader
    auditor.state_reader.get_managed_resources = lambda *args: managed_resources
    
    # リソースの監査を実行
    result = auditor.audit_resources("dummy_tf_dir")
    
    # EC2サービスのリソースを確認
    ec2_resources = result["all_resources"].get("ec2", [])
    
    # マネージドリソースの数を確認
    managed_count = sum(1 for r in ec2_resources if r.get("managed", False))
    assert managed_count == 2
    
    # 各リソースタイプごとのマネージド数を確認
    vpc_managed = sum(1 for r in ec2_resources if r["type"] == "aws_vpc" and r.get("managed", False))
    subnet_managed = sum(1 for r in ec2_resources if r["type"] == "aws_subnet" and r.get("managed", False))
    
    assert vpc_managed == 1
    assert subnet_managed == 1
</document_content>
</document>

<document>
<source>tests/unit/test_state_reader.py</source>
<document_content>import pytest
from unittest.mock import MagicMock
from terraform_aws_migrator.state_reader import TerraformStateReader
from pathlib import Path
import json
import json
import tempfile
import os
import io
import json


def test_get_identifier_for_managed_set(mock_session):
    """リソース識別子の生成テスト"""
    reader = TerraformStateReader(mock_session)
    
    # ARNを持つリソース
    resource_with_arn = {
        "type": "aws_vpc",
        "id": "vpc-12345678",
        "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
        "tags": [{"Key": "Name", "Value": "MainVPC"}]
    }
    identifier = reader._get_identifier_for_managed_set(resource_with_arn)
    assert identifier == "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
    
    # IAMロールポリシーアタッチメント
    role_policy_attachment = {
        "type": "aws_iam_role_policy_attachment",
        "role": "test-role",
        "policy_arn": "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
    }
    identifier = reader._get_identifier_for_managed_set(role_policy_attachment)
    assert identifier == f"arn:aws:iam::{mock_session.client('sts').get_caller_identity()['Account']}:role/test-role/arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
    
    # 通常のリソース（ARNなし）
    normal_resource = {
        "type": "aws_security_group",
        "id": "sg-12345678",
        "tags": [{"Key": "Name", "Value": "WebServerSG"}]
    }
    identifier = reader._get_identifier_for_managed_set(normal_resource)
    assert identifier == "aws_security_group:sg-12345678"


def test_extract_resources_from_state(mock_session, sample_state_data):
    """stateファイルからのリソース抽出テスト"""
    reader = TerraformStateReader(mock_session)
    managed_resources = {}
    
    reader._extract_resources_from_state(sample_state_data, managed_resources)
    
    # VPCリソースの検証
    vpc_identifier = "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
    assert vpc_identifier in managed_resources
    vpc = managed_resources[vpc_identifier]
    assert vpc["type"] == "aws_vpc"
    assert vpc["id"] == "vpc-12345678"
    assert vpc["details"]["cidr_block"] == "10.0.0.0/16"
    
    # サブネットリソースの検証
    subnet_identifier = "aws_subnet:subnet-12345678"
    assert subnet_identifier in managed_resources
    subnet = managed_resources[subnet_identifier]
    assert subnet["type"] == "aws_subnet"
    assert subnet["id"] == "subnet-12345678"
    assert subnet["details"]["vpc_id"] == "vpc-12345678"


def test_duplicate_resource_handling(mock_session):
    """重複リソースの処理テスト"""
    reader = TerraformStateReader(mock_session)
    managed_resources = {}
    
    # 重複するリソースを含むステートデータ
    duplicate_state = {
        "version": 4,
        "resources": [
            {
                "mode": "managed",
                "type": "aws_vpc",
                "name": "main",
                "instances": [
                    {
                        "attributes": {
                            "id": "vpc-12345678",
                            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
                        }
                    }
                ]
            },
            {
                "mode": "managed",
                "type": "aws_vpc",
                "name": "main_duplicate",
                "instances": [
                    {
                        "attributes": {
                            "id": "vpc-12345678",
                            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
                        }
                    }
                ]
            }
        ]
    }
    
    reader._extract_resources_from_state(duplicate_state, managed_resources)
    
    # 同じARNを持つリソースは1つだけ保持されているか確認
    vpc_resources = [r for r in managed_resources.values() if r["type"] == "aws_vpc"]
    assert len(vpc_resources) == 1


def test_module_resource_handling(mock_session):
    """モジュール内のリソース処理テスト"""
    reader = TerraformStateReader(mock_session)
    managed_resources = {}
    
    # モジュール内のリソースを含むステートデータ
    module_state = {
        "version": 4,
        "resources": [
            {
                "module": "module.network",
                "mode": "managed",
                "type": "aws_vpc",
                "name": "main",
                "instances": [
                    {
                        "attributes": {
                            "id": "vpc-12345678",
                            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
                        }
                    }
                ]
            }
        ]
    }
    
    reader._extract_resources_from_state(module_state, managed_resources)
    
    # モジュールパスが含まれていないことを確認
    vpc_resources = [r for r in managed_resources.values() if r["type"] == "aws_vpc"]
    assert len(vpc_resources) == 1
    assert vpc_resources[0]["type"] == "aws_vpc"


def test_tag_handling(mock_session):
    """タグの処理テスト"""
    reader = TerraformStateReader(mock_session)
    managed_resources = {}
    
    # 異なる形式のタグを含むステートデータ
    tag_state = {
        "version": 4,
        "resources": [
            {
                "mode": "managed",
                "type": "aws_vpc",
                "name": "main",
                "instances": [
                    {
                        "attributes": {
                            "id": "vpc-12345678",
                            "tags": {
                                "Name": "MainVPC",
                                "Environment": "Production"
                            }
                        }
                    }
                ]
            },
            {
                "mode": "managed",
                "type": "aws_subnet",
                "name": "public",
                "instances": [
                    {
                        "attributes": {
                            "id": "subnet-12345678",
                            "tags": [
                                {"Key": "Name", "Value": "PublicSubnet"}
                            ]
                        }
                    }
                ]
            }
        ]
    }
    
    reader._extract_resources_from_state(tag_state, managed_resources)
    
    # 異なる形式のタグが正しく処理されているか確認
    vpc = next(r for r in managed_resources.values() if r["type"] == "aws_vpc")
    subnet = next(r for r in managed_resources.values() if r["type"] == "aws_subnet")
    
    assert isinstance(vpc["tags"], list)
    assert isinstance(subnet["tags"], list)
    assert {"Key": "Name", "Value": "MainVPC"} in vpc["tags"]
    assert {"Key": "Name", "Value": "PublicSubnet"} in subnet["tags"]


def test_resource_mode_handling(mock_session):
    """リソースのモード処理テスト"""
    reader = TerraformStateReader(mock_session)
    managed_resources = {}
    
    # 異なるモードのリソースを含むステートデータ
    mode_state = {
        "version": 4,
        "resources": [
            {
                "mode": "managed",
                "type": "aws_vpc",
                "name": "main",
                "instances": [
                    {
                        "attributes": {
                            "id": "vpc-12345678",
                            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
                        }
                    }
                ]
            },
            {
                "mode": "data",
                "type": "aws_vpc",
                "name": "existing",
                "instances": [
                    {
                        "attributes": {
                            "id": "vpc-87654321",
                            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-87654321"
                        }
                    }
                ]
            },
            {
                "type": "aws_subnet",  # モードなし
                "name": "public",
                "instances": [
                    {
                        "attributes": {
                            "id": "subnet-12345678",
                            "vpc_id": "vpc-12345678"
                        }
                    }
                ]
            }
        ]
    }
    
    reader._extract_resources_from_state(mode_state, managed_resources)
    
    # managedモードのリソースのみが含まれているか確認
    assert len(managed_resources) == 1
    vpc = next(r for r in managed_resources.values() if r["type"] == "aws_vpc")
    assert vpc["id"] == "vpc-12345678"

    # dataモードとモードなしのリソースが除外されているか確認
    data_vpc = next((r for r in managed_resources.values() if r.get("id") == "vpc-87654321"), None)
    assert data_vpc is None
    subnet = next((r for r in managed_resources.values() if r["type"] == "aws_subnet"), None)
    assert subnet is None


def test_get_managed_resources_local_state(mock_session, tmp_path):
    """ローカルtfstateファイルからのリソース読み込みテスト"""
    reader = TerraformStateReader(mock_session)
    
    # テスト用のtfstateファイルを作成
    state_data = {
        "version": 4,
        "resources": [
            {
                "mode": "managed",
                "type": "aws_vpc",
                "name": "main",
                "instances": [
                    {
                        "attributes": {
                            "id": "vpc-12345678",
                            "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
                            "cidr_block": "10.0.0.0/16"
                        }
                    }
                ]
            }
        ]
    }
    
    tf_dir = tmp_path / "terraform"
    tf_dir.mkdir()
    state_file = tf_dir / "terraform.tfstate"
    state_file.write_text(json.dumps(state_data))
    
    # リソースの取得
    managed_resources = reader.get_managed_resources(str(tf_dir))
    
    # 結果の検証
    assert len(managed_resources) == 1
    vpc_identifier = "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
    assert vpc_identifier in managed_resources
    vpc = managed_resources[vpc_identifier]
    assert vpc["type"] == "aws_vpc"
    assert vpc["id"] == "vpc-12345678"
    assert vpc["details"]["cidr_block"] == "10.0.0.0/16"


def test_get_managed_resources_from_fixture(mock_session):
    """テストフィクスチャからのマネージドリソース読み込みテスト"""
    reader = TerraformStateReader(mock_session)
    
    # テストフィクスチャのディレクトリを取得
    fixture_dir = Path(__file__).parent.parent / "fixtures"
    
    # マネージドリソースの取得
    managed_resources = reader.get_managed_resources(str(fixture_dir))
    
    # 結果の検証
    assert len(managed_resources) == 5  # VPC, 2 Load Balancers, Subnet, IAM Role
    
    # VPCリソースの検証
    vpc_identifier = "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
    assert vpc_identifier in managed_resources
    vpc = managed_resources[vpc_identifier]
    assert vpc["type"] == "aws_vpc"
    assert vpc["id"] == "vpc-12345678"
    assert vpc["details"]["cidr_block"] == "10.0.0.0/16"
    assert vpc["details"]["enable_dns_hostnames"] is True
    
    # サブネットリソースの検証
    subnet_identifier = "aws_subnet:subnet-12345678"
    assert subnet_identifier in managed_resources
    subnet = managed_resources[subnet_identifier]
    assert subnet["type"] == "aws_subnet"
    assert subnet["id"] == "subnet-12345678"
    assert subnet["details"]["vpc_id"] == "vpc-12345678"
    assert subnet["details"]["cidr_block"] == "10.0.1.0/24"
    
    # IAMロールリソースの検証
    role_identifier = "arn:aws:iam::123456789012:role/test-role"
    assert role_identifier in managed_resources
    role = managed_resources[role_identifier]
    assert role["type"] == "aws_iam_role"
    assert role["id"] == "test-role"
    assert role["details"]["path"] == "/"
    assert "assume_role_policy" in role["details"]

# def test_get_managed_resources_s3_backend(mock_session, tmp_path):
#     """S3バックエンドからのリソース読み込みテスト"""
#     reader = TerraformStateReader(mock_session)
    
#     # テスト用のTerraformバックエンド設定ファイルを作成
#     tf_dir = tmp_path / "terraform"
#     tf_dir.mkdir()
#     backend_file = tf_dir / "main.tf"
#     backend_file.write_text("""
# terraform {
#   backend "s3" {
#     bucket = "test-bucket"
#     key    = "test/terraform.tfstate"
#     region = "ap-northeast-1"
#   }
# }
# """)

#     # テスト用のステートデータを定義
#     state_data = {
#         "version": 4,
#         "resources": [
#             {
#                 "mode": "managed",
#                 "type": "aws_vpc",
#                 "name": "main",
#                 "instances": [
#                     {
#                         "attributes": {
#                             "id": "vpc-12345678",
#                             "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678",
#                             "cidr_block": "10.0.0.0/16"
#                         }
#                     }
#                 ]
#             }
#         ]
#     }

#     # モックレスポンスの準備
#     encoded_state = json.dumps(state_data).encode('utf-8')
#     print(f"Encoded state (bytes): {encoded_state}")
#     print(f"Decoded state: {encoded_state.decode('utf-8')}")

#     # get_objectのモック設定
#     mock_body = MagicMock()
#     mock_reads = []  # 読み取りの履歴を保存
#     def mock_read():
#         result = encoded_state
#         mock_reads.append(result)
#         print(f"[DEBUG] mock_read called, returning: {result[:100]}")
#         return result
#     mock_body.read = mock_read
    
#     def mock_get_object(**kwargs):
#         print(f"[DEBUG] get_object called with args: {kwargs}")
#         return {"Body": mock_body}
#     s3_client.get_object = MagicMock(side_effect=mock_get_object)
#     print("[DEBUG] Configured get_object mock")

#     # head_objectのモック設定
#     def mock_head_object(**kwargs):
#         print(f"[DEBUG] head_object called with args: {kwargs}")
#         return {"ContentLength": len(encoded_state)}
#     s3_client.head_object = MagicMock(side_effect=mock_head_object)
#     print("[DEBUG] Configured head_object mock")

#     # 例外クラスのモック
#     class MockClientError(Exception):
#         def __init__(self, error_response):
#             self.response = error_response
#     s3_client.exceptions.ClientError = MockClientError
#     print("[DEBUG] Configured ClientError mock")

#     # バックエンド設定のモック
#     mock_backend_config = {
#         "bucket": "test-bucket",
#         "key": "test/terraform.tfstate",
#         "region": "ap-northeast-1"
#     }
#     reader._find_s3_backend = MagicMock(return_value=mock_backend_config)
#     print("[DEBUG] Configured backend mock")

#     print("[DEBUG] About to call get_managed_resources")
#     managed_resources = reader.get_managed_resources(str(tf_dir))
    
#     print(f"[DEBUG] Number of mock_read calls: {len(mock_reads)}")
#     for i, data in enumerate(mock_reads):
#         print(f"[DEBUG] mock_read call {i} returned: {data[:100]}")

#     assert len(managed_resources) == 1
</document_content>
</document>

<document>
<source>tests/unit/test_lb_collector.py</source>
<document_content># tests/unit/test_lb_collector.py

import unittest
from unittest.mock import MagicMock, patch
from terraform_aws_migrator.collectors.aws_network.network import LoadBalancerV2Collector
from typing import Dict, Any

class TestLoadBalancerV2Collector(unittest.TestCase):
    def setUp(self):
        self.collector = LoadBalancerV2Collector()
        self.collector.session = MagicMock()
        self.collector.session.region_name = "dummy-region"
        self.collector._account_id = "000000000000"

    @patch.object(LoadBalancerV2Collector, 'client', new_callable=MagicMock)
    def test_collect_load_balancers_managed(self, mock_client):
        # モックされた client の動作を設定
        mock_paginator = MagicMock()
        mock_client.get_paginator.return_value = mock_paginator
        mock_paginator.paginate.return_value = [
            {
                "LoadBalancers": [
                    {
                        "LoadBalancerName": "dummy-lb",
                        "LoadBalancerArn": "arn:aws:elasticloadbalancing:dummy-region:000000000000:loadbalancer/app/dummy-lb/dummy-id",
                        "Type": "application",
                        "Tags": [
                            {"Key": "dummy_key", "Value": "dummy_value"}
                        ],
                        "DNSName": "dummy-lb.dummy-region.elb.amazonaws.com",
                        "Scheme": "internal",
                        "VpcId": "dummy-vpc",
                        "IdleTimeout": {"TimeoutSeconds": 100},
                        "SecurityGroups": ["dummy-sg"],
                        "AvailabilityZones": [
                            {"SubnetId": "dummy-subnet-1"},
                            {"SubnetId": "dummy-subnet-2"}
                        ],
                        "State": {"Code": "active"},
                        "IpAddressType": "ipv4",
                        "LoadBalancerAttributes": [
                            {"Key": "dummy_attr", "Value": "dummy_value"}
                        ]
                    }
                ]
            }
        ]

        state_data = {
            "version": 4,
            "terraform_version": "dummy-version",
            "serial": 1,
            "lineage": "dummy-lineage",
            "outputs": {},
            "resources": [
                {
                    "module": "dummy-module",
                    "mode": "managed",
                    "type": "aws_lb",
                    "name": "dummy-lb-resource",
                    "provider": "dummy-provider",
                    "instances": [
                        {
                            "schema_version": 0,
                            "attributes": {
                                "arn": "arn:aws:elasticloadbalancing:dummy-region:000000000000:loadbalancer/app/dummy-lb/dummy-id",
                                "arn_suffix": "app/dummy-lb/dummy-id",
                                "client_keep_alive": 100,
                                "desync_mitigation_mode": "dummy-desync",
                                "dns_name": "dummy-lb.dummy-region.elb.amazonaws.com",
                                "drop_invalid_header_fields": False,
                                "enable_cross_zone_load_balancing": True,
                                "enable_deletion_protection": False,
                                "enable_http2": True,
                                "enable_tls_version_and_cipher_suite_headers": False,
                                "enable_waf_fail_open": False,
                                "enable_xff_client_port": False,
                                "enable_zonal_shift": False,
                                "id": "arn:aws:elasticloadbalancing:dummy-region:000000000000:loadbalancer/app/dummy-lb/dummy-id",
                                "idle_timeout": 100,
                                "internal": True,
                                "ip_address_type": "ipv4",
                                "load_balancer_type": "application",
                                "name": "dummy-lb-name",
                                "preserve_host_header": False,
                                "security_groups": [
                                    "dummy-sg"
                                ],
                                "subnets": [
                                    "dummy-subnet-1",
                                    "dummy-subnet-2"
                                ],
                                "tags": {},
                                "tags_all": {
                                    "dummy_tag": "dummy_value"
                                },
                                "vpc_id": "dummy-vpc",
                                "xff_header_processing_mode": "dummy-xff",
                                "zone_id": "dummy-zone"
                            }
                        }
                    ]
                }
            ]
        }

        managed_resources = {}
        self.collector._extract_resources_from_state(state_data, managed_resources)

        dummy_arn = "arn:aws:elasticloadbalancing:dummy-region:000000000000:loadbalancer/app/dummy-lb/dummy-id"
        # 管理リソースとして正しく抽出されていることを確認
        self.assertIn(dummy_arn, managed_resources)
        self.assertTrue(managed_resources[dummy_arn]["managed"])

if __name__ == '__main__':
    unittest.main()
</document_content>
</document>

<document>
<source>tests/unit/collectors/test_network.py</source>
<document_content>import pytest
from unittest.mock import MagicMock

from terraform_aws_migrator.collectors.aws_network.network import LoadBalancerV2Collector

def test_generate_resource_identifier_with_arn(mock_session, sample_state_data, mock_collector):
    collector = mock_collector

    # Test load balancer with ARN
    lb_resource = {
        "type": "aws_lb",
        "id": "test-lb"
    }
    expected_lb_arn = f"arn:aws:elasticloadbalancing:{mock_session.region_name}:{collector.account_id}:loadbalancer/app/{lb_resource['id']}/1234567890"
    assert collector.generate_resource_identifier(lb_resource) == expected_lb_arn

def test_generate_resource_identifier_without_arn(mock_session, sample_state_data, mock_collector):
    collector = mock_collector

    # Test load balancer without ARN
    lb_resource = {
        "type": "aws_lb",  # カンマを追加
        "id": "test-lb"
    }
    expected_lb_arn = f"arn:aws:elasticloadbalancing:{mock_session.region_name}:{collector.account_id}:loadbalancer/app/{lb_resource['id']}/1234567890"
    assert collector.generate_resource_identifier(lb_resource) == expected_lb_arn

def test_collect_resources(mock_session, sample_state_data, mock_collector):
    collector = mock_collector
    
    # テスト用のモックリソースを設定
    collector._mock_resources = [{
        "type": "aws_lb",
        "id": "test-lb",
        "arn": "arn:aws:elasticloadbalancing:ap-northeast-1:123456789012:loadbalancer/app/test-lb/1234567890",
        "tags": [],
        "details": {
            "dns_name": "test-lb.ap-northeast-1.elb.amazonaws.com",
            "scheme": "internet-facing",
            "vpc_id": "vpc-12345678",
            "security_groups": ["sg-12345678"],
            "subnets": ["subnet-11111111", "subnet-22222222"],
            "state": "active",
            "uuid": "1234567890"
        }
    }]
    
    resources = collector.collect()

    # Verify load balancer collection
    lb_resources = [r for r in resources if r["type"] == "aws_lb"]
    assert len(lb_resources) == 1
    
    # リソースの詳細を検証
    lb = lb_resources[0]
    assert lb["id"] == "test-lb"
    assert lb["arn"] == "arn:aws:elasticloadbalancing:ap-northeast-1:123456789012:loadbalancer/app/test-lb/1234567890"
    assert lb["details"]["vpc_id"] == "vpc-12345678"

# 他のテストケースも同様に修正してください
</document_content>
</document>

<document>
<source>tests/unit/collectors/test_base.py</source>
<document_content>import pytest
from terraform_aws_migrator.collectors.base import ResourceCollector


class TestCollector(ResourceCollector):
    """テスト用のコレクタークラス"""
    def get_service_name(self) -> str:
        return "ec2"

    def collect(self, target_resource_type: str = ""):
        return []


def test_basic_identifier_generation(mock_session):
    """基本的なリソース識別子生成のテスト"""
    collector = TestCollector(mock_session)
    
    # ARNを持つリソース
    resource = {
        "type": "aws_vpc",
        "id": "vpc-12345678",
        "arn": "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == "arn:aws:ec2:ap-northeast-1:123456789012:vpc/vpc-12345678"
    
    # ARNを持たないリソース
    resource = {
        "type": "aws_security_group",
        "id": "sg-12345678"
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == "aws_security_group:sg-12345678"


def test_iam_resource_identifier_generation(mock_session):
    """IAMリソースの識別子生成テスト"""
    collector = TestCollector(mock_session)
    
    # ロールポリシーアタッチメント
    resource = {
        "type": "aws_iam_role_policy_attachment",
        "role": "test-role",
        "policy_arn": "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == f"arn:aws:iam::123456789012:role/test-role/arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
    
    # ユーザーポリシー
    resource = {
        "type": "aws_iam_user_policy",
        "user": "test-user",
        "name": "test-policy"
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == "test-user:test-policy"
    
    # ユーザーポリシーアタッチメント
    resource = {
        "type": "aws_iam_user_policy_attachment",
        "user": "test-user",
        "policy_arn": "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == "test-user:arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"


def test_tag_based_identifier_generation(mock_session):
    """タグベースの識別子生成テスト"""
    collector = TestCollector(mock_session)
    
    # 辞書形式のタグ
    resource = {
        "type": "aws_vpc",
        "id": "vpc-12345678",
        "tags": {
            "Name": "MainVPC",
            "Environment": "Production"
        }
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == "aws_vpc:MainVPC:vpc-12345678"
    
    # リスト形式のタグ
    resource = {
        "type": "aws_vpc",
        "id": "vpc-12345678",
        "tags": [
            {"Key": "Name", "Value": "MainVPC"},
            {"Key": "Environment", "Value": "Production"}
        ]
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == "aws_vpc:MainVPC:vpc-12345678"


def test_edge_cases_identifier_generation(mock_session):
    """エッジケースの識別子生成テスト"""
    collector = TestCollector(mock_session)
    
    # 必須フィールドが欠けているリソース
    resource = {
        "id": "vpc-12345678"  # typeなし
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == "vpc-12345678"
    
    resource = {
        "type": "aws_vpc"  # idなし
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == ""
    
    # 空のタグを持つリソース
    resource = {
        "type": "aws_vpc",
        "id": "vpc-12345678",
        "tags": {}
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == "aws_vpc:vpc-12345678"
    
    # 無効なタグ形式
    resource = {
        "type": "aws_vpc",
        "id": "vpc-12345678",
        "tags": "invalid"
    }
    identifier = collector.generate_resource_identifier(resource)
    assert identifier == "aws_vpc:vpc-12345678"


def test_get_service_for_resource_type(mock_session):
    """リソースタイプからサービス名を取得するテスト"""
    collector = TestCollector(mock_session)
    
    # EC2関連のリソース
    assert collector.get_service_for_resource_type("aws_vpc") == "ec2"
    assert collector.get_service_for_resource_type("aws_subnet") == "ec2"
    assert collector.get_service_for_resource_type("aws_instance") == "ec2"
    assert collector.get_service_for_resource_type("aws_vpc_endpoint") == "ec2"
    
    # 他のサービスのリソース
    assert collector.get_service_for_resource_type("aws_s3_bucket") == "s3"
    assert collector.get_service_for_resource_type("aws_iam_role") == "iam"
    assert collector.get_service_for_resource_type("aws_lambda_function") == "lambda"
    
    # 無効なリソースタイプ
    assert collector.get_service_for_resource_type("invalid_type") == ""
    assert collector.get_service_for_resource_type("") == ""


def test_build_arn(mock_session):
    """ARN生成のテスト"""
    collector = TestCollector(mock_session)
    
    # S3バケット
    arn = collector.build_arn("bucket", "my-bucket")
    assert arn == "arn:aws:s3:::my-bucket"
    
    # IAMロール
    class IAMCollector(TestCollector):
        def get_service_name(self) -> str:
            return "iam"
    
    iam_collector = IAMCollector(mock_session)
    arn = iam_collector.build_arn("role", "my-role")
    assert arn == "arn:aws:iam::123456789012:role/my-role"
    
    # 通常のリソース
    arn = collector.build_arn("instance", "i-1234567890abcdef0")
    assert arn == f"arn:aws:ec2:ap-northeast-1:123456789012:instance/i-1234567890abcdef0"
</document_content>
</document>

<document>
<source>htmlcov/status.json</source>
<document_content>{"note":"This file is an internal implementation detail to speed up HTML report generation. Its format can change at any time. You might be looking for the JSON report: https://coverage.rtfd.io/cmd.html#cmd-json","format":5,"version":"7.6.10","globals":"99f478a38ee53fcbb3f159a7963b9865","files":{"z_cf88d2db7849d309___init___py":{"hash":"89bf4981a97495fddecf13f2b5fa0caf","index":{"url":"z_cf88d2db7849d309___init___py.html","file":"terraform_aws_migrator/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":0,"n_excluded":0,"n_missing":0,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cf88d2db7849d309___main___py":{"hash":"f82207b6bb688ef435ca25531d48b2ba","index":{"url":"z_cf88d2db7849d309___main___py.html","file":"terraform_aws_migrator/__main__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":3,"n_excluded":0,"n_missing":3,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cf88d2db7849d309_auditor_py":{"hash":"c523dbb3c5ddf6a9d369c5d346774015","index":{"url":"z_cf88d2db7849d309_auditor_py.html","file":"terraform_aws_migrator/auditor.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":208,"n_excluded":0,"n_missing":27,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cf88d2db7849d309_collection_status_py":{"hash":"89e6726b17044bc70900d4cd696d4106","index":{"url":"z_cf88d2db7849d309_collection_status_py.html","file":"terraform_aws_migrator/collection_status.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":34,"n_excluded":0,"n_missing":34,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_e950f48fc324c2e7___init___py":{"hash":"ba6a70f37a17bb8ea4782493ef913ee9","index":{"url":"z_e950f48fc324c2e7___init___py.html","file":"terraform_aws_migrator/collectors/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":25,"n_excluded":0,"n_missing":4,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_e950f48fc324c2e7_aws_application_py":{"hash":"014134720c0000a2ba0d556c15b90232","index":{"url":"z_e950f48fc324c2e7_aws_application_py.html","file":"terraform_aws_migrator/collectors/aws_application.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":26,"n_excluded":0,"n_missing":10,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_e950f48fc324c2e7_aws_compute_py":{"hash":"e7237eb7ec3606b1e77150183eb04140","index":{"url":"z_e950f48fc324c2e7_aws_compute_py.html","file":"terraform_aws_migrator/collectors/aws_compute.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":128,"n_excluded":0,"n_missing":65,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_e950f48fc324c2e7_aws_database_py":{"hash":"16fb24482a8a257736e98c69f071026a","index":{"url":"z_e950f48fc324c2e7_aws_database_py.html","file":"terraform_aws_migrator/collectors/aws_database.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":66,"n_excluded":0,"n_missing":18,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_a01a1bf0550e1bf5___init___py":{"hash":"36b1f7d08ff69fcbe65300c9d572bc09","index":{"url":"z_a01a1bf0550e1bf5___init___py.html","file":"terraform_aws_migrator/collectors/aws_iam/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":5,"n_excluded":0,"n_missing":0,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_a01a1bf0550e1bf5_group_py":{"hash":"691482c0287e8cc60cb6c81af4c7612e","index":{"url":"z_a01a1bf0550e1bf5_group_py.html","file":"terraform_aws_migrator/collectors/aws_iam/group.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":37,"n_excluded":0,"n_missing":19,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_a01a1bf0550e1bf5_instance_profile_py":{"hash":"3b0f754f10f3451af9ee2f7cdc02807d","index":{"url":"z_a01a1bf0550e1bf5_instance_profile_py.html","file":"terraform_aws_migrator/collectors/aws_iam/instance_profile.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":144,"n_excluded":0,"n_missing":108,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_a01a1bf0550e1bf5_policy_py":{"hash":"26555490849d1a55441d5a2b29dd26e5","index":{"url":"z_a01a1bf0550e1bf5_policy_py.html","file":"terraform_aws_migrator/collectors/aws_iam/policy.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":52,"n_excluded":0,"n_missing":27,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_a01a1bf0550e1bf5_role_py":{"hash":"cb5c2415093935eff6bf1545ffeaa1dd","index":{"url":"z_a01a1bf0550e1bf5_role_py.html","file":"terraform_aws_migrator/collectors/aws_iam/role.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":83,"n_excluded":0,"n_missing":40,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_a01a1bf0550e1bf5_user_py":{"hash":"adcdb3b09e0d0779d4aad1ab4863f591","index":{"url":"z_a01a1bf0550e1bf5_user_py.html","file":"terraform_aws_migrator/collectors/aws_iam/user.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":90,"n_excluded":0,"n_missing":52,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_53ded241bbba5aa0___init___py":{"hash":"6c55fa676f7c8fabf8b192811e659d17","index":{"url":"z_53ded241bbba5aa0___init___py.html","file":"terraform_aws_migrator/collectors/aws_network/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":0,"n_excluded":0,"n_missing":0,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_53ded241bbba5aa0_network_py":{"hash":"015bcce0c2be6a7406b066542363341d","index":{"url":"z_53ded241bbba5aa0_network_py.html","file":"terraform_aws_migrator/collectors/aws_network/network.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":274,"n_excluded":0,"n_missing":142,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_53ded241bbba5aa0_route_py":{"hash":"ca3663f8997df257a7ec8446e283ecb4","index":{"url":"z_53ded241bbba5aa0_route_py.html","file":"terraform_aws_migrator/collectors/aws_network/route.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":97,"n_excluded":0,"n_missing":74,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_53ded241bbba5aa0_vpc_py":{"hash":"496d0fca3c87116afaa4a18fd31d98be","index":{"url":"z_53ded241bbba5aa0_vpc_py.html","file":"terraform_aws_migrator/collectors/aws_network/vpc.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":287,"n_excluded":0,"n_missing":194,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_e950f48fc324c2e7_aws_security_py":{"hash":"c2fd62cb44cec0a0c24f5fd552cce390","index":{"url":"z_e950f48fc324c2e7_aws_security_py.html","file":"terraform_aws_migrator/collectors/aws_security.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":45,"n_excluded":0,"n_missing":15,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_e950f48fc324c2e7_aws_storage_py":{"hash":"9c904fc651cff4f48addbb98bdd718e4","index":{"url":"z_e950f48fc324c2e7_aws_storage_py.html","file":"terraform_aws_migrator/collectors/aws_storage.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":4,"n_excluded":0,"n_missing":4,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_fb2a7edc80d4b66c___init___py":{"hash":"93531c293250f3a4d8a3a9443ab03f71","index":{"url":"z_fb2a7edc80d4b66c___init___py.html","file":"terraform_aws_migrator/collectors/aws_storage/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":4,"n_excluded":0,"n_missing":0,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_fb2a7edc80d4b66c_ebs_py":{"hash":"ae23c9e48bec27bfed463a2ad5234cc4","index":{"url":"z_fb2a7edc80d4b66c_ebs_py.html","file":"terraform_aws_migrator/collectors/aws_storage/ebs.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":32,"n_excluded":0,"n_missing":13,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_fb2a7edc80d4b66c_efs_py":{"hash":"6dd4844e100d8e0568d8a6797524d0ce","index":{"url":"z_fb2a7edc80d4b66c_efs_py.html","file":"terraform_aws_migrator/collectors/aws_storage/efs.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":22,"n_excluded":0,"n_missing":4,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_fb2a7edc80d4b66c_s3_py":{"hash":"a5e737c4bb1b92830846c66094d3d921","index":{"url":"z_fb2a7edc80d4b66c_s3_py.html","file":"terraform_aws_migrator/collectors/aws_storage/s3.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":117,"n_excluded":0,"n_missing":100,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_e950f48fc324c2e7_base_py":{"hash":"a9f478a66c51b28fe0049bf47eb69333","index":{"url":"z_e950f48fc324c2e7_base_py.html","file":"terraform_aws_migrator/collectors/base.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":179,"n_excluded":0,"n_missing":48,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cf88d2db7849d309_exclusion_py":{"hash":"ec69fc36019cca2082e45465c3277c77","index":{"url":"z_cf88d2db7849d309_exclusion_py.html","file":"terraform_aws_migrator/exclusion.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":96,"n_excluded":0,"n_missing":28,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_79bc2eb159d603d3___init___py":{"hash":"8f96e0487d400bed7b4c91e861310773","index":{"url":"z_79bc2eb159d603d3___init___py.html","file":"terraform_aws_migrator/formatters/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":0,"n_excluded":0,"n_missing":0,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_79bc2eb159d603d3_output_formatter_py":{"hash":"b665d8577ebc8b0afdc3024011cb2929","index":{"url":"z_79bc2eb159d603d3_output_formatter_py.html","file":"terraform_aws_migrator/formatters/output_formatter.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":94,"n_excluded":0,"n_missing":94,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_6f1ed9ecb843eceb___init___py":{"hash":"a4a7531befc1181a0708fc7c20cc147e","index":{"url":"z_6f1ed9ecb843eceb___init___py.html","file":"terraform_aws_migrator/generators/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":30,"n_excluded":0,"n_missing":30,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_618e05d3bc92774d___init___py":{"hash":"a249417c306c0754afee02771b45e9a9","index":{"url":"z_618e05d3bc92774d___init___py.html","file":"terraform_aws_migrator/generators/aws_iam/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":21,"n_excluded":0,"n_missing":21,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_618e05d3bc92774d_instance_profile_py":{"hash":"8f2f66b9ed861b0b5707d9aff048d90f","index":{"url":"z_618e05d3bc92774d_instance_profile_py.html","file":"terraform_aws_migrator/generators/aws_iam/instance_profile.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":47,"n_excluded":0,"n_missing":47,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_618e05d3bc92774d_policy_py":{"hash":"910944dc96362b5b49198de602d255ce","index":{"url":"z_618e05d3bc92774d_policy_py.html","file":"terraform_aws_migrator/generators/aws_iam/policy.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":51,"n_excluded":0,"n_missing":51,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_618e05d3bc92774d_role_py":{"hash":"7c9d7423efeaf2aa854cb7457839a09e","index":{"url":"z_618e05d3bc92774d_role_py.html","file":"terraform_aws_migrator/generators/aws_iam/role.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":50,"n_excluded":0,"n_missing":50,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_618e05d3bc92774d_role_policy_attachment_py":{"hash":"b5a2b0e1eb79c60ca255154d82ea591d","index":{"url":"z_618e05d3bc92774d_role_policy_attachment_py.html","file":"terraform_aws_migrator/generators/aws_iam/role_policy_attachment.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":38,"n_excluded":0,"n_missing":38,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_618e05d3bc92774d_user_py":{"hash":"187778d5d9f731d8e0bd9a3c44ef3ebe","index":{"url":"z_618e05d3bc92774d_user_py.html","file":"terraform_aws_migrator/generators/aws_iam/user.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":47,"n_excluded":0,"n_missing":47,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_618e05d3bc92774d_user_policy_py":{"hash":"98e8bbfae952181906f5a3faa12451d8","index":{"url":"z_618e05d3bc92774d_user_policy_py.html","file":"terraform_aws_migrator/generators/aws_iam/user_policy.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":38,"n_excluded":0,"n_missing":38,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_618e05d3bc92774d_user_policy_attachment_py":{"hash":"3fdc1c782ae973757fe0d3bcee4c95af","index":{"url":"z_618e05d3bc92774d_user_policy_attachment_py.html","file":"terraform_aws_migrator/generators/aws_iam/user_policy_attachment.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":38,"n_excluded":0,"n_missing":38,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c___init___py":{"hash":"cb1f5f873eecfdb39645e16beaab9b80","index":{"url":"z_cc5618d91ff2863c___init___py.html","file":"terraform_aws_migrator/generators/aws_network/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":22,"n_excluded":0,"n_missing":22,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_lb_py":{"hash":"d2b41c336cfed9e560420d2ca7858b31","index":{"url":"z_cc5618d91ff2863c_lb_py.html","file":"terraform_aws_migrator/generators/aws_network/lb.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":62,"n_excluded":0,"n_missing":62,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_listener_py":{"hash":"aa670804ac09f955ef57ad3e0d41ba22","index":{"url":"z_cc5618d91ff2863c_listener_py.html","file":"terraform_aws_migrator/generators/aws_network/listener.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":130,"n_excluded":0,"n_missing":130,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_listener_rule_py":{"hash":"263f68e1d5ee6f92df6301327ffab0b1","index":{"url":"z_cc5618d91ff2863c_listener_rule_py.html","file":"terraform_aws_migrator/generators/aws_network/listener_rule.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":92,"n_excluded":0,"n_missing":92,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_nacl_dhcp_py":{"hash":"f266ec7edd65c076bc24b3c18bec51c8","index":{"url":"z_cc5618d91ff2863c_nacl_dhcp_py.html","file":"terraform_aws_migrator/generators/aws_network/nacl_dhcp.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":146,"n_excluded":0,"n_missing":146,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_nat_gateway_py":{"hash":"6ba10ca7c69105927765483ef54ec16e","index":{"url":"z_cc5618d91ff2863c_nat_gateway_py.html","file":"terraform_aws_migrator/generators/aws_network/nat_gateway.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":105,"n_excluded":0,"n_missing":105,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_route_py":{"hash":"c0bc95b1c18a6df8688f5738adf92efb","index":{"url":"z_cc5618d91ff2863c_route_py.html","file":"terraform_aws_migrator/generators/aws_network/route.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":138,"n_excluded":0,"n_missing":138,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_route_table_py":{"hash":"8e8eb307f046a095d8a89bb76bc45fc0","index":{"url":"z_cc5618d91ff2863c_route_table_py.html","file":"terraform_aws_migrator/generators/aws_network/route_table.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":84,"n_excluded":0,"n_missing":84,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_subnet_py":{"hash":"61e38685866c61aaec6434448b34e5a8","index":{"url":"z_cc5618d91ff2863c_subnet_py.html","file":"terraform_aws_migrator/generators/aws_network/subnet.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":69,"n_excluded":0,"n_missing":69,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_target_group_py":{"hash":"8b1ba35a536fcaef1f6b67e8a72b91c2","index":{"url":"z_cc5618d91ff2863c_target_group_py.html","file":"terraform_aws_migrator/generators/aws_network/target_group.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":72,"n_excluded":0,"n_missing":72,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_vpc_py":{"hash":"b7f5470b0bb5aa06e3f7195fd7aa364b","index":{"url":"z_cc5618d91ff2863c_vpc_py.html","file":"terraform_aws_migrator/generators/aws_network/vpc.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":103,"n_excluded":0,"n_missing":103,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cc5618d91ff2863c_vpc_endpoint_py":{"hash":"e6275267b84fc3d9d5506df258f474cd","index":{"url":"z_cc5618d91ff2863c_vpc_endpoint_py.html","file":"terraform_aws_migrator/generators/aws_network/vpc_endpoint.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":84,"n_excluded":0,"n_missing":84,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_6f1ed9ecb843eceb_base_py":{"hash":"bea20dcadddf4da173160aacdf653091","index":{"url":"z_6f1ed9ecb843eceb_base_py.html","file":"terraform_aws_migrator/generators/base.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":109,"n_excluded":0,"n_missing":109,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cf88d2db7849d309_main_py":{"hash":"3ff917846905a281d62a3a8e14311a70","index":{"url":"z_cf88d2db7849d309_main_py.html","file":"terraform_aws_migrator/main.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":183,"n_excluded":0,"n_missing":183,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_cf88d2db7849d309_state_reader_py":{"hash":"976482e62e0d130bdda2f08b1f755985","index":{"url":"z_cf88d2db7849d309_state_reader_py.html","file":"terraform_aws_migrator/state_reader.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":265,"n_excluded":0,"n_missing":139,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_1cae05a581ae8bfd___init___py":{"hash":"8f96e0487d400bed7b4c91e861310773","index":{"url":"z_1cae05a581ae8bfd___init___py.html","file":"terraform_aws_migrator/utils/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":0,"n_excluded":0,"n_missing":0,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_1cae05a581ae8bfd_resource_utils_py":{"hash":"4fb463d8a746b75289aea8d74224f36d","index":{"url":"z_1cae05a581ae8bfd_resource_utils_py.html","file":"terraform_aws_migrator/utils/resource_utils.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":34,"n_excluded":0,"n_missing":34,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}}}}</document_content>
</document>

