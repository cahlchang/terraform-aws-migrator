Project Context for Claude

Generated at: 2025-01-28T18:27:40.544175

Project Structure:
```
├── terraform_aws_migrator
│   ├── collectors
│   │   ├── aws_iam
│   │   │   ├── __init__.py
│   │   │   ├── group.py
│   │   │   ├── instance_profile.py
│   │   │   ├── policy.py
│   │   │   ├── role.py
│   │   │   └── user.py
│   │   ├── aws_storage
│   │   │   ├── __init__.py
│   │   │   ├── ebs.py
│   │   │   ├── efs.py
│   │   │   └── s3.py
│   │   ├── __init__.py
│   │   ├── aws_application.py
│   │   ├── aws_compute.py
│   │   ├── aws_database.py
│   │   ├── aws_network.py
│   │   ├── aws_security.py
│   │   ├── aws_storage.py
│   │   └── base.py
│   ├── formatters
│   │   ├── __init__.py
│   │   └── output_formatter.py
│   ├── generators
│   │   ├── aws_compute
│   │   │   ├── ec2.py
│   │   │   ├── lambda.py
│   │   │   └── security_group.py
│   │   ├── aws_iam
│   │   │   ├── __init__.py
│   │   │   ├── instance_profile.py
│   │   │   ├── policy.py
│   │   │   ├── role.py
│   │   │   ├── role_policy_attachment.py
│   │   │   ├── user.py
│   │   │   ├── user_policy.py
│   │   │   └── user_policy_attachment.py
│   │   ├── aws_network
│   │   │   ├── __init__.py
│   │   │   ├── lb.py
│   │   │   ├── listener.py
│   │   │   ├── listener_rule.py
│   │   │   └── target_group.py
│   │   ├── aws_storage
│   │   │   └── s3.py
│   │   ├── __init__.py
│   │   └── base.py
│   ├── utils
│   │   ├── __init__.py
│   │   └── resource_utils.py
│   ├── __init__.py
│   ├── __main__.py
│   ├── auditor.py
│   ├── collection_status.py
│   ├── exclusion.py
│   ├── main.py
│   └── state_reader.py
├── terraform_aws_migrator.egg-info
│   ├── PKG-INFO
│   ├── SOURCES.txt
│   ├── dependency_links.txt
│   ├── entry_points.txt
│   ├── requires.txt
│   └── top_level.txt
├── tmp
├── LICENSE
├── MANIFEST.in
├── README.md
├── context_creator.py
├── context_sync.py
├── debug.py
├── ignore_exampe.md
├── name_test.txt
├── project_context.txt
├── pyproject.toml
├── rate_limit.py
├── requirements.txt
└── setup.py
```

Project Files:

<document>
<source>context_creator.py</source>
<document_content>import os
import asyncio
from pathlib import Path
from typing import List, Set, Optional
from datetime import datetime

class ProjectContextGenerator:
    def __init__(self, project_path: str, output_path: str):
        self.project_path = Path(project_path)
        self.output_path = Path(output_path)
        self.excluded_dirs = {
            '.venv',
            '__pycache__',
            'node_modules',
            '.git',
            'venv'
        }
        self.excluded_patterns = {
            '.pyc',
            '.pyo',
            '.pyd',
            '.so',
            '.dll',
            '.dylib'
        }

    def should_exclude_path(self, path: Path) -> bool:
        try:
            rel_path = path.relative_to(self.project_path)
        except ValueError:
            return True

        parts = rel_path.parts
        for part in parts:
            if part in self.excluded_dirs:
                return True
            if part.startswith('.') and part != '.':
                return True

        if path.is_file() and path.suffix in self.excluded_patterns:
            return True

        return False

    async def read_file_content(self, file_path: Path) -> str:
        """ファイルを読み込む"""
        try:
            def read_file():
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            
            content = await asyncio.to_thread(read_file)
            return content
        except Exception as e:
            print(f"Warning: Error reading file {file_path}: {e}")
            return ""

    async def generate_project_tree(self, directory: Optional[Path] = None, prefix: str = '') -> str:
        """プロジェクトのツリー構造を生成"""
        if directory is None:
            directory = self.project_path

        tree = ''
        try:
            items = sorted(await asyncio.to_thread(list, directory.iterdir()),
                         key=lambda x: (x.is_file(), x.name))
        except PermissionError:
            return tree

        for i, item in enumerate(items):
            if self.should_exclude_path(item):
                continue

            is_last = i == len(items) - 1
            branch = '└── ' if is_last else '├── '
            tree += f"{prefix}{branch}{item.name}\n"

            if item.is_dir():
                new_prefix = prefix + ('    ' if is_last else '│   ')
                tree += await self.generate_project_tree(item, new_prefix)

        return tree

    async def collect_files(self, extensions: Set[str]) -> dict:
        """指定された拡張子のファイルを収集"""
        files_data = {}
        for file_path in self.project_path.rglob('*'):
            if self.should_exclude_path(file_path):
                continue

            if file_path.is_file() and file_path.suffix in extensions:
                relative_path = str(file_path.relative_to(self.project_path))
                content = await self.read_file_content(file_path)
                files_data[relative_path] = content
        return files_data

    async def format_document_tags(self, files_data: dict) -> str:
        """ファイルデータをドキュメントタグ形式にフォーマット"""
        formatted = ""
        for file_path, content in files_data.items():
            formatted += f"<document>\n<source>{file_path}</source>\n"
            formatted += f"<document_content>{content}</document_content>\n"
            formatted += "</document>\n\n"
        return formatted

    async def generate_context_file(self, extensions: List[str]) -> None:
        """コンテキストファイルを生成"""
        try:
            # プロジェクト構造の取得
            tree = await self.generate_project_tree()
            
            # ファイル内容の収集
            files_data = await self.collect_files(set(extensions))
            
            # 出力ディレクトリの作成
            self.output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # コンテキストファイルの作成
            async with asyncio.Lock():
                def write_file():
                    with open(self.output_path, 'w', encoding='utf-8') as f:
                        f.write("Project Context for Claude\n\n")
                        f.write(f"Generated at: {datetime.now().isoformat()}\n\n")
                        f.write("Project Structure:\n")
                        f.write("```\n")
                        f.write(tree)
                        f.write("```\n\n")
                        f.write("Project Files:\n\n")
                        formatted_files = asyncio.run(self.format_document_tags(files_data))
                        f.write(formatted_files)
                
                await asyncio.to_thread(write_file)
                
            print(f"Context file generated successfully at: {self.output_path}")
            print(f"Total files processed: {len(files_data)}")
                    
        except Exception as e:
            print(f"Error generating context file: {e}")
            raise

    def add_excluded_dir(self, dir_name: str) -> None:
        """除外ディレクトリを追加"""
        self.excluded_dirs.add(dir_name)

    def add_excluded_pattern(self, pattern: str) -> None:
        """除外パターンを追加"""
        self.excluded_patterns.add(pattern)

async def main():
    """使用例"""
    try:
        # プロジェクトパスと出力パスを設定
        project_path = "/Users/morin/git/terraform-aws-migrator"
        output_path = "./project_context.txt"

        # ジェネレーターを初期化
        generator = ProjectContextGenerator(
            project_path=project_path,
            output_path=output_path
        )

        # 必要に応じて除外設定を追加
        generator.add_excluded_dir('build')
        generator.add_excluded_pattern('.cache')

        # コンテキストファイルを生成
        await generator.generate_context_file(
            extensions=[".py", ".md", ".tf", ".json"]
        )

    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == "__main__":
    asyncio.run(main())
</document_content>
</document>

<document>
<source>README.md</source>
<document_content># Terraform AWS Migrator

**Terraform AWS Migrator** is a tool designed to audit AWS resources and identify which ones are not managed by Terraform. It compares resources defined in Terraform state files against the actual resources found in your AWS account, helping you maintain proper resource management and avoid resource drift.

## Features

- **Seamless Terraform State Integration**  
  Automatically reads local and remote (S3-backed) Terraform state files to determine which AWS resources are currently under Terraform's control, ensuring consistency and accuracy in your infrastructure management.

- **Extensive AWS Resource Coverage**  
  Employing a pluggable architecture, the tool uses multiple collectors to discover and map a wide array of AWS services:

  - **Application**:
    - Step Functions State Machines
  - **Compute**:
    - EC2 Instances
    - ECS Clusters and Services
    - Lambda Functions
    - EBS Volumes
    - Virtual Private Clouds (VPC)
    - Security Groups
  - **Database**:
    - RDS Database Instances and Clusters
    - DynamoDB Tables
    - ElastiCache Clusters and Replication Groups
  - **Network**:
    - API Gateway REST APIs
    - API Gateway HTTP/WebSocket APIs
    - CloudFront Distributions
    - Legacy Load Balancers (ELB)
    - Application/Network Load Balancers (ALB/NLB)
    - LB Listeners and Listener Rules
    - LB Target Groups
    - Route 53 Hosted Zones
  - **Security**:
    - IAM Users, Groups, Roles, and Policies
    - KMS Customer-Managed Keys
    - Secrets Manager Secrets
  - **Storage**:
    - S3 Buckets
    - EFS File Systems
    - EBS Volumes

- **Unmanaged Resource Detection**  
  Identifies AWS resources not currently represented in your Terraform state, making it easy to bring these unmanaged components under Infrastructure as Code for streamlined operations.

- **Flexible Output Formats**  
  Outputs findings as both JSON and human-readable text, enabling effortless integration into CI/CD pipelines or direct review, ensuring that results are accessible and actionable.

## Getting Started

### Prerequisites

- Python 3.8+
- AWS credentials configured (e.g., via `aws configure` or environment variables)
- Terraform state files locally or accessible via S3

### Installation

1. pip install from GitHub:

   ```bash
   pip install git+https://github.com/cahlchang/terraform-aws-migrator.git
   ```

## Usage

Run the tool by specifying the directory that contains your Terraform configuration and state files:

```bash
terraform_aws_migrator --tf-dir path/to/terraform --output json
```

Command-line options:

- `--tf-dir`: The directory containing Terraform files and state.
- `--output`: The desired output format (text or json). Defaults to text.
- `--output-file`: Optional path to write the results instead of printing to stdout.
- `--list-resources`: List all supported AWS resource types.

### Example

```bash
terraform_aws_migrator --tf-dir ./terraform-code --output json --output-file unmanaged_resources.json
```

This command will:

- Scan your Terraform directory for state files.
- Retrieve AWS resources using various AWS service collectors.
- Identify which AWS resources are unmanaged.
- Output the result in JSON format to unmanaged_resources.json.

## Project Structure

- `terraform_aws_migrator/`:
  Core implementation files:
  - `auditor.py`: Main logic for auditing and comparing Terraform-managed vs. AWS-discovered resources.
  - `state_reader.py`: Reads Terraform state (local or S3) and extracts managed resource IDs.
  - `collectors/`: Contains AWS service-specific collectors (e.g., aws_compute.py, aws_database.py).
  - `formatters/output_formatter.py`: Formats the final report output.
  - `main.py`: Entry point for the CLI.
- `requirements.txt`:
  Lists Python dependencies for easy setup.
- `setup.py`:
  Enables installation as a Python package.

## Contributing

Contributions are welcome! To contribute:

1. Fork this repository.
2. Create a feature branch.
3. Submit a pull request with your changes.

We appreciate feedback on code quality, performance improvements, or suggestions for additional AWS services and Terraform integrations.

## License

This project is licensed under the MIT License.

## Contact

For questions or suggestions, please open an issue or reach out to the maintainer at kahlua.dane@gmail.com.
</document_content>
</document>

<document>
<source>setup.py</source>
<document_content>from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = [line.strip() for line in fh if line.strip() and not line.startswith("#")]

setup(
    name="terraform-aws-migrator",
    version="0.1.0",
    packages=find_packages(include=['terraform_aws_migrator', 'terraform_aws_migrator.*']),
    include_package_data=True,
    package_data={
        'terraform_aws_migrator': ['*', '**/*'],
    },
    install_requires=requirements,
    entry_points={
        'console_scripts': [
            'terraform_aws_migrator=terraform_aws_migrator.main:main',
        ],
    },
    author="morin_river",
    author_email="kahlua.dane@gmail.com",
    description="A tool to migrate unmanaged AWS resources to Terraform",
    long_description=open("README.md").read(),
    long_description_content_type="text/markdown",
    url="https://github.com/cahlchang/terraform-aws-migrator",
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires='>=3.8',
)
</document_content>
</document>

<document>
<source>debug.py</source>
<document_content># debug_terraform_state.py

import json
import argparse
from pathlib import Path
import boto3
import hcl2

class TerraformStateDebugger:
    def __init__(self):
        self.session = boto3.Session()

    def find_s3_backend(self, tf_dir: str) -> dict:
        """Find S3 backend configuration - only searches main.tf for backend config"""
        main_tf = Path(tf_dir) / "main.tf"
        if not main_tf.exists():
            print("main.tf not found")
            return None

        try:
            with open(main_tf) as f:
                content = hcl2.load(f)
                if 'terraform' not in content:
                    return None
                
                for terraform_block in content['terraform']:
                    if 'backend' not in terraform_block:
                        continue
                    
                    backend = terraform_block['backend']
                    if not isinstance(backend, list):
                        continue
                    
                    for backend_config in backend:
                        if 's3' in backend_config:
                            return backend_config['s3']
        except Exception as e:
            print(f"Error reading main.tf: {e}")
            return None
        
        return None

    def read_s3_state(self, bucket: str, key: str, region: str = None) -> dict:
        """Read Terraform state from S3"""
        print(f"\nReading state from S3: {bucket}/{key}")
        region = region or self.session.region_name
        s3_client = self.session.client('s3', region_name=region)
        
        try:
            response = s3_client.get_object(Bucket=bucket, Key=key)
            return json.loads(response['Body'].read().decode('utf-8'))
        except Exception as e:
            print(f"Error reading S3 state: {e}")
            return None

    def analyze_state(self, state_data: dict) -> dict:
        """Analyze terraform state and return managed resources info"""
        if not state_data:
            print("No state data to analyze")
            return {}

        managed_resources = {}

        # For Terraform 0.13+ format
        if 'resources' in state_data:
            for resource in state_data['resources']:
                module_path = resource.get('module', 'root')
                resource_type = resource.get('type', '')
                resource_name = resource.get('name', '')
                
                # Skip data sources
                if resource.get('mode', '') == "data":
                    continue

                instances = resource.get('instances', [])
                for instance in instances:
                    attributes = instance.get('attributes', {})
                    resource_info = {
                        'type': resource_type,
                        'name': resource_name,
                        'module': module_path,
                        'id': attributes.get('id', 'N/A'),
                        'arn': attributes.get('arn'),
                        'tags': attributes.get('tags', {})
                    }
                    
                    key = f"{module_path}/{resource_type}.{resource_name}"
                    managed_resources[key] = resource_info

        # For older state format
        if 'modules' in state_data:
            for module in state_data['modules']:
                module_path = '.'.join(module.get('path', ['root']))
                resources = module.get('resources', {})
                
                for resource_addr, resource in resources.items():
                    # Skip data sources
                    if resource.get('type', '').startswith('data.'):
                        continue
                    
                    primary = resource.get('primary', {})
                    attributes = primary.get('attributes', {})
                    
                    resource_info = {
                        'type': resource.get('type', ''),
                        'name': resource_addr,
                        'module': module_path,
                        'id': attributes.get('id', 'N/A'),
                        'arn': attributes.get('arn'),
                        'tags': attributes.get('tags', {})
                    }
                    
                    key = f"{module_path}/{resource.get('type')}.{resource_addr}"
                    managed_resources[key] = resource_info

        return managed_resources

    def print_resources(self, resources: dict):
        """Print managed resources in a structured format"""
        if not resources:
            print("No managed resources found")
            return

        print("\nManaged Resources:")
        print("-" * 80)
        
        # Group by module
        by_module = {}
        for key, resource in resources.items():
            module = resource['module']
            if module not in by_module:
                by_module[module] = []
            by_module[module].append(resource)

        for module, module_resources in by_module.items():
            print(f"\nModule: {module}")
            print("-" * 40)
            
            # Sort resources by type and name
            module_resources.sort(key=lambda x: (x['type'], x['name']))
            
            for resource in module_resources:
                print(f"\n{resource['type']}.{resource['name']}:")
                print(f"  ID:  {resource['id']}")
                if resource['arn']:
                    print(f"  ARN: {resource['arn']}")
                if resource['tags']:
                    print("  Tags:")
                    for key, value in resource['tags'].items():
                        print(f"    {key}: {value}")

def main():
    parser = argparse.ArgumentParser(description='Debug Terraform state')
    parser.add_argument('--tf-dir', type=str, required=True,
                       help='Directory containing Terraform files')
    args = parser.parse_args()

    debugger = TerraformStateDebugger()

    # Find S3 backend config
    s3_config = debugger.find_s3_backend(args.tf_dir)
    if s3_config:
        print("\nFound S3 backend configuration:")
        print(f"Bucket: {s3_config['bucket']}")
        print(f"Key: {s3_config['key']}")
        
        # Read and analyze S3 state
        state_data = debugger.read_s3_state(
            bucket=s3_config['bucket'],
            key=s3_config['key'],
            region=s3_config.get('region')
        )
        if state_data:
            resources = debugger.analyze_state(state_data)
            debugger.print_resources(resources)
    
    # Also check local state
    tf_dir_path = Path(args.tf_dir)
    state_files = list(tf_dir_path.glob('**/*.tfstate'))
    for state_file in state_files:
        print(f"\nAnalyzing local state file: {state_file}")
        try:
            with open(state_file) as f:
                state_data = json.load(f)
                resources = debugger.analyze_state(state_data)
                debugger.print_resources(resources)
        except Exception as e:
            print(f"Error reading local state file: {e}")

if __name__ == '__main__':
    main()
</document_content>
</document>

<document>
<source>ignore_exampe.md</source>
<document_content># Setting up .tfignore

## Initial Setup

1. Rename the example file to .tfignore:
```bash
mv ignore_exampe.md .tfignore
```

2. If you're on Windows and having trouble with the rename:
```cmd
ren ignore_exampe.md .tfignore
```

## File Location

Place the .tfignore file in one of these locations:
- In your Terraform project root directory (recommended)
- In the same directory where you run terraform_aws_migrator
- Specify a custom path using the --ignore-file flag:
  ```bash
  terraform_aws_migrator --tf-dir ./terraform --ignore-file /path/to/.tfignore
  ```

## Example Configuration

Here's a recommended .tfignore configuration:

```
# Terraform AWS Resource Exclusions
# Format: 
# - aws_<service>_<resource>:<identifier> (Terraform resource type format)
# - <service>:<resource_type>/* (AWS service format)
# - Direct resource IDs or ARNs
# - Name tag values (full value or pattern)
# - <service>:<name-tag-value> (Service with Name tag format)

# EC2 Resources
aws_instance:i-0123456789abcdef0     # Specific test instance by ID
aws_instance:test-*                  # Instances with Name tag starting with "test-"
prod-backup-*                        # Any resource with Name tag starting with "prod-backup-"
ec2:staging-*                        # EC2 instances with Name tag starting with "staging-"

# VPC Resources with Name Tags
aws_vpc:prod-vpc-*                   # VPCs with Name tag starting with "prod-vpc-"
vpc:test-network-*                   # VPCs with Name tag starting with "test-network-"

# Load Balancer Resources
aws_lb:internal-*                    # Load balancers with Name tag starting with "internal-"
lb:test-alb-*                        # Load balancers with Name tag starting with "test-alb-"

# Lambda Resources
aws_lambda_function:maintenance-*     # Maintenance functions (by ID or Name tag)
lambda:backup-*                       # Backup functions (by ID or Name tag)

# Database Resources
aws_dynamodb_table:audit-*           # Audit tables (by ID or Name tag)
aws_db_instance:*-snapshot           # RDS instances with "-snapshot" suffix (ID or Name tag)
rds:dev-*                            # RDS instances with Name tag starting with "dev-"

# IAM Resources
aws_iam_role:service-*               # Service roles
aws_iam_user:system-*                # System users
aws_iam_group:readonly-*             # Read-only groups
aws_iam_policy:AWS*                  # AWS managed policies
```

## Pattern Formats

The .tfignore file supports these pattern formats:

1. Terraform Resource Type Format (Recommended):
   ```
   aws_iam_role:role-name*
   aws_lambda_function:function-name*
   ```

2. AWS Service Format:
   ```
   iam:role/role-name*
   lambda:function/function-name*
   ```

3. Name Tag Format:
   ```
   prod-*              # Match any resource with Name tag starting with "prod-"
   ec2:test-*          # Match EC2 resources with Name tag starting with "test-"
   aws_instance:dev-*  # Match EC2 instances with Name tag starting with "dev-"
   ```

4. Direct ARN Format:
   ```
   arn:aws:iam::*:role/role-name*
   arn:aws:lambda:*:*:function:function-name*
   ```

## Pattern Matching Rules

1. Resource ID Matching:
   - Patterns match against resource IDs directly
   - Example: `i-1234567890abcdef0`, `vpc-12345678`

2. Name Tag Matching:
   - Patterns match against the value of the "Name" tag
   - Can be used with or without service/resource type prefix
   - More flexible for resources that follow naming conventions

3. Service-Specific Matching:
   - Prefix patterns with service name for more targeted exclusions
   - Example: `ec2:prod-*` only matches EC2 resources

4. Full Resource Type Matching:
   - Most specific matching using complete AWS resource type
   - Example: `aws_instance:prod-*`

## Usage Tips

- Use # for comments
- Patterns are case-sensitive
- * matches any number of characters
- Blank lines are ignored
- Each pattern should be on a new line
- More specific patterns should be listed before general patterns
- When using aws_ prefix format, match the exact Terraform resource type name
- Name tag patterns can be used with or without service/type prefixes
- Service-specific patterns (e.g., ec2:prod-*) are more precise than general patterns (prod-*)

## Command Line Usage

```bash
# Use default .tfignore in current directory
terraform_aws_migrator --tf-dir ./terraform

# Specify custom ignore file location
terraform_aws_migrator --tf-dir ./terraform --ignore-file /path/to/.tfignore
```

## Best Practices

1. Start with specific exclusions:
   ```
   # Specific instance
   aws_instance:i-0123456789abcdef0
   ```

2. Add service-prefixed patterns:
   ```
   # All test EC2 instances
   ec2:test-*
   ```

3. Use Name tag patterns for groups of resources:
   ```
   # All resources tagged as production backups
   prod-backup-*
   ```

4. Document your exclusions with comments:
   ```
   # Development environment resources
   dev-*                  # All dev resources
   ec2:dev-*             # Dev EC2 instances
   aws_rds_cluster:dev-* # Dev RDS clusters
   ```
</document_content>
</document>

<document>
<source>rate_limit.py</source>
<document_content>import asyncio
import time
from typing import Optional
import anthropic
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

class RateLimitedAPIClient:
    def __init__(
        self,
        api_key: str,
        requests_per_minute: int = 50,  # デフォルト値は調整可能
        max_retries: int = 3
    ):
        self.client = anthropic.Client(api_key=api_key)
        self.requests_per_minute = requests_per_minute
        self.max_retries = max_retries
        self.last_request_time = 0
        self.min_interval = 60.0 / requests_per_minute  # 1リクエストあたりの最小間隔（秒）

    async def _wait_for_rate_limit(self):
        """レート制限に基づいて適切な待機時間を確保"""
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        if elapsed < self.min_interval:
            wait_time = self.min_interval - elapsed
            await asyncio.sleep(wait_time)
        
        self.last_request_time = time.time()

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type(anthropic.RateLimitError)
    )
    async def send_message(
        self,
        content: str,
        model: str = "claude-3-opus-20240229",
        max_tokens: Optional[int] = None
    ):
        """
        レート制限を考慮してメッセージを送信
        
        Args:
            content: 送信するメッセージ内容
            model: 使用するモデル
            max_tokens: 最大トークン数
        """
        await self._wait_for_rate_limit()
        
        try:
            response = await self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                messages=[
                    {
                        "role": "user",
                        "content": content
                    }
                ]
            )
            return response
            
        except anthropic.RateLimitError as e:
            print(f"Rate limit exceeded: {e}")
            raise
        except Exception as e:
            print(f"Error sending message: {e}")
            raise

async def batch_process_messages(client: RateLimitedAPIClient, messages: list):
    """
    複数のメッセージを一括処理
    
    Args:
        client: RateLimitedAPIClientインスタンス
        messages: 処理するメッセージのリスト
    """
    results = []
    for message in messages:
        try:
            response = await client.send_message(content=message)
            results.append(response)
            
        except anthropic.RateLimitError:
            print(f"Rate limit reached after {len(results)} messages")
            # 一定時間待機してから再開
            await asyncio.sleep(60)
            continue
            
        except Exception as e:
            print(f"Error processing message: {e}")
            continue
    
    return results

# 使用例
async def main():
    client = RateLimitedAPIClient(
        api_key="your-api-key",
        requests_per_minute=50
    )
    
    messages = [
        "Message 1",
        "Message 2",
        # ... more messages
    ]
    
    results = await batch_process_messages(client, messages)
    print(f"Successfully processed {len(results)} messages")

if __name__ == "__main__":
    asyncio.run(main())
</document_content>
</document>

<document>
<source>context_sync.py</source>
<document_content>import os
import json
import asyncio
import subprocess
from pathlib import Path
from typing import List, Set, Optional, Tuple, Dict
from datetime import datetime
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.api_core import exceptions

SCOPES = ['https://www.googleapis.com/auth/drive.file']
CONFIG_DIR = os.path.expanduser('~/.claudesync')
CONFIG_FILE = os.path.join(CONFIG_DIR, 'config.json')
DEFAULT_CONFIG = {
    "service_account_path": os.path.join(CONFIG_DIR, "service-account.json"),
    "drive_folder_id": "",
    "excluded_dirs": [
        ".venv",
        "__pycache__",
        "node_modules",
        ".git",
        "venv",
        "build"
    ],
    "excluded_patterns": [
        ".pyc",
        ".pyo",
        ".pyd",
        ".so",
        ".dll",
        ".dylib",
        ".cache"
    ]
}

class ConfigManager:
    @staticmethod
    def initialize_config() -> None:
        """設定ディレクトリと設定ファイルの初期化"""
        if not os.path.exists(CONFIG_DIR):
            os.makedirs(CONFIG_DIR)
            
        if not os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'w') as f:
                json.dump(DEFAULT_CONFIG, f, indent=2)
                
            print(f"\nCreated default config file at: {CONFIG_FILE}")
            print("Please update the following settings:")
            print("1. service_account_path: Path to your Google service account JSON file")
            print("2. drive_folder_id: ID of the Google Drive folder (e.g., '1ttkULdSquyogNnL5Nrc4cnYuUIlKkUW2')")
            
    @staticmethod
    def load_config() -> Dict:
        """設定ファイルの読み込み"""
        if not os.path.exists(CONFIG_FILE):
            ConfigManager.initialize_config()
            
        try:
            with open(CONFIG_FILE, 'r') as f:
                config = json.load(f)
                
            # 必須項目の検証
            if not config.get('service_account_path'):
                raise ValueError("service_account_path is not set in config file")
            if not config.get('drive_folder_id'):
                raise ValueError("drive_folder_id is not set in config file")
                
            return config
            
        except Exception as e:
            print(f"\nError loading config file: {e}")
            print(f"Please check the configuration at: {CONFIG_FILE}")
            raise

class ProjectContextGenerator:
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.config = ConfigManager.load_config()
        self.excluded_dirs = set(self.config['excluded_dirs'])
        self.excluded_patterns = set(self.config['excluded_patterns'])

    def add_excluded_dir(self, dir_name: str) -> None:
        """除外ディレクトリを追加"""
        self.excluded_dirs.add(dir_name)

    def add_excluded_pattern(self, pattern: str) -> None:
        """除外パターンを追加"""
        self.excluded_patterns.add(pattern)

    def get_google_drive_service(self):
        """Service Account を使用してGoogle Drive API の認証とサービスを取得"""
        try:
            service_account_path = os.path.expanduser(self.config['service_account_path'])
            if not os.path.exists(service_account_path):
                print(f"\nError: Service account file not found at: {service_account_path}")
                print("\nPlease update the service_account_path in the config file:")
                print(f"Config file location: {CONFIG_FILE}")
                raise FileNotFoundError(f"Service account file not found: {service_account_path}")

            credentials = service_account.Credentials.from_service_account_file(
                service_account_path,
                scopes=SCOPES
            )
            return build('drive', 'v3', credentials=credentials)

        except exceptions.PermissionDenied as e:
            print("\nError: Insufficient permissions")
            print("Required permissions for the service account:")
            print("- https://www.googleapis.com/auth/drive.file (Google Drive File Access)")
            raise

        except Exception as e:
            print(f"\nError initializing Google Drive service: {str(e)}")
            raise

    def get_git_info(self) -> Tuple[str, str]:
        """Gitのアカウント名（organization）とリポジトリ名を取得"""
        try:
            original_dir = os.getcwd()
            os.chdir(self.project_path)

            remote_url = subprocess.check_output(
                ["git", "config", "--get", "remote.origin.url"], 
                text=True
            ).strip()
            
            # URLからアカウント名とリポジトリ名を抽出
            if remote_url.startswith('git@'):
                # SSH形式 (git@github.com:account/repo.git)
                path = remote_url.split(':')[1]
            else:
                # HTTPS形式 (https://github.com/account/repo.git)
                path = remote_url.split('github.com/')[-1]
                
            account_name = path.split('/')[0]
            repo_name = path.split('/')[-1].replace('.git', '')

            os.chdir(original_dir)
            return account_name, repo_name
        except subprocess.CalledProcessError as e:
            print(f"Error getting git info: {e}")
            return "unknown-user", "unknown-repo"

    async def generate_project_tree(self) -> str:
        """プロジェクトのディレクトリ構造をツリー形式で生成"""
        output = []
        
        def should_exclude(path: Path) -> bool:
            """除外すべきパスかどうかを判定"""
            return (any(excluded in path.parts for excluded in self.excluded_dirs) or
                    any(path.name.endswith(pattern) for pattern in self.excluded_patterns))

        def add_to_tree(directory: Path, prefix: str = ""):
            """ディレクトリツリーを再帰的に生成"""
            entries = sorted(directory.iterdir(), key=lambda x: (x.is_file(), x.name))
            
            for i, entry in enumerate(entries):
                if should_exclude(entry):
                    continue
                    
                is_last = i == len(entries) - 1
                current_prefix = "└── " if is_last else "├── "
                next_prefix = "    " if is_last else "│   "
                
                output.append(f"{prefix}{current_prefix}{entry.name}")
                
                if entry.is_dir():
                    add_to_tree(entry, prefix + next_prefix)

        add_to_tree(self.project_path)
        return "\n".join(output)

    async def collect_files(self, extensions: Set[str]) -> List[Tuple[str, str]]:
        """指定された拡張子のファイルを収集"""
        files_data = []

        async def process_file(file_path: Path) -> Optional[Tuple[str, str]]:
            """ファイルの内容を読み込む"""
            try:
                if file_path.suffix not in extensions:
                    return None
                    
                async with asyncio.Lock():
                    content = await asyncio.to_thread(
                        lambda: file_path.read_text(encoding='utf-8')
                    )
                return (str(file_path.relative_to(self.project_path)), content)
            except Exception as e:
                print(f"Error reading file {file_path}: {e}")
                return None

        tasks = []
        for root, _, files in os.walk(self.project_path):
            root_path = Path(root)
            if any(excluded in root_path.parts for excluded in self.excluded_dirs):
                continue

            for file in files:
                file_path = root_path / file
                if any(file_path.name.endswith(pattern) for pattern in self.excluded_patterns):
                    continue
                    
                tasks.append(process_file(file_path))

        results = await asyncio.gather(*tasks)
        return [r for r in results if r is not None]

    async def format_document_tags(self, files_data: List[Tuple[str, str]]) -> str:
        """ファイルデータをドキュメントタグ形式にフォーマット"""
        formatted = []
        
        for file_path, content in files_data:
            formatted.extend([
                "<document>",
                f"<source>{file_path}</source>",
                "<document_content>",
                content,
                "</document_content>",
                "</document>",
                ""
            ])
            
        return "\n".join(formatted)

    async def upload_to_google_drive(self, file_path: Path) -> None:
        """Google Drive にファイルをアップロード"""
        try:
            service = self.get_google_drive_service()
            filename = file_path.name

            # ファイルのMIMEタイプを設定
            media = MediaFileUpload(
                str(file_path),
                mimetype='text/plain',
                resumable=True
            )

            try:
                # 既存のファイルを検索
                results = service.files().list(
                    q=f"name = '{filename}' and '{self.config['drive_folder_id']}' in parents and trashed = false",
                    spaces='drive',
                    fields='files(id, name)'
                ).execute()
                
                existing_files = results.get('files', [])

                if existing_files:
                    # 既存のファイルを更新
                    file_id = existing_files[0]['id']
                    service.files().update(
                        fileId=file_id,
                        media_body=media
                    ).execute()
                    print(f"Updated existing file in Google Drive: {filename}")
                else:
                    # 新規ファイルを作成
                    file_metadata = {
                        'name': filename,
                        'parents': [self.config['drive_folder_id']]
                    }
                    service.files().create(
                        body=file_metadata,
                        media_body=media,
                        fields='id',
                        supportsAllDrives=True
                    ).execute()
                    print(f"Created new file in Google Drive: {filename}")

                # 一時ファイルを削除
                file_path.unlink()
                print(f"Cleaned up temporary file: {file_path}")

            except exceptions.PermissionDenied:
                print("\nError: Unable to access the specified Google Drive folder")
                print(f"Please ensure the service account has access to folder ID: {self.config['drive_folder_id']}")
                raise

        except Exception as e:
            print(f"\nError uploading to Google Drive: {str(e)}")
            raise

    async def generate_and_upload_context(self, extensions: List[str]) -> None:
        """コンテキストファイルを生成してGoogle Driveにアップロード"""
        try:
            # Gitの情報を取得してファイル名を生成
            username, repo_name = self.get_git_info()
            filename = f"{username}-{repo_name}.txt"
            temp_path = self.project_path / "tmp" / filename
            
            # 一時ディレクトリを作成
            temp_path.parent.mkdir(parents=True, exist_ok=True)
            
            # コンテンツを生成
            tree = await self.generate_project_tree()
            files_data = await self.collect_files(set(extensions))
            
            # 一時ファイルに書き込み
            async with asyncio.Lock():
                def write_file():
                    with open(temp_path, 'w', encoding='utf-8') as f:
                        f.write("Project Context for Claude\n\n")
                        f.write(f"Generated at: {datetime.now().isoformat()}\n\n")
                        f.write("Project Structure:\n")
                        f.write("```\n")
                        f.write(tree)
                        f.write("```\n\n")
                        f.write("Project Files:\n\n")
                        formatted_files = asyncio.run(self.format_document_tags(files_data))
                        f.write(formatted_files)
                
                await asyncio.to_thread(write_file)
            
            # Google Driveにアップロード
            await self.upload_to_google_drive(temp_path)
            
            print(f"Context generation and upload completed successfully")
            print(f"Total files processed: {len(files_data)}")
                    
        except Exception as e:
            print(f"Error in generate_and_upload_context: {e}")
            raise

def parse_args():
    """コマンドライン引数をパース"""
    import argparse
    parser = argparse.ArgumentParser(description='Project Context Generator for Claude')
    
    parser.add_argument(
        '-t', '--target',
        required=True,
        help='Target directory path for context generation'
    )
    
    parser.add_argument(
        '-e', '--extensions',
        nargs='+',
        default=['.py', '.md'],
        help='File extensions to include (default: .py .md .tf .json)'
    )
    
    parser.add_argument(
        '--exclude-dirs',
        nargs='+',
        default=[],
        help='Additional directories to exclude'
    )
    
    parser.add_argument(
        '--exclude-patterns',
        nargs='+',
        default=[],
        help='Additional file patterns to exclude'
    )
    
    return parser.parse_args()

async def main():
    """メイン処理"""
    try:
        args = parse_args()
        
        # パスを絶対パスに変換
        target_path = str(Path(args.target).resolve())
        # 拡張子の形式を統一（ドットがない場合は追加）
        extensions = [
            ext if ext.startswith('.') else f'.{ext}'
            for ext in args.extensions
        ]
        
        # ジェネレーターの初期化
        generator = ProjectContextGenerator(
            project_path=target_path
        )

        # 除外設定の追加
        for dir_name in args.exclude_dirs:
            generator.add_excluded_dir(dir_name)
            
        for pattern in args.exclude_patterns:
            generator.add_excluded_pattern(pattern)

        # コンテキストを生成してGoogle Driveにアップロード
        await generator.generate_and_upload_context(extensions=extensions)

    except Exception as e:
        print(f"Error in main: {e}")
        raise SystemExit(1)

if __name__ == "__main__":
    asyncio.run(main())
</document_content>
</document>

<document>
<source>terraform_aws_migrator/state_reader.py</source>
<document_content># terraform_aws_migrator/state_reader.py

import json
from pathlib import Path
from typing import Dict, List, Any, Set, Optional
import boto3
import hcl2
from rich.console import Console
import logging
import traceback

logger = logging.getLogger(__name__)


class TerraformStateReader:
    """Handler for reading and processing Terraform state files"""

    def __init__(self, session: boto3.Session):
        self.session = session
        self.console = Console()
        self._account_id = None

    @property
    def account_id(self):
        if not self._account_id:
            self._account_id = self.session.client("sts").get_caller_identity()[
                "Account"
            ]
        return self._account_id

    def read_backend_config(self, tf_dir: str, progress=None) -> List[Dict[str, Any]]:
        """Reads backend configuration from Terraform files"""
        tf_dir_path = Path(tf_dir)
        backend_config = self._find_s3_backend(tf_dir_path)
        return [{"s3": backend_config}] if backend_config else []

    def _extract_resources_from_state(
        self, state_data: Dict[str, Any], managed_resources: Set[str]
    ):
        """
        Extract resource identifiers from state data
        Args:
            state_data: Terraform state data
            managed_resources: Set to store managed resource identifiers
        """
        try:
            # For Terraform 0.13+ format
            if "resources" in state_data:
                for resource in state_data["resources"]:
                    # Skip data sources
                    if resource.get("mode") == "data":
                        continue

                    resource_type = resource.get("type", "")
                    for instance in resource.get("instances", []):
                        formatted_resource = self._format_resource(
                            resource_type,
                            instance.get("attributes", {}),
                            instance.get("index_key"),
                        )
                        if formatted_resource:
                            # Get the identifier for managed_resources set
                            identifier = self._get_identifier_for_managed_set(
                                formatted_resource
                            )
                            if identifier:
                                managed_resources.add(identifier)

            # For older state format
            if "modules" in state_data:
                for module in state_data["modules"]:
                    resources = module.get("resources", {})
                    for resource_addr, resource in resources.items():
                        # Skip data sources
                        if resource_addr.startswith("data."):
                            continue

                        primary = resource.get("primary", {})
                        attributes = primary.get("attributes", {})
                        resource_type = resource.get("type", "")

                        formatted_resource = self._format_resource(
                            resource_type, attributes, None
                        )
                        if formatted_resource:
                            # Get the identifier for managed_resources set
                            identifier = self._get_identifier_for_managed_set(
                                formatted_resource
                            )
                            if identifier:
                                managed_resources.add(identifier)

        except Exception as e:
            self.console.print(f"[red]Error extracting resources from state: {str(e)}")

    def _get_identifier_for_managed_set(
        self, resource: Dict[str, Any]
    ) -> Optional[str]:
        """
        Get the appropriate identifier for the managed_resources set
        Args:
            resource: Formatted resource dictionary
        Returns:
            String identifier for the managed_resources set
        """
        # Return ARN if available
        if "arn" in resource:
            return resource["arn"]

        # If no ARN, construct identifier from type and id
        resource_type = resource.get("type")
        resource_id = resource.get("id")

        if resource_type and resource_id:
            return f"{resource_type}:{resource_id}"

        return resource.get("id")  # Fallback to just ID if nothing else available

    def _format_resource(
        self, resource_type: str, attributes: Dict[str, Any], index_key: Any = None
    ) -> Optional[Dict[str, Any]]:
        """Format a single resource into our expected structure"""
        try:
            resource_id = self._get_resource_id(resource_type, attributes, index_key)
            if not resource_id:
                return None

            formatted = {
                "id": resource_id,
                "type": resource_type,
                "tags": self._extract_tags(attributes),
                "details": {},
            }

            # Add ARN if available
            if "arn" in attributes:
                formatted["arn"] = attributes["arn"]
            elif resource_type.startswith("aws_iam_"):
                formatted["arn"] = (
                    f"arn:aws:iam::{self.account_id}:{resource_type.replace('aws_', '')}/{resource_id}"
                )

            # Add resource-specific details
            if resource_type == "aws_iam_role":
                formatted["details"].update(
                    {
                        "path": attributes.get("path", "/"),
                        "assume_role_policy": json.loads(
                            attributes.get("assume_role_policy", "{}")
                        ),
                        "description": attributes.get("description", ""),
                        "max_session_duration": attributes.get("max_session_duration"),
                        "permissions_boundary": attributes.get("permissions_boundary"),
                    }
                )
            elif resource_type == "aws_iam_role_policy_attachment":
                formatted["details"].update(
                    {
                        "role": attributes.get("role"),
                        "policy_arn": attributes.get("policy_arn"),
                    }
                )

            return formatted

        except Exception as e:
            logger.error(f"Error formatting resource {resource_type}: {str(e)}")
            return None

    def _get_resource_id(
        self, resource_type: str, attributes: Dict[str, Any], index_key: Any = None
    ) -> Optional[str]:
        """Get the appropriate identifier for a resource"""
        if "id" in attributes:
            return attributes["id"]
        elif "name" in attributes:
            return attributes["name"]
        return None

    def _extract_tags(self, attributes: Dict[str, Any]) -> List[Dict[str, str]]:
        """Extract tags from attributes in a consistent format"""
        tags = []
        if "tags" in attributes:
            if isinstance(attributes["tags"], dict):
                tags.extend(
                    {"Key": k, "Value": v} for k, v in attributes["tags"].items()
                )
            elif isinstance(attributes["tags"], list):
                tags.extend(attributes["tags"])
        return tags

    def _find_s3_backend(self, tf_dir: Path) -> Optional[Dict[str, str]]:
        """Find S3 backend configuration in main.tf"""
        main_tf = tf_dir / "main.tf"
        if not main_tf.exists():
            return None

        try:
            with open(main_tf) as f:
                content = hcl2.load(f)
                if "terraform" not in content:
                    return None

                for terraform_block in content["terraform"]:
                    if "backend" not in terraform_block:
                        continue

                    backend = terraform_block["backend"]
                    if not isinstance(backend, list):
                        continue

                    for backend_config in backend:
                        if "s3" in backend_config:
                            return backend_config["s3"]
        except Exception as e:
            self.console.print(f"[yellow]Error reading main.tf: {str(e)}")
            return None

        return None

    def _get_s3_state(
        self, bucket: str, key: str, region: str
    ) -> Optional[Dict[str, Any]]:
        """Read Terraform state file from S3"""
        try:
            s3_client = self.session.client("s3", region_name=region)
            response = s3_client.get_object(Bucket=bucket, Key=key)
            return json.loads(response["Body"].read().decode("utf-8"))
        except Exception as e:
            self.console.print(
                f"[red]Error reading state file from S3 {bucket}/{key}: {str(e)}"
            )
            return None

    def _read_local_state(self, state_file: Path) -> Optional[Dict[str, Any]]:
        """Read a local Terraform state file"""
        try:
            with open(state_file) as f:
                return json.load(f)
        except Exception as e:
            self.console.print(
                f"[yellow]Error reading state file {state_file}: {str(e)}"
            )
            return None

    # terraform_aws_migrator/state_reader.py

    def get_managed_resources(
        self, tf_dir: str, progress=None
    ) -> Dict[str, Dict[str, Any]]:
        """
        Get all resources managed by Terraform from state files with their complete information

        Args:
            tf_dir: Directory containing Terraform files
            progress: Optional progress callback

        Returns:
            Dictionary of managed resources with their complete information
            Format:
            {
                "resource_identifier": {
                    "id": "example_id",
                    "type": "aws_iam_role",
                    "arn": "arn:aws:iam::...",
                    "tags": [...],
                    "details": {...}
                },
                ...
            }
        """
        managed_resources = {}
        tf_dir_path = Path(tf_dir)
        try:
            # First check S3 backend
            s3_config = self._find_s3_backend(tf_dir_path)
            if s3_config and progress:
                state_data = self._get_s3_state(
                    bucket=s3_config["bucket"],
                    key=s3_config["key"],
                    region=s3_config.get("region", self.session.region_name),
                )
                if state_data:
                    self._extract_resources_from_state(state_data, managed_resources)

            # Then check local state files
            state_files = list(Path(tf_dir).rglob("*.tfstate"))
            for state_file in state_files:
                state_data = self._read_local_state(state_file)
                if state_data:
                    self._extract_resources_from_state(state_data, managed_resources)

            return managed_resources

        except Exception as e:
            self.console.print(f"[red]Error reading Terraform state: {str(e)}")
            return {}

    def _extract_resources_from_state(
        self, state_data: Dict[str, Any], managed_resources: Dict[str, Dict[str, Any]]
    ):
        """
        Extract resource information from state data

        Args:
            state_data: Terraform state data
            managed_resources: Dictionary to store managed resource information
        """
        try:
            if "resources" not in state_data:
                return

            for resource in state_data["resources"]:
                if resource.get("mode") == "data":
                    continue

                resource_type = resource.get("type", "")
                for instance in resource.get("instances", []):
                    attributes = instance.get("attributes", {})

                    if resource_type == "aws_iam_role_policy_attachment":
                        role_name = attributes.get("role")
                        policy_arn = attributes.get("policy_arn")
                        if role_name and policy_arn:
                            identifier = f"arn:aws:iam::{self.account_id}:role/{role_name}/{policy_arn}"
                            managed_resources[identifier] = {
                                "id": identifier,
                                "type": resource_type,
                                "role_name": role_name,
                                "policy_arn": policy_arn,
                            }
                    elif resource_type == "aws_iam_user_policy":
                        user_name = attributes.get("user")
                        policy_name = attributes.get("name")
                        identifier = f"{user_name}:{policy_name}"
                        managed_resources[identifier] = {
                            "id": attributes['id'],
                            "type": resource_type,
                            "user_name": user_name,
                        }
                    elif resource_type == "aws_iam_user_policy_attachment":
                        user_name = attributes.get("user")
                        policy_arn = attributes.get("policy_arn")
                        identifier = f"{user_name}:{policy_arn}"
                        managed_resources[identifier] = {
                            "id": identifier,
                            "type": resource_type,
                            "user_name": user_name,
                            "policy_arn": policy_arn,
                        }
                    else:
                        formatted_resource = self._format_resource(
                            resource_type,
                            attributes,
                            instance.get("index_key"),
                        )
                        if formatted_resource:
                            identifier = self._get_identifier_for_managed_set(
                                formatted_resource
                            )
                            if identifier:
                                managed_resources[identifier] = formatted_resource

        except Exception as e:
            raise e

    def get_s3_state_file(
        self, bucket: str, key: str, region: str, progress=None
    ) -> Dict[str, Any]:
        """Read Terraform state file from S3 (for backward compatibility)"""
        return self._get_s3_state(bucket, key, region) or {}

    def _get_resource_id(
        self, resource_type: str, attributes: Dict[str, Any], index_key: Any = None
    ) -> Optional[str]:
        """Get the appropriate identifier for a resource"""
        if "id" in attributes:
            return attributes["id"]
        elif "name" in attributes:
            return attributes["name"]
        return None

    def _extract_tags(self, attributes: Dict[str, Any]) -> List[Dict[str, str]]:
        """Extract tags from attributes in a consistent format"""
        tags = []
        if "tags" in attributes:
            if isinstance(attributes["tags"], dict):
                tags.extend(
                    {"Key": k, "Value": v} for k, v in attributes["tags"].items()
                )
            elif isinstance(attributes["tags"], list):
                tags.extend(attributes["tags"])
        return tags
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collection_status.py</source>
<document_content># terraform_aws_migrator/collection_status.py

from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime


@dataclass
class CollectionStatus:
    """Track the status of resource collection"""

    service: str
    status: str
    start_time: datetime
    end_time: datetime = None

    @property
    def duration(self) -> str:
        """Format duration as [MM:SS]"""
        if not self.end_time:
            duration = datetime.now() - self.start_time
        else:
            duration = self.end_time - self.start_time

        total_seconds = int(duration.total_seconds())
        minutes = total_seconds // 60
        seconds = total_seconds % 60
        return f"[{minutes:02d}:{seconds:02d}]"


class StatusTracker:
    """Track collection status across multiple services"""

    def __init__(self):
        self.statuses: Dict[str, CollectionStatus] = {}

    def start_collection(self, service: str):
        """Record the start of collection for a service"""
        self.statuses[service] = CollectionStatus(
            service=service, status="Processing", start_time=datetime.now()
        )

    def complete_collection(self, service: str, success: bool = True):
        """Record the completion of collection for a service"""
        if service in self.statuses:
            status = self.statuses[service]
            status.status = "Completed" if success else "Failed"
            status.end_time = datetime.now()

    def get_progress_data(self) -> List[Dict[str, Any]]:
        """Get formatted progress data for all services"""
        progress_data = []
        for status in self.statuses.values():
            progress_data.append(
                {
                    "service": status.service,
                    "status": status.status,
                    "time": status.duration,
                }
            )
        return sorted(
            progress_data, key=lambda x: (x["status"] != "Processing", x["service"])
        )


# Ensure the class is properly exported
__all__ = ["StatusTracker", "CollectionStatus"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/__init__.py</source>
<document_content></document_content>
</document>

<document>
<source>terraform_aws_migrator/exclusion.py</source>
<document_content>import fnmatch
import re
from pathlib import Path
from typing import List, Pattern, Optional
import logging

logger = logging.getLogger(__name__)

class ResourceExclusionConfig:
    """Handles the parsing and matching of resource exclusion patterns"""

    DEFAULT_FILENAME = ".tfignore"

    def __init__(self, exclusion_file: Optional[str] = None):
        self.exclusion_file = exclusion_file or self.DEFAULT_FILENAME
        self.patterns: List[str] = []
        self.regex_patterns: List[Pattern] = []
        self._load_patterns()

    def _load_patterns(self) -> None:
        """Load exclusion patterns from the configuration file"""
        try:
            config_path = Path(self.exclusion_file)
            if not config_path.exists():
                logger.debug(f"No exclusion file found at {self.exclusion_file}")
                return

            with open(config_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        line = line.split("#")[0].strip()
                        if line:
                            self.patterns.append(line)
                            # Convert pattern to regex, handling service prefixes
                            pattern = self._convert_pattern_to_regex(line)
                            self.regex_patterns.append(re.compile(pattern))

            logger.info(f"Loaded {len(self.patterns)} exclusion patterns from {self.exclusion_file}")

        except Exception as e:
            logger.error(f"Error loading exclusion patterns: {str(e)}")
            self.patterns = []
            self.regex_patterns = []

    def _convert_pattern_to_regex(self, pattern: str) -> str:
        """Convert an exclusion pattern to regex, handling both AWS resource types and service prefixes"""
        if ":" in pattern:
            type_or_service, identifier = pattern.split(":", 1)
            # Convert glob pattern to regex
            identifier_pattern = fnmatch.translate(identifier)

            # Handle both aws_ prefixed and non-prefixed patterns
            if type_or_service.startswith("aws_"):
                # For exact AWS resource type matching
                return f"{type_or_service}:{identifier_pattern[:-2]}"
            else:
                # For service prefix matching (e.g., iam:)
                aws_prefix = f"aws_{type_or_service}"
                return f"(({type_or_service}:|{aws_prefix}.*:){identifier_pattern[:-2]})"
        else:
            # If no service prefix, just convert glob pattern
            return fnmatch.translate(pattern)[:-2]  # Remove \Z$ from fnmatch.translate()


    def should_exclude(self, resource: dict) -> bool:
            """
            Check if a resource should be excluded based on the patterns

            Args:
                resource (dict): Resource dictionary containing 'id', 'arn', 'type', 'tags', etc.

            Returns:
                bool: True if the resource should be excluded, False otherwise
            """
            if not self.patterns:
                return False

            # Values to check against patterns
            check_values = set()

            # Add basic identifiers
            resource_id = resource.get("id")
            if resource_id:
                check_values.add(resource_id)

            # Add ARN
            arn = resource.get("arn")
            if arn:
                check_values.add(arn)

            # Add type:id format and terraform resource name
            resource_type = resource.get("type")
            if resource_type and resource_id:
                # Remove 'aws_' prefix if present
                service_name = resource_type.replace("aws_", "", 1).split("_")[0]
                check_values.add(f"{service_name}:{resource_id}")
                # Also add the full type:id format
                check_values.add(f"{resource_type}:{resource_id}")
                
                # Add terraform resource name for EC2 instances
                if resource_type == "aws_instance":
                    name_tag_value = None
                    tags = resource.get("tags", [])
                    if isinstance(tags, list):
                        for tag in tags:
                            if isinstance(tag, dict) and tag.get("Key") == "Name":
                                name_tag_value = tag.get("Value")
                                break
                    elif isinstance(tags, dict):
                        name_tag_value = tags.get("Name")
                    
                    if name_tag_value:
                        # Generate resource name in the same way as EC2InstanceGenerator
                        base_name = name_tag_value.replace("-", "_").replace(" ", "_")
                        short_id = resource_id[-4:] if resource_id else ""
                        terraform_resource_name = f"{base_name}_{short_id}"
                        check_values.add(terraform_resource_name)
                        check_values.add(f"{resource_type}:{terraform_resource_name}")

            # Add values from Name tag if present
            tags = resource.get("tags", [])
            name_tag_value = None
            if isinstance(tags, list):
                for tag in tags:
                    if isinstance(tag, dict) and tag.get("Key") == "Name":
                        name_tag_value = tag.get("Value")
                        break
            elif isinstance(tags, dict):
                name_tag_value = tags.get("Name")

            if name_tag_value:
                # Add name tag value directly
                check_values.add(name_tag_value)
                # Add service:name-tag format
                if resource_type:
                    service_name = resource_type.replace("aws_", "", 1).split("_")[0]
                    check_values.add(f"{service_name}:{name_tag_value}")
                    check_values.add(f"{resource_type}:{name_tag_value}")

            # Check each value against all patterns
            for value in check_values:
                for pattern in self.regex_patterns:
                    if pattern.search(str(value)):
                        logger.debug(f"Resource {value} excluded by pattern {pattern.pattern} (values checked: {check_values})")
                        return True

            return False

    def get_patterns(self) -> List[str]:
        """Return the current list of exclusion patterns"""
        return self.patterns.copy()
</document_content>
</document>

<document>
<source>terraform_aws_migrator/auditor.py</source>
<document_content>import time
from typing import Dict, List, Set, Any
import boto3
import traceback
from rich.console import Console
from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    ProgressColumn,
    Task,
)
from rich.text import Text

from terraform_aws_migrator.collectors.base import registry
from terraform_aws_migrator.state_reader import TerraformStateReader
from terraform_aws_migrator.exclusion import ResourceExclusionConfig

import logging

logger = logging.getLogger(__name__)


class CompactTimeColumn(ProgressColumn):
    """Custom time column that displays elapsed time in a compact format"""

    def __init__(self):
        super().__init__()
        self.start_time = time.time()

    def render(self, task: "Task") -> Text:
        """Render the time column."""
        elapsed = int(time.time() - self.start_time)
        minutes = elapsed // 60
        seconds = elapsed % 60
        return Text(f"[{minutes:02d}:{seconds:02d}]")


class AWSResourceAuditor:
    """Main class for detecting unmanaged AWS resources"""

    def __init__(self, exclusion_file: str = None, target_resource_type: str = None):
        self.session = boto3.Session()
        self.state_reader = TerraformStateReader(self.session)
        self.console = Console()
        self.start_time = None
        self.exclusion_config = ResourceExclusionConfig(exclusion_file)
        self.target_resource_type = target_resource_type
        self.resource_type_mappings = {}

    def get_terraform_managed_resources(self, tf_dir: str, progress=None) -> Set[str]:
        """Get set of resource identifiers managed by Terraform"""
        try:
            managed_resources = self.state_reader.get_managed_resources(
                tf_dir, progress
            )
            self.console.print(
                f"[cyan]Found {len(managed_resources)} managed resources in Terraform state"
            )
            return managed_resources
        except Exception as e:
            self.console.print(f"[red]Error reading Terraform state: {str(e)}")
            return set()

    def _get_relevant_collectors(self):
        """Get collectors based on target_resource_type"""
        collectors = registry.get_collectors(self.session)

        if not self.target_resource_type:
            logger.debug(f"Getting all collectors: {len(collectors)}")
            return collectors

        matching_collectors = []
        for collector in collectors:
            resource_types = collector.get_resource_types()
            service_name = collector.get_service_name()
            
            # apply category filter
            if not self.target_resource_type.startswith("aws_"):
                if self.target_resource_type == service_name:
                    matching_collectors.append(collector)
            # apply resource type filter
            elif self.target_resource_type in resource_types:
                matching_collectors.append(collector)

        if not matching_collectors:
            logger.error(
                f"No collector found supporting resource type or service: {self.target_resource_type}"
            )
            return []

        # Add resource type mappings from all matching collectors
        for collector in matching_collectors:
            self.resource_type_mappings.update(collector.get_resource_types())

        return matching_collectors

    def audit_resources(self, tf_dir: str) -> Dict[str, List[Dict[str, Any]]]:
        """Detect AWS resources that are not managed by Terraform, optionally filtered by type"""
        self.start_time = time.time()
        result: Dict[str, List[Dict[str, Any]]] = {"managed": {}, "unmanaged": {}}

        def get_elapsed_time() -> str:
            elapsed = int(time.time() - self.start_time)
            minutes = elapsed // 60
            seconds = elapsed % 60
            return f"[{minutes:02d}:{seconds:02d}]"

        progress = Progress(
            SpinnerColumn(),
            TextColumn("{task.description}", style="bold blue"),
            CompactTimeColumn(),
            console=self.console,
            expand=False,
            refresh_per_second=10,
        )

        with progress:
            # Get Terraform managed resources
            tf_task = progress.add_task(
                "[yellow]Reading Terraform state...", total=None
            )
            managed_resources = self.get_terraform_managed_resources(tf_dir, progress)
            progress.update(tf_task, completed=True)

            # Group managed resources by service
            for resource in managed_resources.values():
                service_name = (
                    resource.get("type", "").split("_")[1]
                    if resource.get("type", "").startswith("aws_")
                    else "other"
                )
                if service_name not in result["managed"]:
                    result["managed"][service_name] = []
                result["managed"][service_name].append(resource)

            # Initialize collectors
            collectors = self._get_relevant_collectors()
            if not collectors:
                if self.target_resource_type:
                    self.console.print(
                        f"[red]No collectors found for resource type: {self.target_resource_type}"
                    )
                return result

            # Add main AWS resource collection task
            aws_task = progress.add_task(
                "[cyan]Collecting AWS resources...", total=None
            )

            # Process each collector
            for collector in collectors:
                service_name = collector.get_service_name()
                try:
                    # Update progress description
                    progress.update(
                        aws_task,
                        description=f"[cyan]Collecting {service_name} resources...",
                    )

                    # Collect resources, passing target_resource_type if specified
                    resources = collector.collect(
                        target_resource_type=self.target_resource_type
                    )

                    # Filter unmanaged resources
                    unmanaged = self._filter_unmanaged_resources(
                        resources, managed_resources
                    )

                    if unmanaged:
                        type_groups = {}
                        for resource in unmanaged:
                            resource_type = resource.get("type", "unknown")
                            if resource_type not in type_groups:
                                type_groups[resource_type] = []
                            type_groups[resource_type].append(resource)

                        for resource_type, resources_list in type_groups.items():
                            display_name = collector.get_type_display_name(
                                resource_type
                            )
                            self.console.print(
                                f"[green]Found {len(resources_list)} unmanaged {display_name} {get_elapsed_time()}"
                            )

                        # Add to result['unmanaged']
                        if service_name not in result["unmanaged"]:
                            result["unmanaged"][service_name] = []
                        result["unmanaged"][service_name].extend(unmanaged)

                except Exception as e:
                    self.console.print(
                        f"[red]Error collecting {service_name} resources: {str(e)}"
                    )

            # Complete the collection task
            progress.update(aws_task, completed=True)

        # Display total execution time
        total_time = int(time.time() - self.start_time)
        minutes = total_time // 60
        seconds = total_time % 60
        self.console.print(
            f"\n[green]Total execution time: [{minutes:02d}:{seconds:02d}]"
        )

        return result

    def _filter_unmanaged_resources(
        self, resources: List[Dict[str, Any]], managed_resources: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Filter out resources that are managed by Terraform or explicitly excluded"""
        unmanaged = []
        managed_identifiers = set()

        # Create a set of managed resource identifiers
        for resource in managed_resources.values():
            resource_type = resource.get("type")
            identifier = resource.get("arn") or resource.get("id")
            if identifier:
                managed_identifiers.add(identifier)
                # S3Policy (TODO: Refactor this)
                if resource_type == "aws_s3_bucket_policy":
                    bucket_name = identifier
                    bucket_arn = f"arn:aws:s3:::{bucket_name}"
                    managed_identifiers.add(bucket_name)
                    managed_identifiers.add(bucket_arn)
                    logger.debug(f"Added S3 bucket policy identifiers: {bucket_name}, {bucket_arn}")

        for resource in resources:
            resource_type = resource.get("type")
            identifier = resource.get("arn") or resource.get("id")
            
            # TODO: Refactor this
            if resource_type in ["aws_s3_bucket_policy", "aws_s3_bucket_acl"]:
                bucket_name = resource.get("id")
                bucket_arn = f"arn:aws:s3:::{bucket_name}"
                logger.info(f"Checking S3 {resource_type}: {bucket_name}")
                
                is_managed = False
                resource_id_in_state = False
                for state_resource in managed_resources.values():
                    state_type = state_resource.get("type")
                    state_id = state_resource.get("id")
                    
                    if state_type == resource_type and state_id == bucket_name:
                        logger.info(f"Found exact match for {resource_type}:{bucket_name} in state")
                        is_managed = True
                        break
                    
                    if state_type == resource_type:
                        from terraform_aws_migrator.generators.aws_storage.s3 import S3BucketACLGenerator
                        generator = S3BucketACLGenerator()
                        state_resource_name = generator._generate_resource_name(state_id)
                        current_resource_name = generator._generate_resource_name(bucket_name)
                        if state_resource_name == current_resource_name:
                            logger.info(f"Resource name collision detected for {bucket_name}")
                            resource_id_in_state = True
                            break
                
                if resource_id_in_state:
                    logger.info(f"Skipping {resource_type} for {bucket_name} due to resource name collision")
                    continue
                
                if not is_managed:
                    logger.info(f"Found unmanaged {resource_type}: {bucket_name}")
                    if not self.exclusion_config.should_exclude(resource):
                        if self.target_resource_type:
                            if self.target_resource_type.startswith("aws_"):
                                if resource_type == self.target_resource_type:
                                    logger.info(f"Adding unmanaged {resource_type}: {bucket_name}")
                                    unmanaged.append(resource)
                            else:
                                if resource_type and resource_type.startswith(f"aws_{self.target_resource_type}_"):
                                    logger.info(f"Adding unmanaged {resource_type}: {bucket_name}")
                                    unmanaged.append(resource)
                        else:
                            logger.info(f"Adding unmanaged {resource_type}: {bucket_name}")
                            unmanaged.append(resource)
                else:
                    logger.info(f"{resource_type} is managed: {bucket_name}")
            # common case
            elif identifier and identifier not in managed_identifiers:
                if not self.exclusion_config.should_exclude(resource):
                    if self.target_resource_type:
                        resource_type = resource.get("type")
                        if self.target_resource_type.startswith("aws_"):
                            # complete resource type is specified
                            if resource_type == self.target_resource_type:
                                unmanaged.append(resource)
                        else:
                            # category is specified
                            if resource_type and resource_type.startswith(f"aws_{self.target_resource_type}_"):
                                unmanaged.append(resource)
                    else:
                        unmanaged.append(resource)

        return unmanaged
</document_content>
</document>

<document>
<source>terraform_aws_migrator/main.py</source>
<document_content># terraform_aws_migrator/main.py

import argparse
import logging
import traceback
from rich.console import Console
from terraform_aws_migrator.utils.resource_utils import show_supported_resources
from terraform_aws_migrator.auditor import AWSResourceAuditor
from terraform_aws_migrator.formatters.output_formatter import format_output
from terraform_aws_migrator.generators import HCLGeneratorRegistry


logger = logging.getLogger(__name__)

def setup_logging(debug: bool = False):
    """Configure logging settings"""
    # Always suppress boto3/botocore logs unless in debug mode
    logging.getLogger("boto3").setLevel(logging.WARNING)
    logging.getLogger("botocore").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)

    # Set root logger to debug level if requested
    if debug:
        logging.getLogger().setLevel(logging.DEBUG)
    else:
        logging.getLogger().setLevel(logging.WARNING)

    logging.basicConfig(format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")


def main():
    parser = argparse.ArgumentParser(
        description="Detect and migrate AWS resources that are not managed by Terraform"
    )
    parser.add_argument(
        "-t",
        "--tf-dir",
        type=str,
        help="Directory containing Terraform files"
    )
    parser.add_argument(
        "--output",
        type=str,
        choices=["text", "json"],
        default="text",
        help="Output format (text or json)",
    )
    parser.add_argument(
        "--output-file",
        type=str,
        help="Output file path (optional, defaults to stdout)",
    )
    parser.add_argument(
        "--list-resources",
        action="store_true",
        help="List supported resource types"
    )
    parser.add_argument(
        "-i",
        "--ignore-file",
        type=str,
        help="Path to resource exclusion file (default: .tfignore)",
        metavar="FILE",
    )
    parser.add_argument(
        "--type",
        type=str,
        help="Resource type to audit/generate (e.g., aws_iam_role)",
    )
    parser.add_argument(
        "--generate",
        action="store_true",
        help="Generate HCL for unmanaged resources"
    )
    parser.add_argument(
        "--module-prefix",
        type=str,
        help="Module prefix for import commands (e.g., 'my_module')"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )

    args = parser.parse_args()
    setup_logging(args.debug)
    console = Console(stderr=True)

    # Show supported resources if requested
    if args.list_resources:
        show_supported_resources()
        return 0

    # Validate required arguments for resource detection
    if not args.tf_dir:
        console.print(
            "[red]Error: --tf-dir is required when not using --list-resources[/red]"
        )
        return 1

    try:
        # HCL generation mode
        if args.generate:
            if not args.type:
                console.print("[red]Error: --type is required when using --generate")
                return 1

            if not HCLGeneratorRegistry.is_supported(args.type):
                console.print(
                    f"[yellow]Warning: Resource type or category '{args.type}' is not yet supported for HCL generation"
                )
                return 1

            # category mode
            if not args.type.startswith("aws_"):
                generators = HCLGeneratorRegistry.get_generators_for_category(args.type)
                if not generators:
                    console.print(
                        f"[yellow]Warning: No generators found for category '{args.type}'"
                    )
                    return 1

                auditor = AWSResourceAuditor(
                    exclusion_file=args.ignore_file,
                    target_resource_type=args.type
                )
                
                resources_result = auditor.audit_resources(args.tf_dir)
                
                # Store all HCL and import commands
                all_hcl = []
                all_imports = []

                # Process resources for each generator in the category
                for resource_type, generator_class in generators.items():
                    target_resources = {}
                    
                    # Filter resources for this type
                    for service_name, service_resources in resources_result["unmanaged"].items():
                        logger.debug(f"Processing service: {service_name} with {len(service_resources)} resources")
                        for resource in service_resources:
                            current_type = resource.get("type")
                            logger.debug(f"Checking resource type: {current_type} against {resource_type}")
                            if current_type == resource_type:
                                resource_id = resource.get("id")
                                if resource_id:
                                    logger.info(f"Found unmanaged {resource_type}: {resource_id}")
                                    target_resources[resource_id] = resource

                    if not target_resources:
                        logger.debug(f"No unmanaged resources found for type: {resource_type}")
                        continue

                    # Get generator instance
                    generator = generator_class(
                        module_prefix=args.module_prefix,
                        state_reader=auditor.state_reader
                    )

                    # Collect HCL
                    type_hcl = []
                    for resource_id, resource in target_resources.items():
                        logger.info(f"Generating HCL for {resource.get('type')} - {resource_id}")
                        logger.debug(f"Resource details: {resource}")
                        hcl = generator.generate(resource)
                        if hcl:
                            logger.info(f"Successfully generated HCL for {resource_id}")
                            logger.debug(f"Generated HCL:\n{hcl}")
                            type_hcl.append(hcl)
                        else:
                            logger.warning(f"Failed to generate HCL for {resource_id}")

                    if type_hcl:
                        logger.info(f"Adding {len(type_hcl)} HCL blocks for {resource_type}")
                        all_hcl.extend(type_hcl)
                    else:
                        logger.warning(f"No HCL generated for {resource_type}")

                    # Collect import commands
                    for resource_id, resource in target_resources.items():
                        import_cmd = generator.generate_import(resource)
                        if import_cmd:
                            all_imports.append(import_cmd)

                # Output all HCL first
                if all_hcl:
                    hcl_content = "\n\n".join(all_hcl)
                    if args.output_file:
                        with open(args.output_file, "a") as f:
                            f.write(hcl_content + "\n\n")
                    else:
                        console.print(hcl_content)

                # Then output all import commands
                if all_imports:
                    import_content = "\n".join(all_imports)
                    if args.output_file:
                        with open(args.output_file, "a") as f:
                            f.write("\n# Import commands\n" + import_content + "\n")
                    else:
                        console.print("\n# Import commands")
                        console.print(import_content)

            else:
                # complete resource type mode
                auditor = AWSResourceAuditor(
                    exclusion_file=args.ignore_file,
                    target_resource_type=args.type
                )
                
                resources_result = auditor.audit_resources(args.tf_dir)
                target_resources = {}
                
                # Process only unmanaged resources
                for service_resources in resources_result["unmanaged"].values():
                    for resource in service_resources:
                        if resource.get("type") == args.type:
                            resource_id = resource.get("id")
                            if resource_id:
                                target_resources[resource_id] = resource

                # Get generator with module prefix and state reader if specified
                generator = HCLGeneratorRegistry.get_generator(
                    args.type,
                    module_prefix=args.module_prefix,
                    state_reader=auditor.state_reader
                )

                console.print(f"Generating HCL for {len(target_resources)} {args.type} resources")

                # Store all HCL and import commands
                all_hcl = []
                all_imports = []

                # Collect HCL
                for resource_id, resource in target_resources.items():
                    hcl = generator.generate(resource)
                    if hcl:
                        all_hcl.append(hcl)

                # Collect import commands
                for resource_id, resource in target_resources.items():
                    import_cmd = generator.generate_import(resource)
                    if import_cmd:
                        all_imports.append(import_cmd)

                # Output all HCL first
                if all_hcl:
                    hcl_content = "\n\n".join(all_hcl)
                    if args.output_file:
                        with open(args.output_file, "a") as f:
                            f.write(hcl_content + "\n\n")
                    else:
                        console.print(hcl_content)

                # Then output all import commands
                if all_imports:
                    import_content = "\n".join(all_imports)
                    if args.output_file:
                        with open(args.output_file, "a") as f:
                            f.write("\n# Import commands\n" + import_content + "\n")
                    else:
                        console.print("\n# Import commands")
                        console.print(import_content)

        else:
            # Normal mode - now supports --type for filtering
            auditor = AWSResourceAuditor(
                exclusion_file=args.ignore_file,
                target_resource_type=args.type
            )
            resources_result = auditor.audit_resources(args.tf_dir)

            # Format and display the output
            formatted_output = format_output(resources_result["unmanaged"], args.output)

            if args.output_file:
                with open(args.output_file, "w") as f:
                    f.write(formatted_output)
                console.print(f"[green]Detection results written to {args.output_file}")
            else:
                console.print(formatted_output)

    except KeyboardInterrupt:
        console.print("\n[yellow]Detection cancelled by user")
        return 1
    except Exception as e:
        console.print(f"[red]Error during detection: {str(e)}")
        console.print(f"[red]Error during detection: {traceback.format_exc()}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
</document_content>
</document>

<document>
<source>terraform_aws_migrator/__main__.py</source>
<document_content>from .main import main

if __name__ == "__main__":
    main()
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_application.py</source>
<document_content># terraform_aws_migrator/collectors/aws_application.py

from typing import Dict, List, Any
from .base import ResourceCollector, register_collector


@register_collector
class StepFunctionCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "stepfunctions"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_sfn_state_machine": "Step Functions State Machines"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            paginator = self.client.get_paginator("list_state_machines")
            for page in paginator.paginate():
                for state_machine in page["stateMachines"]:
                    # Get detailed information about the state machine
                    try:
                        details = self.client.describe_state_machine(
                            stateMachineArn=state_machine["stateMachineArn"]
                        )
                        tags = self.client.list_tags_for_resource(
                            resourceArn=state_machine["stateMachineArn"]
                        ).get("tags", [])
                    except Exception:
                        details = {}
                        tags = []

                    resources.append(
                        {
                            "type": "aws_sfn_state_machine",
                            "id": state_machine["name"],
                            "arn": state_machine["stateMachineArn"],
                            "tags": tags,
                            "details": {
                                "creation_date": str(state_machine.get("creationDate")),
                                "type": details.get("type"),
                                "status": details.get("status"),
                                "revision_id": details.get("revisionId"),
                            },
                        }
                    )
        except Exception as e:
            print(f"Error collecting Step Functions: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/__init__.py</source>
<document_content># terraform_aws_migrator/collectors/__init__.py

import importlib
from pathlib import Path
from .base import ResourceCollector, register_collector


def _import_collectors():
    current_dir = Path(__file__).parent

    for file_path in current_dir.glob("aws_*.py"):
        module_name = f".{file_path.stem}"
        try:
            importlib.import_module(
                module_name, package="terraform_aws_migrator.collectors"
            )
        except ImportError as e:
            print(f"Warning: Failed to import {module_name}: {e}")

    for dir_path in current_dir.glob("aws_*"):
        if not dir_path.is_dir():
            continue

        for file_path in dir_path.glob("*.py"):
            if file_path.name == "__init__.py":
                continue

            relative_path = file_path.relative_to(current_dir)
            module_name = f".{relative_path.parent.name}.{file_path.stem}"
            try:
                importlib.import_module(
                    module_name, package="terraform_aws_migrator.collectors"
                )
            except ImportError as e:
                print(f"Warning: Failed to import {module_name}: {e}")


_import_collectors()

__all__ = ["ResourceCollector", "register_collector"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_database.py</source>
<document_content># resource_collectors/database.py

from typing import Dict, List, Any
from .base import ResourceCollector, register_collector


@register_collector
class RDSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "rds"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_db_instance": "RDS Database Instances",
            "aws_rds_cluster": "RDS Database Clusters"
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # DB instances
            paginator = self.client.get_paginator("describe_db_instances")
            for page in paginator.paginate():
                for instance in page["DBInstances"]:
                    resources.append(
                        {
                            "type": "instance",
                            "id": instance["DBInstanceIdentifier"],
                            "arn": instance["DBInstanceArn"],
                            "engine": instance["Engine"],
                            "tags": self.client.list_tags_for_resource(
                                ResourceName=instance["DBInstanceArn"]
                            )["TagList"],
                        }
                    )

            # DB clusters
            paginator = self.client.get_paginator("describe_db_clusters")
            for page in paginator.paginate():
                for cluster in page["DBClusters"]:
                    resources.append(
                        {
                            "type": "cluster",
                            "id": cluster["DBClusterIdentifier"],
                            "arn": cluster["DBClusterArn"],
                            "engine": cluster["Engine"],
                            "tags": self.client.list_tags_for_resource(
                                ResourceName=cluster["DBClusterArn"]
                            )["TagList"],
                        }
                    )
        except Exception as e:
            print(f"Error collecting RDS resources: {str(e)}")

        return resources


@register_collector
class DynamoDBCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "dynamodb"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_dynamodb_table": "DynamoDB Tables"
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("list_tables")
            for page in paginator.paginate():
                for table_name in page["TableNames"]:
                    table = self.client.describe_table(TableName=table_name)["Table"]
                    tags = self.client.list_tags_of_resource(
                        ResourceArn=f"arn:aws:dynamodb:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:table/{table_name}"
                    ).get("Tags", [])

                    resources.append(
                        {
                            "type": "table",
                            "id": table_name,
                            "arn": table["TableArn"],
                            "tags": tags,
                        }
                    )
        except Exception as e:
            print(f"Error collecting DynamoDB tables: {str(e)}")

        return resources


@register_collector
class ElastiCacheCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "elasticache"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_elasticache_cluster": "ElastiCache Clusters",
            "aws_elasticache_replication_group": "ElastiCache Replication Groups"
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # Cache clusters
            paginator = self.client.get_paginator("describe_cache_clusters")
            for page in paginator.paginate():
                for cluster in page["CacheClusters"]:
                    resources.append(
                        {
                            "type": "cluster",
                            "id": cluster["CacheClusterId"],
                            "arn": f"arn:aws:elasticache:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:cluster:{cluster['CacheClusterId']}",
                            "engine": cluster["Engine"],
                            "tags": self.client.list_tags_for_resource(
                                ResourceName=f"arn:aws:elasticache:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:cluster:{cluster['CacheClusterId']}"
                            )["TagList"],
                        }
                    )

            # Replication groups
            paginator = self.client.get_paginator("describe_replication_groups")
            for page in paginator.paginate():
                for group in page["ReplicationGroups"]:
                    resources.append(
                        {
                            "type": "replication_group",
                            "id": group["ReplicationGroupId"],
                            "arn": f"arn:aws:elasticache:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:replicationgroup:{group['ReplicationGroupId']}",
                            "tags": self.client.list_tags_for_resource(
                                ResourceName=f"arn:aws:elasticache:{self.session.region_name}:{self.session.client('sts').get_caller_identity()['Account']}:replicationgroup:{group['ReplicationGroupId']}"
                            )["TagList"],
                        }
                    )
        except Exception as e:
            print(f"Error collecting ElastiCache resources: {str(e)}")

        return resources

</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_security.py</source>
<document_content># resource_collectors/security.py

from typing import Dict, List, Any
from .base import ResourceCollector, register_collector


@register_collector
class KMSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "kms"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_kms_key": "KMS Customer-Managed Keys"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("list_keys")
            for page in paginator.paginate():
                for key in page["Keys"]:
                    key_id = key["KeyId"]
                    try:
                        key_info = self.client.describe_key(KeyId=key_id)["KeyMetadata"]
                        if (
                            key_info["KeyManager"] == "CUSTOMER"
                        ):  # Only collect customer-managed keys
                            tags = self.client.list_resource_tags(KeyId=key_id)["Tags"]
                            resources.append(
                                {
                                    "type": "key",
                                    "id": key_id,
                                    "arn": key_info["Arn"],
                                    "tags": tags,
                                }
                            )
                    except self.client.exceptions.NotFoundException:
                        continue
        except Exception as e:
            print(f"Error collecting KMS resources: {str(e)}")

        return resources


@register_collector
class SecretsManagerCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "secretsmanager"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_secretsmanager_secret": "Secrets Manager Secrets"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("list_secrets")
            for page in paginator.paginate():
                for secret in page["SecretList"]:
                    resources.append(
                        {
                            "type": "secret",
                            "id": secret["Name"],
                            "arn": secret["ARN"],
                            "tags": secret.get("Tags", []),
                        }
                    )
        except Exception as e:
            print(f"Error collecting Secrets Manager resources: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_compute.py</source>
<document_content># terraform_aws_migrator/collectors/aws_compute.py

from typing import Dict, List, Any
from .base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class EC2Collector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "ec2"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_instance": "EC2 Instances",
            "aws_vpc": "Virtual Private Clouds",
            "aws_security_group": "Security Groups",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            if not target_resource_type or target_resource_type == "aws_instance":
                resources.extend(self._collect_ec2_instances())

            if not target_resource_type or target_resource_type == "aws_vpc":
                resources.extend(self._collect_vpcs())

            if not target_resource_type or target_resource_type == "aws_security_group":
                resources.extend(self._collect_security_groups())

        except Exception as e:
            logger.error(f"Error collecting EC2 resources: {str(e)}")

        return resources

    def _collect_ec2_instances(self) -> List[Dict[str, Any]]:
        """Collect EC2 instance resources"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_instances")
            for page in paginator.paginate():
                for reservation in page["Reservations"]:
                    for instance in reservation["Instances"]:
                        instance_details = {
                            "type": "aws_instance",
                            "id": instance["InstanceId"],
                            "arn": self.build_arn("instance", instance["InstanceId"]),
                            "tags": instance.get("Tags", []),
                            "details": {
                                "instance_type": instance.get("InstanceType"),
                                "ami": instance.get("ImageId"),
                                "availability_zone": instance.get("Placement", {}).get(
                                    "AvailabilityZone"
                                ),
                                "subnet_id": instance.get("SubnetId"),
                                "vpc_id": instance.get("VpcId"),
                                "key_name": instance.get("KeyName"),
                                "vpc_security_group_ids": [
                                    sg["GroupId"]
                                    for sg in instance.get("SecurityGroups", [])
                                ],
                                "ebs_optimized": instance.get("EbsOptimized", False),
                                "monitoring": instance.get("Monitoring", {}).get(
                                    "State"
                                )
                                == "enabled",
                            },
                        }

                        # Get block device mapping
                        block_devices = []
                        for device in instance.get("BlockDeviceMappings", []):
                            if "Ebs" in device:
                                block_devices.append(
                                    {
                                        "device_name": device["DeviceName"],
                                        "volume_id": device["Ebs"]["VolumeId"],
                                        "delete_on_termination": device["Ebs"].get(
                                            "DeleteOnTermination", True
                                        ),
                                    }
                                )
                        if block_devices:
                            instance_details["details"]["block_devices"] = block_devices

                        # Get IPs
                        if instance.get("PublicIpAddress"):
                            instance_details["details"]["public_ip"] = instance[
                                "PublicIpAddress"
                            ]
                        if instance.get("PrivateIpAddress"):
                            instance_details["details"]["private_ip"] = instance[
                                "PrivateIpAddress"
                            ]

                        resources.append(instance_details)

            logger.debug(f"Found {len(resources)} EC2 instances")
            return resources

        except Exception as e:
            logger.error(f"Error collecting EC2 instances: {str(e)}")
            return []

    def _collect_vpcs(self) -> List[Dict[str, Any]]:
        """Collect VPC resources"""
        resources = []
        try:
            for vpc in self.client.describe_vpcs()["Vpcs"]:
                resources.append(
                    {
                        "type": "aws_vpc",
                        "id": vpc["VpcId"],
                        "arn": self.build_arn("vpc", vpc["VpcId"]),
                        "tags": vpc.get("Tags", []),
                        "details": {
                            "cidr_block": vpc.get("CidrBlock"),
                            "instance_tenancy": vpc.get("InstanceTenancy"),
                            "enable_dns_support": vpc.get("EnableDnsSupport"),
                            "enable_dns_hostnames": vpc.get("EnableDnsHostnames"),
                            "is_default": vpc.get("IsDefault", False),
                        },
                    }
                )

            logger.debug(f"Found {len(resources)} VPCs")
            return resources

        except Exception as e:
            logger.error(f"Error collecting VPCs: {str(e)}")
            return []

    def _collect_security_groups(self) -> List[Dict[str, Any]]:
        """Collect security group resources"""
        resources = []
        try:
            for sg in self.client.describe_security_groups()["SecurityGroups"]:
                resources.append(
                    {
                        "type": "aws_security_group",
                        "id": sg["GroupId"],
                        "arn": self.build_arn("security-group", sg["GroupId"]),
                        "tags": sg.get("Tags", []),
                        "details": {
                            "name": sg["GroupName"],
                            "description": sg.get("Description", ""),
                            "vpc_id": sg.get("VpcId"),
                            "revoke_rules_on_delete": False, #default value
                            "ingress_rules": [
                                {
                                    "from_port": rule.get("FromPort"),
                                    "to_port": rule.get("ToPort"),
                                    "protocol": rule.get("IpProtocol"),
                                    "cidr_blocks": [
                                        ip_range["CidrIp"]
                                        for ip_range in rule.get("IpRanges", [])
                                    ],
                                    "ipv6_cidr_blocks": [
                                        ip_range["CidrIpv6"]
                                        for ip_range in rule.get("Ipv6Ranges", [])
                                    ],
                                    "security_groups": [
                                        sg_ref["GroupId"]
                                        for sg_ref in rule.get("UserIdGroupPairs", [])
                                    ],
                                }
                                for rule in sg.get("IpPermissions", [])
                            ],
                            "egress_rules": [
                                {
                                    "from_port": rule.get("FromPort"),
                                    "to_port": rule.get("ToPort"),
                                    "protocol": rule.get("IpProtocol"),
                                    "cidr_blocks": [
                                        ip_range["CidrIp"]
                                        for ip_range in rule.get("IpRanges", [])
                                    ],
                                    "ipv6_cidr_blocks": [
                                        ip_range["CidrIpv6"]
                                        for ip_range in rule.get("Ipv6Ranges", [])
                                    ],
                                    "security_groups": [
                                        sg_ref["GroupId"]
                                        for sg_ref in rule.get("UserIdGroupPairs", [])
                                    ],
                                }
                                for rule in sg.get("IpPermissionsEgress", [])
                            ],
                        },
                    }
                )

            logger.debug(f"Found {len(resources)} security groups")
            return resources

        except Exception as e:
            logger.error(f"Error collecting security groups: {str(e)}")
            return []


@register_collector
class ECSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "ecs"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_ecs_cluster": "ECS Clusters", "aws_ecs_service": "ECS Services"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # Clusters
            cluster_arns = self.client.list_clusters()["clusterArns"]
            if cluster_arns:
                clusters = self.client.describe_clusters(clusters=cluster_arns)[
                    "clusters"
                ]
                for cluster in clusters:
                    resources.append(
                        {
                            "type": "cluster",
                            "id": cluster["clusterName"],
                            "arn": cluster["clusterArn"],
                            "tags": cluster.get("tags", []),
                        }
                    )

                    # Services in each cluster
                    paginator = self.client.get_paginator("list_services")
                    for page in paginator.paginate(cluster=cluster["clusterName"]):
                        service_arns = page["serviceArns"]
                        if service_arns:
                            services = self.client.describe_services(
                                cluster=cluster["clusterName"], services=service_arns
                            )["services"]
                            for service in services:
                                resources.append(
                                    {
                                        "type": "service",
                                        "id": service["serviceName"],
                                        "arn": service["serviceArn"],
                                        "cluster": cluster["clusterName"],
                                        "tags": service.get("tags", []),
                                    }
                                )

        except Exception as e:
            logger.error(f"Error collecting ECS resources: {str(e)}")

        return resources


@register_collector
class LambdaCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "lambda"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_lambda_function": "Lambda Functions"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            paginator = self.client.get_paginator("list_functions")
            for page in paginator.paginate():
                for function in page["Functions"]:
                    # Get function tags
                    try:
                        tags = self.client.list_tags(
                            Resource=function["FunctionArn"]
                        ).get("Tags", {})
                    except Exception:
                        tags = {}

                    details = {
                        "runtime": function.get("Runtime"),
                        "role": function.get("Role"),
                        "handler": function.get("Handler"),
                        "description": function.get("Description"),
                        "memory_size": function.get("MemorySize"),
                        "timeout": function.get("Timeout"),
                        "last_modified": str(function.get("LastModified")),
                        "version": function.get("Version"),
                        "package_type": function.get("PackageType"),
                        "publish": function.get("Publish", False),
                    }

                    if function.get("PackageType") == "Image":
                        try:
                            function_detail = self.client.get_function(FunctionName=function["FunctionName"])
                            code = function_detail.get("Code", {})
                            logger.debug(f"Lambda function code info: {code}")
                            details["image_uri"] = code.get("ImageUri")
                            if image_config := function_detail.get("ImageConfigResponse"):
                                logger.debug(f"Lambda function image config: {image_config}")
                        except Exception as e:
                            logger.error(f"Error getting function details: {str(e)}")
                            details["image_config"] = {
                                "command": image_config.get("ImageConfig", {}).get("Command"),
                                "entry_point": image_config.get("ImageConfig", {}).get("EntryPoint"),
                                "working_directory": image_config.get("ImageConfig", {}).get("WorkingDirectory"),
                            }

                    if env_vars := function.get("Environment", {}).get("Variables"):
                        details["environment"] = {"variables": env_vars}

                    if vpc_config := function.get("VpcConfig"):
                        details["vpc_config"] = {
                            "subnet_ids": vpc_config.get("SubnetIds", []),
                            "security_group_ids": vpc_config.get("SecurityGroupIds", []),
                        }

                    if layers := function.get("Layers"):
                        details["layers"] = [layer.get("Arn") for layer in layers]

                    if dlq := function.get("DeadLetterConfig"):
                        details["dead_letter_config"] = {
                            "target_arn": dlq.get("TargetArn")
                        }

                    if tracing := function.get("TracingConfig"):
                        details["tracing_config"] = {
                            "mode": tracing.get("Mode")
                        }

                    if fs_configs := function.get("FileSystemConfigs", []):
                        details["file_system_config"] = [{
                            "arn": fs.get("Arn"),
                            "local_mount_path": fs.get("LocalMountPath")
                        } for fs in fs_configs]

                    resources.append({
                        "type": "aws_lambda_function",
                        "id": function["FunctionName"],
                        "arn": function["FunctionArn"],
                        "tags": tags,
                        "details": details,
                    })
        except Exception as e:
            print(f"Error collecting Lambda functions: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage.py</source>
<document_content>from .aws_storage.s3 import S3Collector
from .aws_storage.efs import EFSCollector
from .aws_storage.ebs import EBSCollector

__all__ = ["S3Collector", "EFSCollector", "EBSCollector"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_network.py</source>
<document_content># terraform_aws_migrator/collectors/aws_networking.py

from typing import Dict, List, Any
from .base import ResourceCollector, register_collector

import logging
import json

logger = logging.getLogger(__name__)


@register_collector
class APIGatewayCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "apigateway"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_api_gateway_rest_api": "API Gateway REST APIs"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # REST APIs
            apis = self.client.get_rest_apis()["items"]
            for api in apis:
                resources.append(
                    {
                        "type": "aws_api_gateway_rest_api",
                        "id": api["id"],
                        "name": api["name"],
                        "arn": f"arn:aws:apigateway:{self.session.region_name}::/restapis/{api['id']}",
                        "tags": api.get("tags", {}),
                    }
                )
        except Exception as e:
            print(f"Error collecting API Gateway resources: {str(e)}")

        return resources


@register_collector
class APIGatewayV2Collector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "apigatewayv2"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_apigatewayv2_api": "API Gateway HTTP/WebSocket APIs"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # HTTP and WebSocket APIs
            apis = self.client.get_apis()["Items"]
            for api in apis:
                resources.append(
                    {
                        "type": "aws_apigatewayv2_api",
                        "id": api["ApiId"],
                        "name": api["Name"],
                        "arn": f"arn:aws:apigateway:{self.session.region_name}::/apis/{api['ApiId']}",
                        "tags": api.get("Tags", {}),
                    }
                )
        except Exception as e:
            print(f"Error collecting API Gateway V2 resources: {str(e)}")

        return resources


@register_collector
class Route53Collector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "route53"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_route53_zone": "Route 53 Hosted Zones"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # Hosted zones
            paginator = self.client.get_paginator("list_hosted_zones")
            for page in paginator.paginate():
                for zone in page["HostedZones"]:
                    tags = self.client.list_tags_for_resource(
                        ResourceType="hostedzone",
                        ResourceId=zone["Id"].replace("/hostedzone/", ""),
                    )["ResourceTagSet"]["Tags"]

                    resources.append(
                        {
                            "type": "aws_route53_zone",
                            "id": zone["Id"],
                            "name": zone["Name"],
                            "tags": tags,
                        }
                    )
        except Exception as e:
            print(f"Error collecting Route53 resources: {str(e)}")

        return resources


@register_collector
class CloudFrontCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "cloudfront"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_cloudfront_distribution": "CloudFront Distributions"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("list_distributions")
            for page in paginator.paginate():
                for dist in page["DistributionList"].get("Items", []):
                    tags = self.client.list_tags_for_resource(Resource=dist["ARN"])[
                        "Tags"
                    ]["Items"]

                    resources.append(
                        {
                            "type": "aws_cloudfront_distribution",
                            "id": dist["Id"],
                            "domain_name": dist["DomainName"],
                            "arn": dist["ARN"],
                            "tags": tags,
                        }
                    )
        except Exception as e:
            print(f"Error collecting CloudFront resources: {str(e)}")

        return resources


@register_collector
class LoadBalancerV2Collector(ResourceCollector):
    """Collector for ALB/NLB and related resources (ELBv2)"""

    @classmethod
    def get_service_name(self) -> str:
        return "elbv2"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_lb": "Application and Network Load Balancers",
            "aws_lb_target_group": "Target Groups for ALB/NLB",
            "aws_lb_listener": "Listeners for ALB/NLB",
            "aws_lb_listener_rule": "Routing rules for ALB listeners",
        }

    @classmethod
    def get_resource_service_mappings(cls) -> Dict[str, str]:
        return {
            "aws_lb_target_group": "elbv2",
            "aws_lb": "elbv2",
            "aws_lb_listener": "elbv2",
            "aws_lb_listener_rule": "elbv2",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            # If a specific resource type is requested, collect only that type
            if target_resource_type:
                if target_resource_type == "aws_lb":
                    resources.extend(self._collect_load_balancers())
                elif target_resource_type == "aws_lb_listener":
                    resources.extend(self._collect_listeners())
                elif target_resource_type == "aws_lb_listener_rule":
                    resources.extend(self._collect_listener_rules())
                elif target_resource_type == "aws_lb_target_group":
                    resources.extend(self._collect_target_groups())
            else:
                # Collect all ALB-related resources
                resources.extend(self._collect_load_balancers())
                resources.extend(self._collect_listeners())
                resources.extend(self._collect_listener_rules())
                resources.extend(self._collect_target_groups())

        except Exception as e:
            logger.error(f"Error collecting ALB resources: {str(e)}")
            raise e

        return resources

    def _collect_load_balancers(self) -> List[Dict[str, Any]]:
        """Collect Application Load Balancers"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_load_balancers")
            for page in paginator.paginate():
                for lb in page["LoadBalancers"]:
                    if lb["Type"] == "application":  # Only collect ALBs
                        try:
                            # Get tags
                            tags_response = self.client.describe_tags(
                                ResourceArns=[lb["LoadBalancerArn"]]
                            )
                            tags = (
                                tags_response["TagDescriptions"][0]["Tags"]
                                if tags_response["TagDescriptions"]
                                else []
                            )

                            resources.append({
                                "type": "aws_lb",
                                "id": lb["LoadBalancerName"],
                                "arn": lb["LoadBalancerArn"],
                                "tags": tags,
                                "details": {
                                    "dns_name": lb.get("DNSName"),
                                    "scheme": lb.get("Scheme"),
                                    "vpc_id": lb.get("VpcId"),
                                    "idle_timeout": int(
                                        next(
                                            (attr["Value"] for attr in self.client.describe_load_balancer_attributes(
                                                LoadBalancerArn=lb["LoadBalancerArn"]
                                            )["Attributes"] if attr["Key"] == "idle_timeout.timeout_seconds"),
                                            60  # default value if not found
                                        )
                                    ),
                                    "security_groups": lb.get("SecurityGroups", []),
                                    "subnets": [
                                        az["SubnetId"]
                                        for az in lb.get("AvailabilityZones", [])
                                    ],
                                    "state": lb.get("State", {}).get("Code"),
                                    "ip_address_type": lb.get("IpAddressType"),
                                },
                            })
                        except Exception as e:
                            logger.error(f"Error collecting tags for ALB {lb['LoadBalancerName']}: {str(e)}")

        except Exception as e:
            logger.error(f"Error collecting Application Load Balancers: {str(e)}")

        return resources

    def _collect_target_groups(self) -> List[Dict[str, Any]]:
        """Collect Target Groups and their attachments"""
        resources = []
        try:
            # Collect Target Groups
            paginator = self.client.get_paginator("describe_target_groups")
            for page in paginator.paginate():
                for tg in page["TargetGroups"]:
                    try:
                        # Get tags
                        tags_response = self.client.describe_tags(
                            ResourceArns=[tg["TargetGroupArn"]]
                        )
                        tags = (
                            tags_response["TagDescriptions"][0]["Tags"]
                            if tags_response["TagDescriptions"]
                            else []
                        )
                    except Exception:
                        tags = []

                    # Build health check configuration
                    health_check = None
                    if tg.get("HealthCheckEnabled"):
                        health_check = {
                            "enabled": True,
                            "path": tg.get("HealthCheckPath", "/"),
                            "interval": tg.get("HealthCheckIntervalSeconds"),
                            "timeout": tg.get("HealthCheckTimeoutSeconds"),
                            "healthy_threshold": tg.get("HealthyThresholdCount"),
                            "unhealthy_threshold": tg.get("UnhealthyThresholdCount"),
                            "matcher": tg.get("Matcher", {}).get("HttpCode", "200"),
                        }
                        if tg.get("HealthCheckProtocol"):
                            health_check["protocol"] = tg["HealthCheckProtocol"]
                        if tg.get("HealthCheckPort"):
                            health_check["port"] = tg["HealthCheckPort"]

                    # Build resource details
                    resource = {
                        "type": "aws_lb_target_group",
                        "id": tg["TargetGroupName"],
                        "arn": tg["TargetGroupArn"],
                        "protocol": tg.get("Protocol"),
                        "port": tg.get("Port"),
                        "vpc_id": tg.get("VpcId"),
                        "target_type": tg.get("TargetType"),
                        "tags": tags,
                    }

                    # Add target group attributes
                    try:
                        attrs = self.client.describe_target_group_attributes(
                            TargetGroupArn=tg["TargetGroupArn"]
                        )["Attributes"]

                        for attr in attrs:
                            if attr["Key"] == "deregistration_delay.timeout_seconds":
                                resource["deregistration_delay"] = int(attr["Value"])
                            elif attr["Key"] == "lambda.multi_value_headers.enabled":
                                resource["lambda_multi_value_headers_enabled"] = (
                                    attr["Value"].lower() == "true"
                                )
                            elif attr["Key"] == "proxy_protocol_v2.enabled":
                                resource["proxy_protocol_v2"] = (
                                    attr["Value"].lower() == "true"
                                )
                            elif attr["Key"] == "slow_start.duration_seconds":
                                resource["slow_start"] = int(attr["Value"])
                    except Exception as e:
                        logger.warning(
                            f"Failed to get target group attributes for {tg['TargetGroupName']}: {str(e)}"
                        )

                    # Add health check configuration if any
                    if health_check:
                        resource["health_check"] = health_check

                    resources.append(resource)

        except Exception as e:
            logger.error(f"Error collecting target groups: {str(e)}", exc_info=True)

        return resources


    def _collect_listeners(self) -> List[Dict[str, Any]]:
        resources:List = []
        if not hasattr(self, 'client'):
            logger.error("No ELBv2 client available")
            return resources

        try:
            paginator = self.client.get_paginator("describe_load_balancers")
            lb_pages = paginator.paginate()
        except Exception as e:
            logger.error(f"Failed to create load balancer paginator: {e}")
            raise e

        for page in lb_pages:
            for lb in page.get("LoadBalancers", []):
                lb_arn = lb.get("LoadBalancerArn")
                if not lb_arn:
                    continue

                try:
                    listener_paginator = self.client.get_paginator("describe_listeners")
                    listener_pages = listener_paginator.paginate(LoadBalancerArn=lb_arn)
                except Exception as e:
                    logger.error(f"Failed to get listeners for LB {lb_arn}: {e}")
                    raise e

                for listener_page in listener_pages:
                    for listener in listener_page.get("Listeners", []):
                        listener_arn = listener.get("ListenerArn")
                        if not listener_arn:
                            continue

                        tags = []
                        try:
                            tags_response = self.client.describe_tags(
                                ResourceArns=[listener_arn]
                            )
                            if tags_response.get("TagDescriptions"):
                                tags = tags_response["TagDescriptions"][0].get("Tags", [])
                        except Exception as e:
                            logger.debug(f"Failed to get tags for listener {listener_arn}: {e}")

                        # Create resource with all available information
                        resources.append({
                            "type": "aws_lb_listener",
                            "id": listener_arn.split("/")[-1],
                            "arn": listener_arn,
                            "tags": tags,
                            "details": {
                                "load_balancer_arn": lb_arn,
                                "port": listener.get("Port"),
                                "protocol": listener.get("Protocol"),
                                "ssl_policy": listener.get("SslPolicy"),
                                "certificates": listener.get("Certificates", []),
                                "actions": listener.get("DefaultActions", [])
                            }
                        })
        return resources

    def _collect_listener_rules(self) -> List[Dict[str, Any]]:
        """Collect Rules for ALB Listeners"""
        resources = []
        try:
            paginator = self.client.get_paginator("describe_load_balancers")
            for page in paginator.paginate():
                for lb in page["LoadBalancers"]:
                    try:
                        listener_paginator = self.client.get_paginator("describe_listeners")
                        for listener_page in listener_paginator.paginate(LoadBalancerArn=lb["LoadBalancerArn"]):
                            for listener in listener_page["Listeners"]:
                                try:
                                    rules = self.client.describe_rules(
                                        ListenerArn=listener["ListenerArn"]
                                    ).get("Rules", [])
                                    
                                    for rule in rules:
                                        # Skip default rules
                                        if rule.get("IsDefault", False):
                                            continue
                                            
                                        # Get rule tags
                                        try:
                                            tags_response = self.client.describe_tags(
                                                ResourceArns=[rule["RuleArn"]]
                                            )
                                            tags = (
                                                tags_response["TagDescriptions"][0]["Tags"]
                                                if tags_response["TagDescriptions"]
                                                else []
                                            )
                                        except Exception:
                                            tags = []

                                        resources.append({
                                            "type": "aws_lb_listener_rule",
                                            "id": rule["RuleArn"].split("/")[-1],
                                            "arn": rule["RuleArn"],
                                            "tags": tags,
                                            "details": {
                                                "listener_arn": listener["ListenerArn"],
                                                "priority": rule.get("Priority"),
                                                "conditions": rule.get("Conditions", []),
                                                "actions": rule.get("Actions", []),
                                            },
                                        })
                                except Exception as e:
                                    logger.error(f"Error collecting rules for listener {listener['ListenerArn']}: {e}")
                    except Exception as e:
                        logger.error(f"Error collecting rules for LB {lb['LoadBalancerArn']}: {e}")
        except Exception as e:
            logger.error(f"Error collecting listener rules: {e}")
        return resources

@register_collector
class ClassicLoadBalancerCollector(ResourceCollector):
    """Collector for Classic Load Balancers (ELB)"""

    @classmethod
    def get_service_name(self) -> str:
        return "elb"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_elb": "Legacy Load Balancers"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            paginator = self.client.get_paginator("describe_load_balancers")
            for page in paginator.paginate():
                for lb in page["LoadBalancerDescriptions"]:
                    # Get tags
                    try:
                        tags_response = self.client.describe_tags(
                            LoadBalancerNames=[lb["LoadBalancerName"]]
                        )
                        tags = (
                            tags_response["TagDescriptions"][0]["Tags"]
                            if tags_response["TagDescriptions"]
                            else []
                        )
                    except Exception:
                        tags = []

                    resources.append(
                        {
                            "type": "aws_elb",
                            "id": lb["LoadBalancerName"],
                            "arn": f"arn:aws:elasticloadbalancing:{self.session.region_name}:{self.account_id}:loadbalancer/{lb['LoadBalancerName']}",
                            "tags": tags,
                            "details": {
                                "dns_name": lb.get("DNSName"),
                                "scheme": lb.get("Scheme"),
                                "vpc_id": lb.get("VPCId"),
                                "subnets": lb.get("Subnets", []),
                                "security_groups": lb.get("SecurityGroups", []),
                                "instances": [
                                    instance["InstanceId"]
                                    for instance in lb.get("Instances", [])
                                ],
                                "listeners": [
                                    {
                                        "protocol": listener.get("Protocol"),
                                        "load_balancer_port": listener.get(
                                            "LoadBalancerPort"
                                        ),
                                        "instance_protocol": listener.get(
                                            "InstanceProtocol"
                                        ),
                                        "instance_port": listener.get("InstancePort"),
                                        "ssl_certificate_id": listener.get(
                                            "SSLCertificateId"
                                        ),
                                    }
                                    for listener in lb.get("ListenerDescriptions", [])
                                ],
                                "health_check": lb.get("HealthCheck"),
                            },
                        }
                    )

            if self.progress_callback:
                self.progress_callback("elb", "Completed", len(resources))

        except Exception as e:
            if self.progress_callback:
                self.progress_callback("elb", f"Error: {str(e)}", 0)

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/base.py</source>
<document_content>from abc import ABC, abstractmethod
from typing import Dict, List, Any, Callable, Optional
import boto3
import logging

logger = logging.getLogger(__name__)


class ResourceCollector(ABC):
    """Base class for AWS resource collectors"""

    def __init__(
        self,
        session: boto3.Session = None,
        progress_callback: Optional[Callable] = None,
    ):
        self._client = None
        self._account_id = None
        self._region = None
        self.session = session or boto3.Session()
        self.progress_callback = progress_callback
        logger.debug(f"Initializing collector: {self.__class__.__name__}")

    @abstractmethod
    def get_service_name(self) -> str:
        """Return the AWS service name for this collector"""
        raise NotImplementedError("Subclasses must implement get_service_name")

    @classmethod
    def get_resource_types(cls) -> Dict[str, str]:
        """Return dictionary of resource types supported by this collector"""
        return {}

    @classmethod
    def get_type_display_name(cls, resource_type: str) -> str:
        """Get display name for a resource type"""
        resource_types = cls.get_resource_types()
        return resource_types.get(resource_type, resource_type)

    @property
    def client(self):
        if self._client is None:
            self._client = self.session.client(self.get_service_name())
            logger.debug(f"Created client for service: {self.get_service_name()}")
        return self._client

    @property
    def account_id(self):
        if self._account_id is None:
            self._account_id = self.session.client("sts").get_caller_identity()[
                "Account"
            ]
        return self._account_id

    @property
    def region(self):
        """Get current AWS region"""
        if self._region is None:
            self._region = self.session.region_name
        return self._region

    @abstractmethod
    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        """Collect resources for the service"""
        pass

    @staticmethod
    def extract_tags(tags: List[Dict[str, str]]) -> Dict[str, str]:
        """Convert AWS tags list to dictionary"""
        return {tag["Key"]: tag["Value"] for tag in tags} if tags else {}

    @classmethod
    def get_resource_service_mappings(cls) -> Dict[str, str]:
        """Return dictionary of resource type to service name mappings"""
        return {}

    def build_arn(self, resource_type: str, resource_id: str) -> str:
        """Build ARN for a resource"""
        service = self.get_service_name()
        account = self.account_id
        region = self.region

        if service == "s3":
            return f"arn:aws:s3:::{resource_id}"
        elif service == "iam":
            return f"arn:aws:iam::{account}:{resource_type}/{resource_id}"
        else:
            return f"arn:aws:{service}:{region}:{account}:{resource_type}/{resource_id}"


class CollectorRegistry:
    """Registry for resource collectors"""

    def __init__(self):
        self.collectors = []

    def register(self, collector_class: type):
        """Register a collector class"""
        self.collectors.append(collector_class)
        return collector_class

    def get_collectors(self, session: boto3.Session, target_type: str = "") -> List[ResourceCollector]:
        """
        Get collector instances with the given session, optionally filtered by target type.
        target_type can be either a full resource type (e.g. aws_s3_bucket) or a service category (e.g. s3)
        """
        logger.debug(f"Getting collectors, total registered: {len(self.collectors)}")
        instances = []
        for collector_cls in self.collectors:
            try:
                collector = collector_cls(session)
                # category or resource type filter
                if target_type:
                    resource_types = collector.get_resource_types()
                    service_name = collector.get_service_name()
                    if target_type != service_name and target_type not in resource_types:
                        continue
                instances.append(collector)
                logger.debug(f"Initialized collector: {collector_cls.__name__}")
            except Exception as e:
                logger.error(
                    f"Failed to initialize collector {collector_cls.__name__}: {e}"
                )
        return instances

    def iter_classes(self):
        """Iterator over collector classes"""
        return iter(self.collectors)

    def __iter__(self):
        """Make the registry iterable over collector classes"""
        return self.iter_classes()

    def __len__(self):
        """Get number of registered collectors"""
        return len(self.collectors)


# Global registry instance
registry = CollectorRegistry()


def register_collector(collector_class: type):
    """Decorator to register a collector class"""
    logger.debug(f"Registering collector class: {collector_class.__name__}")
    return registry.register(collector_class)
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage/__init__.py</source>
<document_content>from .s3 import S3Collector
from .efs import EFSCollector
from .ebs import EBSCollector

__all__ = ["S3Collector", "EFSCollector", "EBSCollector"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage/efs.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class EFSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "efs"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_efs_file_system": "EFS File Systems"}

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("describe_file_systems")
            for page in paginator.paginate():
                for fs in page["FileSystems"]:
                    resources.append(
                        {
                            "type": "aws_efs_file_system",
                            "id": fs["FileSystemId"],
                            "arn": fs["FileSystemArn"],
                            "tags": fs.get("Tags", []),
                        }
                    )
        except Exception as e:
            print(f"Error collecting EFS filesystems: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage/s3.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class S3Collector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "s3"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_s3_bucket": "S3 Buckets",
            "aws_s3_bucket_versioning": "S3 Bucket Versioning",
            "aws_s3_bucket_server_side_encryption_configuration": "S3 Bucket Encryption",
            "aws_s3_bucket_public_access_block": "S3 Bucket Public Access Block",
            "aws_s3_bucket_acl": "S3 Bucket ACL",
            "aws_s3_bucket_policy": "S3 Bucket Policy",
            "aws_s3_bucket_cors_configuration": "S3 Bucket CORS",
            "aws_s3_bucket_website_configuration": "S3 Bucket Website",
            "aws_s3_bucket_logging": "S3 Bucket Logging",
            "aws_s3_bucket_lifecycle_configuration": "S3 Bucket Lifecycle"
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            for bucket in self.client.list_buckets()["Buckets"]:
                bucket_name = bucket["Name"]
                try:
                    tags = self.client.get_bucket_tagging(Bucket=bucket_name).get(
                        "TagSet", []
                    )
                except:  # noqa: E722
                    tags = []

                # Get encryption configuration
                try:
                    encryption = self.client.get_bucket_encryption(Bucket=bucket_name)
                    encryption_rules = encryption.get('ServerSideEncryptionConfiguration', {}).get('Rules', [])
                    if encryption_rules:
                        encryption_config = encryption_rules[0].get('ApplyServerSideEncryptionByDefault', {})
                        encryption_details = {
                            "sse_algorithm": encryption_config.get('SSEAlgorithm'),
                            "kms_master_key_id": encryption_config.get('KMSMasterKeyID')
                        }
                except:  # noqa: E722
                    encryption_details = {}

                # Get ACL configuration
                # Get ACL configuration
                acl_details = None
                try:
                    acl = self.client.get_bucket_acl(Bucket=bucket_name)
                    if acl and isinstance(acl, dict):
                        owner = acl.get('Owner', {})
                        grants = acl.get('Grants', [])
                        if owner or grants:
                            acl_details = {
                                "owner": owner,
                                "grants": grants
                            }
                            logger.info(f"Successfully retrieved ACL for bucket: {bucket_name}")
                except self.client.exceptions.ClientError as e:
                    error_code = e.response['Error']['Code']
                    error_message = e.response['Error']['Message']
                    logger.warning(f"Error getting ACL for bucket {bucket_name}: Code={error_code}, Message={error_message}")
                except Exception as e:
                    logger.warning(f"Unexpected error getting ACL for bucket {bucket_name}: {str(e)}")

                # Get bucket policy
                try:
                    policy = self.client.get_bucket_policy(Bucket=bucket_name)
                    policy_text = policy.get('Policy')
                    if policy_text:
                        try:
                            # ポリシーが有効なJSONであることを確認
                            import json
                            json.loads(policy_text)
                            logger.info(f"Found valid bucket policy for {bucket_name}")
                            logger.debug(f"Policy content: {policy_text}")
                            
                            # ポリシーが存在し、有効なJSONの場合のみ追加
                            resources.append({
                                "type": "aws_s3_bucket_policy",
                                "id": bucket_name,
                                "arn": f"arn:aws:s3:::{bucket_name}",  # ARNを追加
                                "details": {
                                    "policy": policy_text
                                }
                            })
                            logger.info(f"Successfully added bucket policy for: {bucket_name}")
                        except json.JSONDecodeError as je:
                            logger.error(f"Invalid JSON in bucket policy for {bucket_name}: {je}")
                    else:
                        logger.warning(f"Empty policy returned for bucket: {bucket_name}")
                except self.client.exceptions.ClientError as e:
                    error_code = e.response['Error']['Code']
                    error_message = e.response['Error']['Message']
                    if error_code in ['NoSuchPolicy', 'NoSuchBucketPolicy']:
                        logger.debug(f"No bucket policy found for bucket: {bucket_name} (Code={error_code}, Message={error_message})")
                    else:
                        logger.warning(f"Unexpected error getting bucket policy for {bucket_name}: Code={error_code}, Message={error_message}")
                except Exception as e:
                    logger.warning(f"Unexpected error getting bucket policy for {bucket_name}: {str(e)}")

                # Get CORS configuration
                try:
                    cors = self.client.get_bucket_cors(Bucket=bucket_name)
                    cors_rules = cors.get('CORSRules', [])
                except:  # noqa: E722
                    cors_rules = []

                # Get website configuration
                try:
                    website = self.client.get_bucket_website(Bucket=bucket_name)
                    website_config = {
                        "index_document": website.get('IndexDocument', {}).get('Suffix'),
                        "error_document": website.get('ErrorDocument', {}).get('Key'),
                        "routing_rules": website.get('RoutingRules', [])
                    }
                except:  # noqa: E722
                    website_config = {}

                # Get logging configuration
                try:
                    logging = self.client.get_bucket_logging(Bucket=bucket_name)
                    logging_config = logging.get('LoggingEnabled', {})
                    if logging_config:
                        logging_details = {
                            "target_bucket": logging_config.get('TargetBucket'),
                            "target_prefix": logging_config.get('TargetPrefix')
                        }
                    else:
                        logging_details = {}
                except:  # noqa: E722
                    logging_details = {}

                # Get versioning configuration
                try:
                    versioning = self.client.get_bucket_versioning(Bucket=bucket_name)
                    versioning_status = versioning.get('Status')
                except:  # noqa: E722
                    versioning_status = None

                # Get public access block configuration
                try:
                    public_access = self.client.get_public_access_block(Bucket=bucket_name)
                    public_access_block = public_access.get('PublicAccessBlockConfiguration', {})
                except:  # noqa: E722
                    public_access_block = {}

                # メインのバケットリソース
                resources.append({
                    "type": "aws_s3_bucket",
                    "id": bucket_name,
                    "arn": f"arn:aws:s3:::{bucket_name}",
                    "tags": tags
                })

                # バージョニング設定
                if versioning_status:
                    resources.append({
                        "type": "aws_s3_bucket_versioning",
                        "id": bucket_name,
                        "details": {
                            "versioning_status": versioning_status
                        }
                    })

                # 暗号化設定
                if encryption_details:
                    resources.append({
                        "type": "aws_s3_bucket_server_side_encryption_configuration",
                        "id": bucket_name,
                        "details": encryption_details
                    })

                # パブリックアクセスブロック設定
                if public_access_block:
                    resources.append({
                        "type": "aws_s3_bucket_public_access_block",
                        "id": bucket_name,
                        "details": public_access_block
                    })

                # ACL設定
                if acl_details and isinstance(acl_details, dict):
                    owner = acl_details.get("owner", {})
                    grants = acl_details.get("grants", [])
                    if owner or grants:
                        logger.info(f"Adding ACL resource for bucket: {bucket_name}")
                        resources.append({
                            "type": "aws_s3_bucket_acl",
                            "id": bucket_name,
                            "details": acl_details
                        })
                        logger.debug(f"ACL details for {bucket_name}: {acl_details}")

                # バケットポリシーの追加処理は上部で実施済み

                # CORS設定
                if cors_rules:
                    resources.append({
                        "type": "aws_s3_bucket_cors_configuration",
                        "id": bucket_name,
                        "details": {
                            "cors_rules": cors_rules
                        }
                    })

                # Webサイト設定
                if website_config.get("index_document") or website_config.get("error_document"):
                    resources.append({
                        "type": "aws_s3_bucket_website_configuration",
                        "id": bucket_name,
                        "details": website_config
                    })

                # ロギング設定
                if logging_details:
                    resources.append({
                        "type": "aws_s3_bucket_logging",
                        "id": bucket_name,
                        "details": logging_details
                    })
        except Exception as e:
            print(f"Error collecting S3 buckets: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_storage/ebs.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class EBSCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "ec2"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {"aws_ebs_volume": "EBS Volumes"}

    def _should_manage_volume(self, volume: Dict[str, Any]) -> bool:
        """
        Determine if an EBS volume should be explicitly managed.

        Returns True if:
        - Volume is not attached to any instance (unattached volumes should be managed)
        - Volume has DeleteOnTermination=False for any attachment (preserved volumes should be managed)

        Returns False if:
        - Volume is attached with DeleteOnTermination=True (these are managed with EC2)
        """
        attachments = volume.get("Attachments", [])

        # If volume is not attached, it should be managed
        if not attachments:
            return True

        # Check DeleteOnTermination flag for all attachments
        # If any attachment has DeleteOnTermination=False, the volume should be managed
        for attachment in attachments:
            if not attachment.get("DeleteOnTermination", True):
                return True

        # Volume is attached and all attachments have DeleteOnTermination=True
        return False

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []

        try:
            paginator = self.client.get_paginator("describe_volumes")
            for page in paginator.paginate():
                for volume in page["Volumes"]:
                    # Only include volumes that should be explicitly managed
                    if not self._should_manage_volume(volume):
                        continue

                    resources.append(
                        {
                            "type": "aws_ebs_volume",
                            "id": volume["VolumeId"],
                            "arn": self.build_arn("volume", volume["VolumeId"]),
                            "tags": volume.get("Tags", []),
                            "details": {
                                "size": volume.get("Size"),
                                "encrypted": volume.get("Encrypted"),
                                "volume_type": volume.get("VolumeType"),
                                "create_time": str(volume.get("CreateTime")),
                                "attachments": [
                                    {
                                        "instance_id": att.get("InstanceId"),
                                        "device": att.get("Device"),
                                        "delete_on_termination": att.get(
                                            "DeleteOnTermination", True
                                        ),
                                    }
                                    for att in volume.get("Attachments", [])
                                ],
                            },
                        }
                    )

        except Exception as e:
            logger.error(f"Error collecting EBS volumes: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/user.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector

import logging

logger = logging.getLogger(__name__)


@register_collector
class IAMUserCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "iam"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_iam_user": "IAM Users",
            "aws_iam_user_policy": "IAM User Policies",
            "aws_iam_user_policy_attachment": "IAM User Policy Attachments",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            if target_resource_type:
                if target_resource_type == "aws_iam_user":
                    resources.extend(self._collect_users())
                elif target_resource_type == "aws_iam_user_policy":
                    resources.extend(self._collect_user_policies())
                elif target_resource_type == "aws_iam_user_policy_attachment":
                    resources.extend(self._collect_user_policy_attachments())
            else:
                resources.extend(self._collect_users())
                resources.extend(self._collect_user_policies())
                resources.extend(self._collect_user_policy_attachments())
        except Exception as e:
            print(f"Error collecting IAM resources: {str(e)}")

        return resources

    def _collect_users(self) -> List[Dict[str, Any]]:
        """Collect IAM users"""
        resources = []
        paginator = self.client.get_paginator("list_users")
        for page in paginator.paginate():
            for user in page["Users"]:
                try:
                    tags = self.client.list_user_tags(UserName=user["UserName"])["Tags"]
                    resources.append(
                        {
                            "type": "aws_iam_user",
                            "id": user["UserName"],
                            "arn": user["Arn"],
                            "tags": tags,
                        }
                    )
                except Exception as e:
                    print(
                        f"Error collecting tags for user {user['UserName']}: {str(e)}"
                    )
        return resources

    def _collect_user_policies(self) -> List[Dict[str, Any]]:
        """Collect inline user policies"""
        resources = []
        user_paginator = self.client.get_paginator("list_users")
        for user_page in user_paginator.paginate():
            for user in user_page["Users"]:
                try:
                    policy_paginator = self.client.get_paginator("list_user_policies")
                    for policy_page in policy_paginator.paginate(
                        UserName=user["UserName"]
                    ):
                        for policy_name in policy_page["PolicyNames"]:
                            # Get the policy document
                            try:
                                policy = self.client.get_user_policy(
                                    UserName=user["UserName"], PolicyName=policy_name
                                )
                                resources.append(
                                    {
                                        "type": "aws_iam_user_policy",
                                        "id": f"{user['UserName']}:{policy_name}",
                                        "user_name": user["UserName"],
                                        "policy_name": policy_name,
                                        "policy_document": policy["PolicyDocument"],
                                    }
                                )
                            except Exception as e:
                                logger.error(
                                    f"Error getting policy document for user {user['UserName']}, "
                                    f"policy {policy_name}: {str(e)}"
                                )
                except Exception as e:
                    logger.error(
                        f"Error collecting inline policies for user {user['UserName']}: {str(e)}"
                    )
        return resources

    def _collect_user_policy_attachments(self) -> List[Dict[str, Any]]:
        """Collect user policy attachments"""
        resources = []
        try:
            # list_users with pagination
            user_paginator = self.client.get_paginator("list_users")
            user_page_num = 0
            for user_page in user_paginator.paginate():
                user_page_num += 1
                logger.debug(
                    f"Processing user page {user_page_num} with {len(user_page['Users'])} users"
                )

                for user in user_page["Users"]:
                    user_name = user["UserName"]
                    try:
                        # list_attached_user_policies with pagination
                        attachment_paginator = self.client.get_paginator(
                            "list_attached_user_policies"
                        )
                        policy_page_num = 0
                        total_policies = 0

                        for attachment_page in attachment_paginator.paginate(
                            UserName=user_name,
                            PaginationConfig={"PageSize": 100, "MaxItems": None},
                        ):
                            policy_page_num += 1
                            policies = attachment_page["AttachedPolicies"]
                            total_policies += len(policies)

                            for policy in policies:
                                try:
                                    resources.append(
                                        {
                                            "type": "aws_iam_user_policy_attachment",
                                            "id": f"{user_name}:{policy['PolicyArn']}",
                                            "user_name": user_name,
                                            "policy_arn": policy["PolicyArn"],
                                        }
                                    )
                                except KeyError as ke:
                                    logger.error(
                                        f"Missing required key in policy data for user {user_name}: {ke}"
                                    )
                                    logger.debug(f"Policy data: {policy}")
                                    continue

                        logger.debug(
                            f"User {user_name}: Processed {policy_page_num} pages, found {total_policies} policies"
                        )

                    except Exception as e:
                        logger.error(
                            f"Error collecting policies for user {user_name}: {str(e)}"
                        )
                        logger.debug(f"Full error: {e}", exc_info=True)

        except Exception as e:
            logger.error(f"Error in user policy attachment collection: {str(e)}")
            logger.debug("Full error trace:", exc_info=True)

        logger.info(f"Collected {len(resources)} total user policy attachments")
        return resources

    # return resources
    # def _collect_user_policy_attachments(self) -> List[Dict[str, Any]]:
    #     """Collect user policy attachments"""
    #     resources = []
    #     user_paginator = self.client.get_paginator("list_users")
    #     for user_page in user_paginator.paginate():
    #         for user in user_page["Users"]:
    #             try:
    #                 attachment_paginator = self.client.get_paginator(
    #                     "list_attached_user_policies"
    #                 )
    #                 for attachment_page in attachment_paginator.paginate(
    #                     UserName=user["UserName"]
    #                 ):
    #                     for policy in attachment_page["AttachedPolicies"]:
    #                         resources.append(
    #                             {
    #                                 "type": "aws_iam_user_policy_attachment",
    #                                 "id": f"{user['UserName']}:{policy['PolicyName']}",
    #                                 "user_name": user["UserName"],
    #                                 "policy_arn": policy["PolicyArn"],
    #                             }
    #                         )
    #             except Exception as e:
    #                 print(
    #                     f"Error collecting policy attachments for user {user['UserName']}: {str(e)}"
    #                 )
    #     return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/instance_profile.py</source>
<document_content># terraform_aws_migrator/collectors/aws_iam/instance_profile.py

from typing import Dict, List, Any, Optional, Set
import concurrent.futures
import logging
from ..base import ResourceCollector, register_collector

logger = logging.getLogger(__name__)


@register_collector
class IAMInstanceProfileCollector(ResourceCollector):
    """Collector for IAM Instance Profiles with caching and parallel processing"""

    def __init__(self, session, progress_callback=None):
        super().__init__(session, progress_callback)
        self._role_details_cache = {}
        self._policy_cache = {}
        self._max_workers = 10

    @classmethod
    def get_service_name(cls) -> str:
        return "iam"

    @classmethod
    def get_resource_types(cls) -> Dict[str, str]:
        return {"aws_iam_instance_profile": "IAM Instance Profiles"}

    def _is_aws_managed_path(self, path: str) -> bool:
        """Check if the path indicates an AWS managed role/profile"""
        aws_managed_paths = {"/aws-service-role/", "/service-role/", "/aws-reserved/"}
        return any(path.startswith(prefix) for prefix in aws_managed_paths)

    def _is_aws_service_principal(self, assume_role_policy: Dict) -> bool:
        """Check if the assume role policy trusts AWS services"""
        try:
            statements = assume_role_policy.get("Statement", [])
            for statement in statements:
                principal = statement.get("Principal", {})
                service = principal.get("Service")
                if service:
                    if isinstance(service, str):
                        return service.endswith(".amazonaws.com")
                    elif isinstance(service, list):
                        return any(s.endswith(".amazonaws.com") for s in service)
        except Exception as e:
            logger.error(f"Error parsing assume role policy: {e}")
        return False

    def _get_attached_policies(self, role_name: str) -> List[Dict[str, Any]]:
        """Get attached policies for a role with caching"""
        cache_key = f"policies_{role_name}"
        if cache_key in self._policy_cache:
            return self._policy_cache[cache_key]

        try:
            attached_policies = []
            paginator = self.client.get_paginator("list_attached_role_policies")
            for page in paginator.paginate(RoleName=role_name):
                attached_policies.extend(page["AttachedPolicies"])

            self._policy_cache[cache_key] = attached_policies
            return attached_policies
        except Exception as e:
            logger.error(f"Error getting attached policies for role {role_name}: {e}")
            return []

    def _get_role_details(self, role_name: str) -> Dict[str, Any]:
        """Get detailed information about a role with caching"""
        if role_name in self._role_details_cache:
            return self._role_details_cache[role_name]

        try:
            role = self.client.get_role(RoleName=role_name)["Role"]
            attached_policies = self._get_attached_policies(role_name)

            details = {
                "Path": role.get("Path", "/"),
                "AssumeRolePolicyDocument": role.get("AssumeRolePolicyDocument", {}),
                "AttachedPolicies": attached_policies,
            }

            self._role_details_cache[role_name] = details
            return details
        except Exception as e:
            logger.error(f"Error getting role details for {role_name}: {e}")
            return {}

    def _is_aws_quick_setup_role(self, role_name: str) -> bool:
        """Check if this is specifically an SSM Quick Setup role"""
        quick_setup_patterns = ["AmazonSSMRoleForInstancesQuickSetup"]
        return any(role_name.startswith(pattern) for pattern in quick_setup_patterns)

    def _is_aws_service_managed_role(
        self, role_details: Dict[str, Any], role_name: str
    ) -> bool:
        """Determine if a role is managed by an AWS service based on multiple criteria"""
        if not role_details:
            return False

        # Check path - this is a strong indicator
        if self._is_aws_managed_path(role_details.get("Path", "/")):
            logger.debug(f"Role {role_name} has AWS managed path")
            return True

        # Check for SSM Quick Setup role
        if role_name == "AmazonSSMRoleForInstancesQuickSetup":
            logger.debug(f"Role {role_name} is an SSM Quick Setup role")
            return True

        # Check assume role policy for AWS service principals
        assume_role_policy = role_details.get("AssumeRolePolicyDocument", {})
        statements = assume_role_policy.get("Statement", [])

        # Count unique service principals
        service_principals = set()
        for statement in statements:
            principal = statement.get("Principal", {})
            service = principal.get("Service")
            if service:
                if isinstance(service, str):
                    service_principals.add(service)
                elif isinstance(service, list):
                    service_principals.update(service)

        # If only EC2 is trusted, this is likely a user-managed role
        if service_principals == {"ec2.amazonaws.com"}:
            return False

        # Get attached policies
        attached_policies = role_details.get("AttachedPolicies", [])
        if not attached_policies:
            return False

        # Exclude roles that only have SSM core policies
        if (
            len(attached_policies) == 1
            and attached_policies[0]["PolicyName"] == "AmazonSSMManagedInstanceCore"
        ):
            return False

        # Check for AWS service management indicators
        aws_service_keywords = {
            "AWSQuickSetup",
            "AWSSystemsManager",
            "aws-service-role",
            "service-role",
        }

        has_service_keyword = any(
            any(keyword in policy["PolicyName"] for keyword in aws_service_keywords)
            for policy in attached_policies
        )

        return has_service_keyword

    def _process_profile(self, profile: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Process a single instance profile"""
        try:
            if self._should_exclude_profile(profile):
                return None

            profile_name = profile["InstanceProfileName"]

            # Get profile tags
            try:
                tags = self.client.list_instance_profile_tags(
                    InstanceProfileName=profile_name
                ).get("Tags", [])
            except Exception as e:
                logger.warning(f"Failed to get tags for profile {profile_name}: {e}")
                tags = []

            return {
                "type": "aws_iam_instance_profile",
                "id": profile_name,
                "arn": profile["Arn"],
                "tags": tags,
                "details": {
                    "path": profile.get("Path", "/"),
                    "create_date": str(profile.get("CreateDate", "")),
                    "role_name": (
                        profile["Roles"][0]["RoleName"]
                        if profile.get("Roles")
                        else None
                    ),
                },
            }
        except Exception as e:
            logger.error(
                f"Error processing profile {profile.get('InstanceProfileName', 'unknown')}: {e}"
            )
            return None

    def _should_exclude_profile(self, profile: Dict[str, Any]) -> bool:
        """Determine if an instance profile should be excluded based on AWS service management"""
        profile_name = profile["InstanceProfileName"]

        # Special case for SSM Quick Setup profile
        if profile_name == "AmazonSSMRoleForInstancesQuickSetup":
            logger.debug(
                f"Excluding profile {profile_name} as it is an SSM Quick Setup profile"
            )
            return True

        # Check profile path
        if self._is_aws_managed_path(profile.get("Path", "/")):
            logger.debug(f"Excluding profile {profile_name} due to AWS managed path")
            return True

        # Check attached role
        roles = profile.get("Roles", [])
        if roles:
            role_name = roles[0]["RoleName"]
            role_details = self._get_role_details(role_name)

            if self._is_aws_service_managed_role(role_details, role_name):
                logger.debug(
                    f"Excluding profile {profile_name} due to AWS service managed role"
                )
                return True

        # By default, include the profile
        logger.debug(f"Including profile {profile_name} as customer-managed")
        return False

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        """Collect IAM Instance Profile resources with parallel processing"""
        resources = []
        profiles_to_process = []

        try:
            # First, collect all profiles
            paginator = self.client.get_paginator("list_instance_profiles")
            for page in paginator.paginate():
                profiles_to_process.extend(page["InstanceProfiles"])

            # Process profiles in parallel
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=self._max_workers
            ) as executor:
                future_to_profile = {
                    executor.submit(self._process_profile, profile): profile
                    for profile in profiles_to_process
                }

                for future in concurrent.futures.as_completed(future_to_profile):
                    profile = future_to_profile[future]
                    try:
                        if result := future.result():
                            resources.append(result)
                    except Exception as e:
                        logger.error(
                            f"Error processing profile {profile.get('InstanceProfileName', 'unknown')}: {e}"
                        )

        except Exception as e:
            logger.error(f"Error collecting IAM instance profile resources: {e}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/policy.py</source>
<document_content># terraform_aws_migrator/collectors/aws_iam/policy.py

from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector
import logging

logger = logging.getLogger(__name__)


@register_collector
class IAMPolicyCollector(ResourceCollector):
    """Collector for IAM Policies"""

    @classmethod
    def get_service_name(cls) -> str:
        """Return the AWS service name for this collector"""
        return "iam"

    @classmethod
    def get_resource_types(cls) -> Dict[str, str]:
        """Return supported resource types and their descriptions"""
        return {
            "aws_iam_policy": "IAM Policies",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        """
        Collect IAM policy resources.
        
        Args:
            target_resource_type: Optional specific resource type to collect
            
        Returns:
            List of collected IAM policy resources
        """
        if target_resource_type and target_resource_type != "aws_iam_policy":
            return []

        try:
            return self._collect_customer_managed_policies()
        except Exception as e:
            logger.error(f"Error collecting IAM policy resources: {e}")
            return []

    def _collect_customer_managed_policies(self) -> List[Dict[str, Any]]:
        """Collect customer managed IAM policies"""
        resources = []
        paginator = self.client.get_paginator("list_policies")

        try:
            for page in paginator.paginate(Scope="Local"):
                for policy in page["Policies"]:
                    policy_resource = self._process_single_policy(policy)
                    if policy_resource:
                        resources.append(policy_resource)
        except Exception as e:
            logger.error(f"Error during policy collection: {e}")

        return resources

    def _process_single_policy(self, policy: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process a single IAM policy and format its data
        
        Args:
            policy: Raw policy data from AWS
            
        Returns:
            Formatted policy resource dictionary
        """
        try:
            policy_arn = policy["Arn"]
            policy_version = self._get_policy_version(policy_arn, policy["DefaultVersionId"])
            tags = self._get_policy_tags(policy_arn)

            return {
                "type": "aws_iam_policy",
                "id": policy["PolicyName"],
                "arn": policy_arn,
                "tags": tags,
                "details": {
                    "description": policy.get("Description", ""),
                    "path": policy["Path"],
                    "policy_document": policy_version["Document"] if policy_version else {},
                    "is_attachable": policy["IsAttachable"],
                    "attachment_count": policy.get("AttachmentCount", 0),
                    "create_date": str(policy["CreateDate"]),
                    "update_date": str(policy["UpdateDate"]),
                },
            }
        except Exception as e:
            logger.error(f"Error processing policy {policy.get('PolicyName', 'Unknown')}: {e}")
            return None

    def _get_policy_version(self, policy_arn: str, version_id: str) -> Dict[str, Any]:
        """Get the specified version of an IAM policy"""
        try:
            return self.client.get_policy_version(
                PolicyArn=policy_arn,
                VersionId=version_id
            )["PolicyVersion"]
        except Exception as e:
            logger.error(f"Error getting policy version for {policy_arn}: {e}")
            return {}

    def _get_policy_tags(self, policy_arn: str) -> List[Dict[str, str]]:
        """Get tags for an IAM policy"""
        try:
            return self.client.list_policy_tags(PolicyArn=policy_arn)["Tags"]
        except Exception:
            return []
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/__init__.py</source>
<document_content>from .role import IAMRoleCollector
from .user import IAMUserCollector
from .group import IAMGroupCollector
from .policy import IAMPolicyCollector

__all__ = [
    'IAMRoleCollector',
    'IAMUserCollector',
    'IAMGroupCollector',
    'IAMPolicyCollector'
]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/role.py</source>
<document_content>from typing import Dict, List, Any
from ..base import ResourceCollector, register_collector

import logging
import traceback
logger = logging.getLogger(__name__)


@register_collector
class IAMRoleCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "iam"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_iam_role": "IAM Roles",
            "aws_iam_role_policy": "IAM Role Policies",
            "aws_iam_role_policy_attachment": "IAM Role Policy Attachments",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources = []
        try:
            if target_resource_type:
                if target_resource_type == "aws_iam_role":
                    resources.extend(self._collect_roles())
                elif target_resource_type == "aws_iam_role_policy":
                    resources.extend(self._collect_role_policies())
                elif target_resource_type == "aws_iam_role_policy_attachment":
                    resources.extend(self._collect_role_policy_attachments())
            else:
                resources.extend(self._collect_roles())
                resources.extend(self._collect_role_policies())
                resources.extend(self._collect_role_policy_attachments())
        except Exception as e:
            print(f"Error collecting IAM resources: {traceback.format_exc()}")

        return resources

    def _collect_roles(self) -> List[Dict[str, Any]]:
        resources = []
        paginator = self.client.get_paginator("list_roles")
        for page in paginator.paginate():
            for role in page["Roles"]:
                if not any(
                    rule(role["RoleName"]) for rule in self.get_excluded_rules()
                ):
                    try:
                        tags = self.client.list_role_tags(RoleName=role["RoleName"])[
                            "Tags"
                        ]
                        resource_id = role["RoleName"]
                        resources.append(
                            {
                                "type": "aws_iam_role",
                                "id": resource_id,
                                "arn": role["Arn"],
                                "tags": tags,
                                "details": {
                                    "path": role.get("Path"),
                                    "assume_role_policy": role.get(
                                        "AssumeRolePolicyDocument", {}
                                    ),
                                },
                            }
                        )
                    except Exception as e:
                        logger.error(
                            f"Error collecting details for role {role['RoleName']}: {str(e)}"
                        )
        return resources

    def _collect_role_policies(self) -> List[Dict[str, Any]]:
        resources = []
        role_paginator = self.client.get_paginator("list_roles")
        for role_page in role_paginator.paginate():
            for role in role_page["Roles"]:
                if not any(
                    rule(role["RoleName"]) for rule in self.get_excluded_rules()
                ):
                    try:
                        policy_paginator = self.client.get_paginator(
                            "list_role_policies"
                        )
                        for policy_page in policy_paginator.paginate(
                            RoleName=role["RoleName"]
                        ):
                            for policy_name in policy_page["PolicyNames"]:
                                resource_id = f"{role['RoleName']}_{policy_name}"
                                resources.append(
                                    {
                                        "type": "aws_iam_role_policy",
                                        "id": resource_id,
                                        "role_name": role["RoleName"],
                                        "policy_name": policy_name,
                                    }
                                )
                    except Exception as e:
                        logger.error(
                            f"Error collecting inline policies for role {role['RoleName']}: {str(e)}"
                        )
        return resources

    def _collect_role_policy_attachments(self) -> List[Dict[str, Any]]:
        """Collect role policy attachments"""
        resources = []
        role_paginator = self.client.get_paginator("list_roles")
        role_names: List = []
        for role_page in role_paginator.paginate():
            for role in role_page["Roles"]:
                role_names.append(role["RoleName"])

        logger.debug(f"Collecting policy attachments for {len(role_names)} roles")
        # Use STS client to get account ID
        sts_client = self.session.client('sts')
        account_id = sts_client.get_caller_identity()["Account"]
        
        for role_name in role_names:
                if not any(
                    rule(role_name) for rule in self.get_excluded_rules()
                ):
                    try:
                        logger.debug(f"Getting attached policies for role: {role_name}")
                        paginator = self.client.get_paginator("list_attached_role_policies")
                        for page in paginator.paginate(RoleName=role_name):
                            for policy in page["AttachedPolicies"]:
                                attachment = {
                                    "type": "aws_iam_role_policy_attachment",
                                    "id": f"arn:aws:iam::{account_id}:role/{role_name}/{policy['PolicyArn']}",
                                    "role_name": role_name,
                                    "policy_arn": policy["PolicyArn"],
                                }
                                resources.append(attachment)
                                logger.debug(f"Found policy attachment: {attachment['id']}")
                    except Exception as e:
                        logger.error(
                            f"Error collecting policy attachments for role {role_name}: {str(e)}"
                        )

        logger.debug(f"Collected total of {len(resources)} policy attachments")
        return resources

    def get_excluded_rules(self) -> List[callable]:
        """Rules for excluding AWS-managed roles"""
        return [
            lambda x: x.startswith("AWSServiceRole"),
            lambda x: x.startswith("aws-service-role"),
            lambda x: x.startswith("OrganizationAccountAccessRole"),
        ]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/collectors/aws_iam/group.py</source>
<document_content># terraform_aws_migrator/collectors/aws_iam/group.py

from typing import Dict, List, Any
from terraform_aws_migrator.collectors.base import ResourceCollector, register_collector
import logging
from ..base import ResourceCollector, register_collector

logger = logging.getLogger(__name__)


@register_collector
class IAMGroupCollector(ResourceCollector):
    @classmethod
    def get_service_name(self) -> str:
        return "iam"

    @classmethod
    def get_resource_types(self) -> Dict[str, str]:
        return {
            "aws_iam_group": "IAM Groups",
            "aws_iam_group_policy": "IAM Group Policies",
            "aws_iam_group_policy_attachment": "IAM Group Policy Attachments",
            "aws_iam_group_membership": "IAM Group Memberships",
        }

    def collect(self, target_resource_type: str = "") -> List[Dict[str, Any]]:
        resources: List = []
        try:
            if target_resource_type != "aws_iam_group":
                return resources
            # Collect IAM groups
            paginator = self.client.get_paginator("list_groups")
            for page in paginator.paginate():
                for group in page["Groups"]:
                    try:
                        group_name = group["GroupName"]

                        # Get group members
                        members = self.client.get_group(GroupName=group_name)["Users"]

                        # Get attached policies
                        attached_policies = self.client.list_attached_group_policies(
                            GroupName=group_name
                        )["AttachedPolicies"]

                        # Get inline policies
                        inline_policies = self.client.list_group_policies(
                            GroupName=group_name
                        )["PolicyNames"]

                        inline_policy_documents = {}
                        for policy_name in inline_policies:
                            policy = self.client.get_group_policy(
                                GroupName=group_name, PolicyName=policy_name
                            )
                            inline_policy_documents[policy_name] = policy[
                                "PolicyDocument"
                            ]

                        resources.append(
                            {
                                "type": "aws_iam_group",
                                "id": group_name,
                                "arn": group["Arn"],
                                "details": {
                                    "path": group["Path"],
                                    "members": [user["UserName"] for user in members],
                                    "attached_policies": attached_policies,
                                    "inline_policies": inline_policy_documents,
                                },
                            }
                        )
                    except Exception as e:
                        logger.error(
                            f"Error collecting details for group {group['GroupName']}: {str(e)}"
                        )
                        continue

        except Exception as e:
            logger.error(f"Error collecting IAM group resources: {str(e)}")

        return resources
</document_content>
</document>

<document>
<source>terraform_aws_migrator/formatters/output_formatter.py</source>
<document_content># terraform_aws_migrator/formatters/output_formatter.py

import json
from typing import Dict, List, Any
import logging
from ..collectors.base import registry

logger = logging.getLogger(__name__)


def format_output(resources: Dict[str, List[Dict[str, Any]]], output_format: str = "text") -> str:
    """
    Format the output of unmanaged AWS resources

    Args:
        resources: Dictionary containing unmanaged resources by service
        output_format: Desired output format ("text" or "json")

    Returns:
        Formatted string containing the unmanaged resources
    """
    try:
        if output_format == "json":
            return json.dumps(resources, indent=2, default=str)

        if not resources:
            return "No unmanaged resources found."

        output = []
        output.append("\nUnmanaged AWS Resources:")
        output.append("=" * 40)

        # Count total unmanaged resources
        total_unmanaged = sum(len(res_list) for res_list in resources.values())

        # Resource Summary
        output.append("\nResource Summary:")
        output.append(f"Total Unmanaged Resources: {total_unmanaged}")

        # Create collectors map for resource type lookups
        collectors = {
            collector_cls.get_service_name(): collector_cls
            for collector_cls in registry
        }

        # Group resources by type
        resource_counts = {}
        for service_name, service_resources in resources.items():
            collector_cls = collectors.get(service_name)
            if not collector_cls:
                continue

            for resource in service_resources:
                resource_type = resource.get("type", "unknown")
                full_type = f"{service_name}.{resource_type}"
                if full_type not in resource_counts:
                    resource_counts[full_type] = []
                resource_counts[full_type].append(resource)

        # Resources by Type
        if resource_counts:
            output.append("\nResources by Type:")
            for full_type, resources_list in sorted(resource_counts.items()):
                service_name, resource_type = full_type.split(".", 1)
                collector_cls = collectors.get(service_name)

                if collector_cls:
                    display_name = collector_cls.get_type_display_name(resource_type)
                else:
                    display_name = full_type

                output.append(f"- Found {len(resources_list)} unmanaged {display_name}")

        # Detailed Resources Section
        if output_format == "text":
            output.append("\nDetailed Resources:")
            for full_type, resources_list in sorted(resource_counts.items()):
                service_name, resource_type = full_type.split(".", 1)
                collector_cls = collectors.get(service_name)

                if collector_cls:
                    display_name = collector_cls.get_type_display_name(resource_type)
                else:
                    display_name = full_type

                output.append(f"\n{display_name}:")

                for resource in resources_list:
                    resource_id = resource.get("id", "N/A")
                    resource_arn = resource.get("arn", "N/A")

                    output.append(f"  ID: {resource_id}")
                    output.append(f"  ARN: {resource_arn}")

                    # Add details if present
                    details = resource.get("details", {})
                    if details:
                        output.append("  Details:")
                        for detail_key, value in sorted(details.items()):
                            output.append(f"    {detail_key}: {value}")

                    # Add tags if present
                    tags = resource.get("tags", [])
                    if tags:
                        output.append("  Tags:")
                        if isinstance(tags, list):
                            for tag in tags:
                                if isinstance(tag, dict):
                                    key = tag.get("Key", "N/A")
                                    value = tag.get("Value", "N/A")
                                    output.append(f"    {key}: {value}")
                        elif isinstance(tags, dict):
                            for tag_key, value in sorted(tags.items()):
                                output.append(f"    {tag_key}: {value}")
                    output.append("")  # Empty line for readability

        return "\n".join(output)

    except Exception as e:
        logger.exception("Error formatting output")
        return f"Error formatting output: {str(e)}"
</document_content>
</document>

<document>
<source>terraform_aws_migrator/formatters/__init__.py</source>
<document_content></document_content>
</document>

<document>
<source>terraform_aws_migrator/utils/resource_utils.py</source>
<document_content># terraform_aws_migrator/utils/resource_utils.py

import importlib
import inspect
from pathlib import Path
from typing import Dict, List, Any
from rich.console import Console

from terraform_aws_migrator.collectors.base import ResourceCollector


def get_collectors_info() -> Dict[str, List[Dict[str, Any]]]:
    """Get information about all available collectors grouped by category"""
    collectors_dir = Path(__file__).parent.parent / "collectors"
    categories = {}

    # Find all collector modules (aws_*.py files)
    for file_path in collectors_dir.glob("aws_*.py"):
        if file_path.name == "aws_base.py":
            continue

        # Get category from filename (aws_compute.py -> Compute)
        category = file_path.stem.replace("aws_", "").title()

        # Import the module
        module_name = f"terraform_aws_migrator.collectors.{file_path.stem}"
        module = importlib.import_module(module_name)

        # Find all collector classes in the module
        collectors = []
        for name, obj in inspect.getmembers(module):
            if (
                inspect.isclass(obj)
                and issubclass(obj, ResourceCollector)
                and obj != ResourceCollector
            ):
                service_name = obj.get_service_name()
                resource_types = obj.get_resource_types()
                collectors.extend(
                    [
                        {
                            "type": resource_type,
                            "description": description,
                            "service": service_name,
                        }
                        for resource_type, description in resource_types.items()
                    ]
                )

        if collectors:
            categories[category] = collectors

    return categories


def show_supported_resources():
    """Display information about supported resource types"""
    console = Console()
    categories = get_collectors_info()

    console.print("\n[bold cyan]Supported AWS Resource Types[/bold cyan]")
    console.print("These resources can be detected by terraform_aws_migrator:\n")

    for category, resources in sorted(categories.items()):
        console.print(f"[bold yellow]{category}[/bold yellow]")
        for resource in sorted(resources, key=lambda x: x["type"]):
            console.print(f"  • {resource['type']}: {resource['description']}")
        console.print("")
</document_content>
</document>

<document>
<source>terraform_aws_migrator/utils/__init__.py</source>
<document_content></document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/__init__.py</source>
<document_content># terraform_aws_migrator/generators/__init__.py

import logging
import pkgutil
import importlib
from pathlib import Path
from .base import HCLGenerator, HCLGeneratorRegistry, register_generator

logger = logging.getLogger(__name__)


def load_generators():
    """Dynamically load all generator modules from all subdirectories"""
    logger.debug("Starting to load generator modules")

    # Get the directory containing the generators
    generators_dir = Path(__file__).parent

    def load_from_directory(directory: Path, package_prefix: str):
        """Recursively load modules from a directory"""
        if not directory.exists():
            return

        # Load modules from current directory
        for module_info in pkgutil.iter_modules([str(directory)]):
            # Skip __init__.py and base.py
            if module_info.name in ['__init__', 'base']:
                continue

            module_name = f"{package_prefix}.{module_info.name}"
            try:
                importlib.import_module(module_name)
                logger.debug(f"Successfully loaded module: {module_name}")
            except Exception as e:
                logger.error(f"Failed to load generator module {module_name}: {str(e)}")

        # Recursively process subdirectories
        for item in directory.iterdir():
            if item.is_dir() and not item.name.startswith('_'):
                subpackage = f"{package_prefix}.{item.name}"
                load_from_directory(item, subpackage)

    # Start loading from the root generators directory
    load_from_directory(generators_dir, __package__)

    # List all registered generators after loading
    registered_types = list(HCLGeneratorRegistry._generators.keys())
    logger.debug(f"Currently registered generator types: {registered_types}")


# Initialize registry and load generators
load_generators()

__all__ = ["HCLGenerator", "HCLGeneratorRegistry", "register_generator"]
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/base.py</source>
<document_content># terraform_aws_migrator/generators/base.py

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Type, Union
import importlib
import os
import pkgutil
import logging

logger = logging.getLogger(__name__)


class HCLGenerator(ABC):
    """Base class for HCL generators"""

    def __init__(self, module_prefix: Optional[str] = None, state_reader: Optional[Any] = None):
        """
        Initialize the generator

        Args:
            module_prefix (str, optional): Module prefix for import commands
            state_reader (TerraformStateReader, optional): State reader instance
        """
        self.module_prefix = module_prefix
        self.state_reader = state_reader
        self.managed_resources = {}
        if state_reader:
            self.managed_resources = state_reader.get_managed_resources("")

    def is_resource_managed(self, resource_type: str, resource_name: str) -> bool:
        """Check if a resource is managed by Terraform"""
        if not self.state_reader:
            return False

        for resource in self.managed_resources.values():
            if resource.get("type") == resource_type and resource.get("id") == resource_name:
                return True
        return False

    @classmethod
    @abstractmethod
    def resource_type(cls) -> str:
        """Return the resource type this generator handles"""
        pass

    @abstractmethod
    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for the given resource"""
        pass

    def get_import_prefix(self) -> str:
        """
        Get the module prefix for import commands

        Returns:
            str: Module prefix string (e.g., "module.my_module") or empty string
        """
        return f"module.{self.module_prefix}" if self.module_prefix else ""


class HCLGeneratorRegistry:
    """Registry for HCL generators"""

    _generators: Dict[str, Type[HCLGenerator]] = {}
    _initialized = False

    @classmethod
    def register(cls, generator_class: Type[HCLGenerator]) -> Type[HCLGenerator]:
        """Register a generator class"""
        resource_type = generator_class.resource_type()
        cls._generators[resource_type] = generator_class
        logger.debug(f"Registered generator for {resource_type}")
        return generator_class

    @classmethod
    def get_generator(
        cls, resource_type: str, module_prefix: Optional[str] = None, state_reader: Optional[Any] = None
    ) -> Optional[HCLGenerator]:
        """
        Get a generator instance for the given resource type

        Args:
            resource_type (str): AWS resource type
            module_prefix (str, optional): Module prefix for import commands
            state_reader (TerraformStateReader, optional): State reader instance

        Returns:
            Optional[HCLGenerator]: Generator instance if supported, None otherwise
        """
        if not cls._initialized:
            cls._initialize()

        generator_class = cls._generators.get(resource_type)
        if generator_class:
            return generator_class(module_prefix=module_prefix, state_reader=state_reader)
        return None

    @classmethod
    def is_supported(cls, resource_type: str) -> bool:
        """
        Check if a resource type or category is supported
        Args:
            resource_type (str): Resource type (e.g., aws_s3_bucket) or category (e.g., s3)
        """
        if not cls._initialized:
            cls._initialize()

        # 完全なリソースタイプの場合
        if resource_type in cls._generators:
            return True

        # カテゴリの場合（例：s3）
        # そのカテゴリに属する任意のリソースタイプが登録されているかチェック
        for registered_type in cls._generators.keys():
            if registered_type.startswith(f"aws_{resource_type}_"):
                return True

        return False

    @classmethod
    def get_generators_for_category(cls, category: str) -> Dict[str, Type[HCLGenerator]]:
        """Get all generators for a given category"""
        if not cls._initialized:
            cls._initialize()

        logger.info(f"Looking for generators in category: {category}")
        logger.debug(f"Available generators: {list(cls._generators.keys())}")
        
        generators = {
            resource_type: generator_class
            for resource_type, generator_class in cls._generators.items()
            if resource_type.startswith(f"aws_{category}_")
        }
        
        if generators:
            logger.info(f"Found {len(generators)} generators for category {category}: {list(generators.keys())}")
        else:
            logger.warning(f"No generators found for category: {category}")
        
        return generators

    @classmethod
    def _initialize(cls) -> None:
        """Initialize the registry by discovering and loading all generators"""
        if cls._initialized:
            logger.debug("Registry already initialized")
            return

        logger.debug("Starting registry initialization")

        # Get the generators directory path
        generators_dir = os.path.dirname(__file__)

        # Function to recursively load modules from a directory
        def load_modules_from_dir(dir_path: str, package_prefix: str) -> None:
            for item in os.listdir(dir_path):
                item_path = os.path.join(dir_path, item)

                # Skip __pycache__ and files starting with _
                if item.startswith("_") or item == "__pycache__":
                    continue

                if os.path.isdir(item_path):
                    # It's a subdirectory - recurse into it
                    subpackage = f"{package_prefix}.{item}"
                    load_modules_from_dir(item_path, subpackage)

                elif item.endswith(".py"):
                    # It's a Python file - try to import it
                    module_name = (
                        f"{package_prefix}.{item[:-3]}"  # Remove .py extension
                    )
                    try:
                        if (
                            module_name != "terraform_aws_migrator.generators.base"
                        ):  # Skip base.py
                            logger.info(f"Attempting to load module: {module_name}")
                            module = importlib.import_module(module_name)
                            logger.info(f"Successfully loaded module: {module_name}")
                            # モジュール内のジェネレータークラスを確認
                            for attr_name in dir(module):
                                attr = getattr(module, attr_name)
                                if isinstance(attr, type) and issubclass(attr, HCLGenerator) and attr != HCLGenerator:
                                    logger.info(f"Found generator class in {module_name}: {attr_name}")
                    except Exception as e:
                        logger.error(
                            f"Failed to load generator module {module_name}: {str(e)}"
                        )

        # Load all modules from the generators directory
        load_modules_from_dir(generators_dir, "terraform_aws_migrator.generators")

        if not cls._generators:
            logger.warning("No generators were registered")
        else:
            logger.debug(f"Registered generators: {list(cls._generators.keys())}")

        cls._initialized = True

    @classmethod
    def list_supported_types(cls) -> Dict[str, str]:
        """List all supported resource types"""
        if not cls._initialized:
            cls._initialize()

        return {
            resource_type: generator_class.__doc__ or ""
            for resource_type, generator_class in cls._generators.items()
        }

    @classmethod
    @abstractmethod
    def resource_type(cls) -> str:
        """Return the resource type this generator handles"""
        pass

    @abstractmethod
    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for the given resource"""
        pass

    @abstractmethod
    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate Terraform import command for the given resource"""
        pass


def register_generator(generator_class: Type[HCLGenerator]) -> Type[HCLGenerator]:
    """Decorator to register a generator class"""
    return HCLGeneratorRegistry.register(generator_class)
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_storage/s3.py</source>
<document_content>from typing import Dict, Any, Optional
import logging
import json
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class S3BucketGenerator(HCLGenerator):
    """Generator for aws_s3_bucket resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_s3_bucket"

    def _generate_resource_name(self, bucket_name: str) -> str:
        """Generate a safe resource name from bucket name"""
        return bucket_name.replace("-", "_").replace(".", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                logger.error("Missing required bucket name")
                return None

            resource_name = self._generate_resource_name(bucket_name)
            hcl_blocks = []

            # Main bucket resource
            main_block = [
                f'resource "aws_s3_bucket" "{resource_name}" {{',
                f'  bucket = "{bucket_name}"',
                '  force_destroy = false  # Default to false for safety'
            ]

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                main_block.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        main_block.append(f'    "{key}" = "{value}"')
                main_block.append("  }")

            main_block.append("}")
            return "\n".join(main_block)

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                logger.error("Missing bucket name for import command")
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket: {str(e)}")
            return None

@register_generator
class S3BucketACLGenerator(HCLGenerator):
    """Generator for aws_s3_bucket_acl resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_s3_bucket_acl"

    def _generate_resource_name(self, bucket_name: str) -> str:
        # Convert dots and underscores to hyphens first, then replace hyphens with underscores
        return bucket_name.replace(".", "-").replace("-", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            details = resource.get("details", {})
            if not details:
                return None

            owner = details.get("owner", {})
            grants = details.get("grants", [])

            hcl_blocks = [
                f'resource "aws_s3_bucket_acl" "{resource_name}" {{',
                f'  bucket = "{bucket_name}"',
                '  access_control_policy {',
            ]

            # Add owner block
            if owner:
                hcl_blocks.extend([
                    '    owner {',
                    f'      id = "{owner.get("ID", "")}"',
                    '    }',
                ])

            # Add grants
            if grants:
                for grant in grants:
                    grantee = grant.get("Grantee", {})
                    hcl_blocks.extend([
                        '    grant {',
                        f'      permission = "{grant.get("Permission", "")}"',
                        '',
                        '      grantee {',
                        f'        type = "{grantee.get("Type", "")}"',
                    ])
                    
                    # Add grantee details based on type
                    # display_nameは自動的に設定されるため、明示的に設定しない
                    if grantee.get("ID"):
                        hcl_blocks.append(f'        id = "{grantee.get("ID")}"')
                    if grantee.get("URI"):
                        hcl_blocks.append(f'        uri = "{grantee.get("URI")}"')
                    
                    hcl_blocks.extend([
                        '      }',
                        '    }',
                    ])

            hcl_blocks.extend([
                '  }',
                '}',
            ])

            return "\n".join(hcl_blocks)

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket ACL: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket_acl.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket ACL: {str(e)}")
            return None

@register_generator
class S3BucketPolicyGenerator(HCLGenerator):
    """Generator for aws_s3_bucket_policy resources"""

    @classmethod
    def resource_type(cls) -> str:
        logger.info("Registering S3BucketPolicyGenerator for aws_s3_bucket_policy")
        return "aws_s3_bucket_policy"

    def _generate_resource_name(self, bucket_name: str) -> str:
        name = bucket_name.replace("-", "_").replace(".", "_")
        logger.debug(f"Generated resource name for bucket {bucket_name}: {name}")
        return name

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        logger.info(f"Starting HCL generation for S3 bucket policy: {resource.get('id')}")
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            policy = resource.get("details", {}).get("policy")
            if not policy:
                return None

            # Ensure policy is properly formatted
            if isinstance(policy, str):
                try:
                    # バケットポリシーはすでにJSON文字列として取得されているため、
                    # 一度パースしてから使用します
                    policy_json = json.loads(policy)
                    logger.info(f"Successfully parsed policy for bucket {bucket_name}")
                    logger.debug(f"Policy content: {json.dumps(policy_json, indent=2)}")
                    
                    # ポリシーを整形して出力
                    formatted_policy = json.dumps(policy_json, indent=2)
                    hcl = [
                        f'resource "aws_s3_bucket_policy" "{resource_name}" {{',
                        f'  bucket = "{bucket_name}"',
                        '  policy = jsonencode(',
                        '    ' + formatted_policy.replace('\n', '\n    '),
                        '  )',
                        "}"
                    ]
                    
                    result = "\n".join(hcl)
                    logger.info(f"Generated HCL for bucket policy: {bucket_name}")
                    logger.debug(f"Generated HCL:\n{result}")
                    return result
                    
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON policy string for bucket {bucket_name}: {e}")
                    return None
            else:
                logger.error(f"Policy must be a JSON string, got {type(policy)} for bucket {bucket_name}")
                logger.debug(f"Actual policy content: {policy}")
                return None

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket policy: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket_policy.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket policy: {str(e)}")
            return None

@register_generator
class S3BucketPublicAccessBlockGenerator(HCLGenerator):
    """Generator for aws_s3_bucket_public_access_block resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_s3_bucket_public_access_block"

    def _generate_resource_name(self, bucket_name: str) -> str:
        return bucket_name.replace("-", "_").replace(".", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            public_access = resource.get("details", {})
            if not public_access:
                return None

            return "\n".join([
                f'resource "aws_s3_bucket_public_access_block" "{resource_name}" {{',
                f'  bucket = "{bucket_name}"',
                f'  block_public_acls       = {str(public_access.get("block_public_acls", True)).lower()}',
                f'  block_public_policy     = {str(public_access.get("block_public_policy", True)).lower()}',
                f'  ignore_public_acls      = {str(public_access.get("ignore_public_acls", True)).lower()}',
                f'  restrict_public_buckets = {str(public_access.get("restrict_public_buckets", True)).lower()}',
                "}"
            ])

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket public access block: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket_public_access_block.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket public access block: {str(e)}")
            return None

@register_generator
class S3BucketCORSGenerator(HCLGenerator):
    """Generator for aws_s3_bucket_cors_configuration resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_s3_bucket_cors_configuration"

    def _generate_resource_name(self, bucket_name: str) -> str:
        return bucket_name.replace("-", "_").replace(".", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            cors_rules = resource.get("details", {}).get("cors_rules", [])
            if not cors_rules:
                return None

            cors_block = [
                f'resource "aws_s3_bucket_cors_configuration" "{resource_name}" {{',
                f'  bucket = "{bucket_name}"'
            ]

            for rule in cors_rules:
                cors_block.append("  cors_rule {")
                if allowed_headers := rule.get("AllowedHeaders"):
                    cors_block.append(f'    allowed_headers = {json.dumps(allowed_headers)}')
                if allowed_methods := rule.get("AllowedMethods"):
                    cors_block.append(f'    allowed_methods = {json.dumps(allowed_methods)}')
                if allowed_origins := rule.get("AllowedOrigins"):
                    cors_block.append(f'    allowed_origins = {json.dumps(allowed_origins)}')
                if expose_headers := rule.get("ExposeHeaders"):
                    cors_block.append(f'    expose_headers = {json.dumps(expose_headers)}')
                if max_age_seconds := rule.get("MaxAgeSeconds"):
                    cors_block.append(f'    max_age_seconds = {max_age_seconds}')
                cors_block.append("  }")

            cors_block.append("}")
            return "\n".join(cors_block)

        except Exception as e:
            logger.error(f"Error generating HCL for S3 bucket CORS configuration: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            bucket_name = resource.get("id")
            if not bucket_name:
                return None

            resource_name = self._generate_resource_name(bucket_name)
            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_s3_bucket_cors_configuration.{resource_name} {bucket_name}"

        except Exception as e:
            logger.error(f"Error generating import command for S3 bucket CORS configuration: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/__init__.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/__init__.py

import os
import importlib
import logging
from typing import List

logger = logging.getLogger(__name__)

def _load_modules() -> List[str]:
    """
    Dynamically load all Python modules in the current directory.
    Skips __init__.py and files starting with _.
    """
    current_dir = os.path.dirname(__file__)
    loaded_modules = []

    for filename in os.listdir(current_dir):
        if (filename.startswith("_") or 
            not filename.endswith(".py") or 
            filename == "__init__.py"):
            continue

        module_name = filename[:-3]
        full_module_path = f"{__package__}.{module_name}"

        try:
            importlib.import_module(full_module_path)
            loaded_modules.append(module_name)
            logger.debug(f"Successfully loaded module: {full_module_path}")
        except Exception as e:
            logger.error(f"Failed to load module {full_module_path}: {str(e)}")

    return loaded_modules

# Load all modules when this package is imported
loaded_modules = _load_modules()

# Export the names of all loaded modules
__all__ = loaded_modules
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/target_group.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/target_group.py

from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class ALBTargetGroupGenerator(HCLGenerator):
    """Generator for aws_lb_target_group resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lb_target_group"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for target group based on existing resource"""
        try:
            name = resource.get("id")
            if not name:
                return None

            # Start building HCL
            hcl = [
                f'resource "aws_lb_target_group" "{name}" {{',
                f'  name = "{name}"',
            ]

            # Add basic settings
            protocol = resource.get("protocol")
            if protocol:
                hcl.append(f'  protocol = "{protocol}"')

            port = resource.get("port")
            if port:
                hcl.append(f"  port = {port}")

            vpc_id = resource.get("vpc_id")
            if vpc_id:
                hcl.append(f'  vpc_id = "{vpc_id}"')

            target_type = resource.get("target_type")
            if target_type:
                hcl.append(f'  target_type = "{target_type}"')

            # Add health check if enabled
            health_check = resource.get("health_check")
            if health_check:  # health_check exists means it's enabled
                hcl.append("  health_check {")
                for key, value in health_check.items():
                    if value is not None:
                        if isinstance(value, bool):
                            hcl.append(f"    {key} = {str(value).lower()}")
                        elif isinstance(value, (int, float)):
                            hcl.append(f"    {key} = {value}")
                        else:
                            hcl.append(f'    {key} = "{value}"')
                hcl.append("  }")

            # Add target group attributes
            deregistration_delay = resource.get("deregistration_delay")
            if deregistration_delay:
                hcl.append(f"  deregistration_delay = {deregistration_delay}")

            lambda_multi_value_headers = resource.get(
                "lambda_multi_value_headers_enabled", False
            )
            hcl.append(
                f"  lambda_multi_value_headers_enabled = {str(lambda_multi_value_headers).lower()}"
            )

            proxy_protocol_v2 = resource.get("proxy_protocol_v2", False)
            hcl.append(f"  proxy_protocol_v2 = {str(proxy_protocol_v2).lower()}")

            slow_start = resource.get("slow_start", 0)
            hcl.append(f"  slow_start = {slow_start}")

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for target group: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for target group"""
        try:
            tg_arn = resource.get("arn")
            tg_name = resource.get("id")

            if not tg_arn or not tg_name:
                logger.error("Missing ARN or name for target group import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lb_target_group.{tg_name} {tg_arn}"

        except Exception as e:
            logger.error(f"Error generating import command for target group: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/listener_rule.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/listener_rule.py

from typing import Dict, Any, Optional, List
import json
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class ALBListenerRuleGenerator(HCLGenerator):
    """Generator for aws_lb_listener_rule resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lb_listener_rule"

    def _format_forward_action(self, action: Dict[str, Any]) -> str:
        """Format forward action with target groups and stickiness"""
        target_groups = action.get("ForwardConfig", {}).get("TargetGroups", [])
        stickiness = action.get("ForwardConfig", {}).get("TargetGroupStickinessConfig", {})
        if not target_groups:
            return ""

        blocks = ['    forward {']
        
        # Add target groups even with weight 0
        for tg in target_groups:
            blocks.extend([
                '      target_group {',
                f'        arn = "{tg.get("TargetGroupArn")}"',
                f'        weight = {tg.get("Weight", 0)}',
                '      }'
            ])

        # Add stickiness if present
        stickiness = action.get("ForwardConfig", {}).get("TargetGroupStickinessConfig", {})
        blocks.extend([
            '      stickiness {',
            f'        enabled = {str(stickiness.get("Enabled", True)).lower()}',
            f'        duration = {stickiness.get("DurationSeconds", stickiness.get("duration", 3600))}',  # Try to get DurationSeconds first, then duration, finally fallback to 3600
            '      }'
        ])

        blocks.append('    }')
        return '\n'.join(blocks)

    def _format_conditions(self, conditions: List[Dict[str, Any]]) -> str:
        """Format conditions block including http_header"""
        condition_blocks = []
        
        for condition in conditions:
            # HTTP Header condition
            if 'HttpHeaderConfig' in condition:
                config = condition['HttpHeaderConfig']
                condition_blocks.extend([
                    '  condition {',
                    '    http_header {',
                    f'      http_header_name = "{config["HttpHeaderName"]}"',
                    f'      values = {json.dumps(config["Values"])}',
                    '    }',
                    '  }'
                ])
            
            # Path Pattern condition
            elif 'PathPatternConfig' in condition:
                config = condition['PathPatternConfig']
                condition_blocks.extend([
                    '  condition {',
                    '    path_pattern {',
                    f'      values = {json.dumps(config["Values"])}',
                    '    }',
                    '  }'
                ])

            # Host Header condition
            elif 'HostHeaderConfig' in condition:
                config = condition['HostHeaderConfig']
                condition_blocks.extend([
                    '  condition {',
                    '    host_header {',
                    f'      values = {json.dumps(config["Values"])}',
                    '    }',
                    '  }'
                ])

        return '\n'.join(condition_blocks)

    def _format_tags(self, tags: List[Dict[str, str]]) -> Optional[str]:
        """Format resource tags"""
        if not tags:
            return None

        tag_blocks = ['  tags = {']
        for tag in tags:
            if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                key = tag["Key"].replace('"', '\\"')
                value = tag["Value"].replace('"', '\\"')
                tag_blocks.append(f'    "{key}" = "{value}"')
        tag_blocks.append('  }')
        return '\n'.join(tag_blocks)

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            rule_id = resource.get('id')
            details = resource.get('details', {})
            tags = resource.get('tags', [])
            listener_arn = details.get('listener_arn')
            priority = details.get('priority')
            actions = details.get('actions', [])
            conditions = details.get('conditions', [])

            if not all([rule_id, listener_arn, priority]):
                logger.error("Missing required fields for listener rule")
                return None

            # Start building HCL
            hcl_blocks = [
                f'resource "aws_lb_listener_rule" "rule_{rule_id}" {{',
                f'  listener_arn = "{listener_arn}"',
                f'  priority     = {priority}'
            ]

            # Add actions
            for action in actions:
                action_type = action.get('Type', '').lower()
                hcl_blocks.append('  action {')
                hcl_blocks.append(f'    type = "{action_type}"')
                
                if action_type == 'forward':
                    forward_config = self._format_forward_action(action)
                    if forward_config:
                        hcl_blocks.append(forward_config)
                
                hcl_blocks.append('  }')

            # Add conditions
            if conditions:
                condition_blocks = self._format_conditions(conditions)
                if condition_blocks:
                    hcl_blocks.append(condition_blocks)

            # Add tags if present
            tags_block = self._format_tags(tags)
            if tags_block:
                hcl_blocks.append(tags_block)

            hcl_blocks.append('}')
            return '\n'.join(hcl_blocks)

        except Exception as e:
            logger.error(f"Error generating HCL for listener rule: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for listener rule"""
        try:
            rule_arn = resource.get('arn')
            rule_id = resource.get('id')

            if not rule_arn or not rule_id:
                logger.error("Missing ARN or ID for listener rule import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lb_listener_rule.rule_{rule_id} {rule_arn}"

        except Exception as e:
            logger.error(f"Error generating import command for listener rule: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/lb.py</source>
<document_content>from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class LoadBalancerGenerator(HCLGenerator):
    """Generator for aws_lb resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lb"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for an Application Load Balancer"""
        try:
            lb_name = resource.get("id")
            details = resource.get("details", {})

            if not lb_name:
                logger.error("Missing required load balancer name")
                return None

            # Start building HCL
            hcl = [
                f'resource "aws_lb" "{lb_name}" {{',
                f'  name = "{lb_name}"',
            ]

            # Add load balancer type
            hcl.append('  load_balancer_type = "application"')

            # Add internal/external scheme
            scheme = details.get("scheme")
            if scheme == "internal":
                hcl.append('  internal = true')
            else:
                hcl.append('  internal = false')

            # Add security groups
            security_groups = details.get("security_groups", [])
            if security_groups:
                groups_str = '", "'.join(security_groups)
                hcl.append(f'  security_groups = ["{groups_str}"]')

            # Add subnets
            subnets = details.get("subnets", [])
            if subnets:
                subnets_str = '", "'.join(subnets)
                hcl.append(f'  subnets = ["{subnets_str}"]')

            # Add IP address type
            ip_address_type = details.get("ip_address_type")
            if ip_address_type:
                hcl.append(f'  ip_address_type = "{ip_address_type.lower()}"')

            # Add idle timeout
            idle_timeout = details.get("idle_timeout")
            if isinstance(idle_timeout, (int, str)) and str(idle_timeout).isdigit():
                hcl.append(f'  idle_timeout = {int(idle_timeout)}')

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close the resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for Load Balancer: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for Load Balancer"""
        try:
            lb_arn = resource.get("arn")
            lb_name = resource.get("id")

            if not lb_arn or not lb_name:
                logger.error("Missing required ARN or name for Load Balancer import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lb.{lb_name} {lb_arn}"

        except Exception as e:
            logger.error(f"Error generating import command for Load Balancer: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_network/listener.py</source>
<document_content># terraform_aws_migrator/generators/aws_network/listener.py

from typing import Dict, Any, List, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

class ListenerConfigError(Exception):
    """Exception raised for missing listener configuration"""
    pass

@register_generator
class ALBListenerGenerator(HCLGenerator):
    """Generator for aws_lb_listener resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lb_listener"

    def _format_certificates(self, certificates: list) -> str:
        """Format SSL certificate configuration block"""
        if not certificates:
            return ""
        
        cert_blocks = []
        for cert in certificates:
            if not cert.get("CertificateArn"):
                continue
            cert_block = [
                "  certificate {",
                f'    certificate_arn = "{cert["CertificateArn"]}"'
            ]
            if cert.get("IsDefault"):
                cert_block.append("    is_default = true")
            cert_block.append("  }")
            cert_blocks.append("\n".join(cert_block))
        
        return "\n".join(cert_blocks)

    def _format_forward_config(self, action: Dict[str, Any]) -> List[str]:
        """Format forward action configuration"""
        config = []
        target_group_arn = None
        
        # Get target group ARN and prepare target groups
        if "TargetGroupArn" in action:
            target_group_arn = action["TargetGroupArn"]
            target_groups = [{
                "TargetGroupArn": target_group_arn,
                "Weight": action.get("Weight", 1)
            }]
        else:
            forward_config = action.get("ForwardConfig", {})
            target_groups = forward_config.get("TargetGroups", [])
            if target_groups and len(target_groups) == 1:
                target_group_arn = target_groups[0]["TargetGroupArn"]

        # Add target_group_arn setting
        if target_group_arn:
            config.append(f'    target_group_arn = "{target_group_arn}"')

        # Add forward block
        lines = ["forward {"]
        
        # Format target groups
        for tg in target_groups:
            target_group_config = [
                "  target_group {",
                f'    arn    = "{tg["TargetGroupArn"]}"'
            ]
            
            # Use weight from the configuration if available
            if "Weight" in tg:
                target_group_config.append(f'    weight = {tg["Weight"]}')
            
            target_group_config.append("  }")
            lines.extend(target_group_config)

        # Add stickiness configuration from actual settings
        stickiness = action.get("ForwardConfig", {}).get("TargetGroupStickinessConfig", {})
        duration_seconds = stickiness.get("DurationSeconds", 1)  # Fallback to 1 if not set
        
        # Stickiness block is always required with both enabled and duration
        lines.extend([
            "  stickiness {",
            f'    enabled  = {str(stickiness.get("Enabled", False)).lower()}',
            f'    duration = {duration_seconds}',
            "  }"
        ])

        lines.append("}")
        config.append("    " + "\n    ".join(lines))
        
        return config

    def _format_fixed_response_config(self, action: Dict[str, Any]) -> List[str]:
        """Format fixed response action configuration"""
        fixed_response_config = action.get("FixedResponseConfig", {})
        if not fixed_response_config:
            return []

        lines = ["    fixed_response {"]
        
        if "ContentType" in fixed_response_config:
            lines.append(f'      content_type = "{fixed_response_config["ContentType"]}"')
        if "MessageBody" in fixed_response_config:
            lines.append(f'      message_body = "{fixed_response_config["MessageBody"]}"')
        if "StatusCode" in fixed_response_config:
            lines.append(f'      status_code  = "{fixed_response_config["StatusCode"]}"')

        lines.append("    }")
        return lines

    def _format_redirect_config(self, action: Dict[str, Any]) -> List[str]:
        """Format redirect action configuration"""
        redirect_config = action.get("RedirectConfig", {})
        if not redirect_config:
            return []

        lines = ["    redirect {"]
        
        param_mapping = {
            "Host": "host",
            "Path": "path",
            "Port": "port",
            "Protocol": "protocol",
            "Query": "query",
            "StatusCode": "status_code"
        }

        for aws_param, tf_param in param_mapping.items():
            if aws_param in redirect_config:
                value = redirect_config[aws_param]
                lines.append(f'      {tf_param} = "{value}"')

        lines.append("    }")
        return lines

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for an ALB listener"""
        try:
            listener_id = resource.get("id")
            details = resource.get("details", {})
            
            if not listener_id or not details:
                raise ListenerConfigError("Missing required listener details")

            # Start building HCL
            hcl = [
                f'resource "aws_lb_listener" "listener_{listener_id}" {{',
                f'  load_balancer_arn = "{details.get("load_balancer_arn")}"',
                f'  port              = {details.get("port", 80)}',
                f'  protocol          = "{details.get("protocol", "HTTP")}"'
            ]

            # Add SSL policy for HTTPS
            if details.get("protocol") == "HTTPS":
                ssl_policy = details.get("ssl_policy")
                if ssl_policy:
                    hcl.append(f'  ssl_policy = "{ssl_policy}"')

                # Add certificate configuration
                certificates = details.get("certificates", [])
                cert_blocks = self._format_certificates(certificates)
                if cert_blocks:
                    hcl.append(cert_blocks)

            # Handle default action
            actions = details.get("actions", [])
            if not actions:
                raise ListenerConfigError(f"No default_action found for listener {listener_id}")

            default_action = actions[0]  # First action is the default action
            action_type = default_action.get("Type", "").lower()
            
            # Add default action block
            hcl.append("  default_action {")
            hcl.append(f'    type = "{action_type}"')
            
            # Format action configuration based on type
            if action_type == "forward":
                config = self._format_forward_config(default_action)
            elif action_type == "fixed-response":
                config = self._format_fixed_response_config(default_action)
            elif action_type == "redirect":
                config = self._format_redirect_config(default_action)
            else:
                config = []

            hcl.extend(config)
            
            # Close default_action block
            hcl.append("  }")

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for ALB listener: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for ALB listener"""
        try:
            listener_arn = resource.get("arn")
            listener_id = resource.get("id")

            if not listener_arn or not listener_id:
                raise ListenerConfigError("Missing ARN or ID for listener import command")

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lb_listener.listener_{listener_id} {listener_arn}"

        except Exception as e:
            logger.error(f"Error generating import command for ALB listener: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/user.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/user.py

from typing import Dict, Any, Optional, List
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMUserGenerator(HCLGenerator):
    """Generator for aws_iam_user resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_user"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("id")
            details = resource.get("details", {})

            # Start building HCL
            hcl = [
                f'resource "aws_iam_user" "{user_name}" {{',
                f'  name = "{user_name}"',
            ]

            # Add path if not default
            path = details.get("path", "/")
            if path != "/":
                hcl.append(f'  path = "{path}"')

            # Add permissions boundary if present
            permissions_boundary = details.get("permissions_boundary")
            if permissions_boundary:
                hcl.append(f'  permissions_boundary = "{permissions_boundary}"')

            # Add force_destroy if specified
            force_destroy = details.get("force_destroy", False)
            if force_destroy:
                hcl.append("  force_destroy = true")

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM user: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for IAM user"""
        try:
            user_name = resource.get("id")
            if not user_name:
                logger.error("Missing user name for import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_iam_user.{user_name} {user_name}"

        except Exception as e:
            logger.error(f"Error generating import command for IAM user: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/instance_profile.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/instance_profile.py

from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)

@register_generator
class IAMInstanceProfileGenerator(HCLGenerator):
    """Generator for aws_iam_instance_profile resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_instance_profile"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for an IAM Instance Profile"""
        try:
            profile_name = resource.get("id")
            details = resource.get("details", {})

            if not profile_name:
                logger.error("Missing required instance profile name")
                return None

            # Start building HCL
            hcl = [
                f'resource "aws_iam_instance_profile" "{profile_name}" {{',
                f'  name = "{profile_name}"'
            ]

            # Add path if not default
            path = details.get("path")
            if path and path != "/":
                hcl.append(f'  path = "{path}"')

            # Add role if present
            role_name = details.get("role_name")
            if role_name:
                hcl.append(f'  role = "{role_name}"')

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM instance profile: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for IAM Instance Profile"""
        try:
            profile_name = resource.get("id")
            if not profile_name:
                logger.error("Missing instance profile name for import command")
                return None

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_iam_instance_profile.{profile_name} {profile_name}"

        except Exception as e:
            logger.error(f"Error generating import command for IAM instance profile: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/policy.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/policy.py

from typing import Dict, Any, Optional
import json
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMPolicyGenerator(HCLGenerator):
    """Generator for aws_iam_policy resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_policy"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            policy_name = resource.get("id")
            details = resource.get("details", {})
            policy_document = details.get("policy_document", {})

            if not policy_name or not policy_document:
                logger.error("Missing required fields for IAM policy generation")
                return None

            # Start building HCL
            hcl = [
                f'resource "aws_iam_policy" "{policy_name}" {{',
                f'  name = "{policy_name}"',
            ]

            # Add description if present
            description = details.get("description")
            if description:
                hcl.append(f'  description = "{description}"')

            # Add path if not default
            path = details.get("path", "/")
            if path != "/":
                hcl.append(f'  path = "{path}"')

            # Add policy document
            hcl.append(
                f"  policy = jsonencode({json.dumps(policy_document, indent=2)})"
            )

            # Add tags if present
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    key = tag.get("Key", "").replace('"', '\\"')
                    value = tag.get("Value", "").replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM policy: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate terraform import command for IAM policy"""
        try:
            arn = resource.get("arn")
            policy_name = resource.get("id")

            if not arn or not policy_name:
                logger.error("Missing ARN or policy name for import command generation")
                return None

            prefix = self.get_import_prefix()
            return (
                f"terraform import {prefix + '.' if prefix else ''}"
                f"aws_iam_policy.{policy_name} {arn}"
            )

        except Exception as e:
            logger.error(f"Error generating import command for IAM policy: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/__init__.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/__init__.py

import os
import importlib
import logging
from typing import List

logger = logging.getLogger(__name__)


def _load_modules() -> List[str]:
    """Dynamically load all Python modules in the current directory"""
    current_dir = os.path.dirname(__file__)
    loaded_modules = []

    for filename in os.listdir(current_dir):
        if (
            filename.startswith("_")
            or not filename.endswith(".py")
            or filename == "__init__.py"
        ):
            continue

        module_name = filename[:-3]
        full_module_path = f"{__package__}.{module_name}"

        try:
            importlib.import_module(full_module_path)
            loaded_modules.append(module_name)
        except Exception as e:
            logger.error(
                f"Failed to load IAM generator module {full_module_path}: {str(e)}"
            )

    return loaded_modules


# Load all modules when this package is imported
loaded_modules = _load_modules()

# Export loaded module names
__all__ = loaded_modules
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/user_policy_attachment.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/user_policy_attachment.py

from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMUserPolicyAttachmentGenerator(HCLGenerator):
    """Generator for aws_iam_user_policy_attachment resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_user_policy_attachment"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("user_name")
            policy_arn = resource.get("policy_arn")

            if not all([user_name, policy_arn]):
                logger.error(
                    "Missing required fields for user policy attachment generation"
                )
                return None

            # Create unique resource name from user name and policy name
            policy_name = policy_arn.split("/")[-1].replace("-", "_")
            resource_name = f"{user_name}_{policy_name}"

            # Generate HCL
            hcl = [
                f'resource "aws_iam_user_policy_attachment" "{resource_name}" {{',
                f'  user       = "{user_name}"',
                f'  policy_arn = "{policy_arn}"',
                "}",
            ]

            return "\n".join(hcl)

        except Exception as e:
            logger.error(
                f"Error generating HCL for IAM user policy attachment: {str(e)}"
            )
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("user_name")
            policy_arn = resource.get("policy_arn")

            if not all([user_name, policy_arn]):
                logger.error(
                    "Missing required fields for user policy attachment import command"
                )
                return None

            # Create the same resource name used in generate()
            policy_name = policy_arn.split("/")[-1].replace("-", "_")
            resource_name = f"{user_name}_{policy_name}"

            # Import identifier format: username/policy_arn
            import_id = f"{user_name}/{policy_arn}"

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_iam_user_policy_attachment.{resource_name} {import_id}"

        except Exception as e:
            logger.error(
                f"Error generating import command for IAM user policy attachment: {str(e)}"
            )
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/role.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/role.py

from typing import Dict, Any, Optional
import json
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMRoleGenerator(HCLGenerator):
    """Generator for aws_iam_role resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_role"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            role_name = resource.get("id")
            details = resource.get("details", {})

            # Buffer to store all HCL blocks
            hcl_blocks = []

            # Generate main role HCL
            if resource["type"] == "aws_iam_role":
                assume_role_policy = details.get("assume_role_policy", {})
                description = details.get("description", "")
                path = details.get("path", "/")

                role_hcl = [
                    f'resource "aws_iam_role" "{role_name}" {{',
                    f'  name = "{role_name}"',
                ]

                if description:
                    role_hcl.append(f'  description = "{description}"')

                if path != "/":
                    role_hcl.append(f'  path = "{path}"')

                role_hcl.append(
                    f"  assume_role_policy = jsonencode({json.dumps(assume_role_policy, indent=2)})"
                )

                # Add tags if present
                tags = resource.get("tags", [])
                if tags:
                    role_hcl.append("  tags = {")
                    for tag in tags:
                        key = tag.get("Key", "").replace('"', '\\"')
                        value = tag.get("Value", "").replace('"', '\\"')
                        role_hcl.append(f'    "{key}" = "{value}"')
                    role_hcl.append("  }")

                role_hcl.append("}")
                hcl_blocks.append("\n".join(role_hcl))

            return "\n\n".join(hcl_blocks)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM role: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            role_name = resource.get("id")
            prefix = self.get_import_prefix()
            if not role_name:
                logger.error("Missing role name for import command generation")
                return None

            return f"terraform import {prefix + '.' if prefix else ''}" \
                f"aws_iam_role.{role_name} {role_name}"

        except Exception as e:
            logger.error(f"Error generating import command for IAM role: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/role_policy_attachment.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/role_policy_attachment.py

from typing import Dict, Any, Optional
import logging
from ..base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMRolePolicyAttachmentGenerator(HCLGenerator):
    """Generator for aws_iam_role_policy_attachment resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_role_policy_attachment"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            role_name = resource.get("role_name")
            policy_arn = resource.get("policy_arn")

            if not role_name or not policy_arn:
                logger.error("Missing required fields for role policy attachment")
                return None

            # Create unique resource name
            policy_name = policy_arn.split("/")[-1].replace("-", "_")
            resource_name = f"{role_name}_{policy_name}"

            # Generate HCL
            hcl = [
                f'resource "aws_iam_role_policy_attachment" "{resource_name}" {{',
                f'  role       = "{role_name}"',
                f'  policy_arn = "{policy_arn}"',
                "}",
            ]

            return "\n".join(hcl)

        except Exception as e:
            logger.error(
                f"Error generating HCL for IAM role policy attachment: {str(e)}"
            )
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            role_name = resource.get("role_name")
            policy_arn = resource.get("policy_arn")

            if not role_name or not policy_arn:
                logger.error(
                    "Missing required fields for role policy attachment import command"
                )
                return None

            policy_name = policy_arn.split("/")[-1].replace("-", "_")
            resource_name = f"{role_name}_{policy_name}"

            import_id = f"{role_name}/{policy_arn}"
            prefix = ""
            return f"terraform import {prefix}.aws_iam_role_policy_attachment.{resource_name} {import_id}"

        except Exception as e:
            logger.error(
                f"Error generating import command for IAM role policy attachment: {str(e)}"
            )
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_iam/user_policy.py</source>
<document_content># terraform_aws_migrator/generators/aws_iam/user_policy.py

from typing import Dict, Any, Optional
import json
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class IAMUserPolicyGenerator(HCLGenerator):
    """Generator for aws_iam_user_policy resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_iam_user_policy"

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("user_name")
            policy_name = resource.get("policy_name")
            policy_document = resource.get("policy_document", {})

            if not all([user_name, policy_name, policy_document]):
                logger.error("Missing required fields for user policy generation")
                return None

            # Create unique resource identifier
            resource_id = f"{user_name}_{policy_name}".replace("-", "_")

            # Generate HCL
            hcl = [
                f'resource "aws_iam_user_policy" "{resource_id}" {{',
                f'  name = "{policy_name}"',
                f'  user = "{user_name}"',
                "",
                f"  policy = jsonencode({json.dumps(policy_document, indent=2)})",
                "}",
            ]

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for IAM user policy: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            user_name = resource.get("user_name")
            policy_name = resource.get("policy_name")

            if not all([user_name, policy_name]):
                logger.error("Missing required fields for user policy import command")
                return None

            # Create resource identifier matching the one in generate()
            resource_id = f"{user_name}_{policy_name}".replace("-", "_")

            # Import identifier format: username:policyname
            import_id = f"{user_name}:{policy_name}"

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_iam_user_policy.{resource_id} {import_id}"

        except Exception as e:
            logger.error(
                f"Error generating import command for IAM user policy: {str(e)}"
            )
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_compute/lambda.py</source>
<document_content># terraform_aws_migrator/generators/aws_compute/lambda.py

from typing import Dict, Any, Optional, List, Tuple, Union
import logging
import json
import base64
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class LambdaFunctionGenerator(HCLGenerator):
    """Generator for aws_lambda_function resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_lambda_function"

    def _get_name_from_tags(self, tags: Dict[str, str]) -> Optional[str]:
        """Get Name tag value from tags dictionary"""
        if isinstance(tags, dict):
            return tags.get("Name")
        return None

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from function name"""
        function_name = resource.get("id", "")
        return function_name.replace("-", "_").replace(".", "_")

    def _get_lambda_code(self, code_location: Dict[str, Any]) -> Optional[str]:
        """Retrieve Lambda function code from code location"""
        if not code_location:
            return None

        try:
            if "ZipFile" in code_location:
                return base64.b64decode(code_location["ZipFile"]).decode('utf-8')
            return None
        except Exception as e:
            logger.error(f"Error retrieving Lambda code: {str(e)}")
            return None

    def _get_source_config(self, details: Dict[str, Any], resource_name: str) -> Tuple[List[str], Optional[Dict[str, str]]]:
        """Determine and format the appropriate source configuration"""
        package_type = details.get("package_type", "Zip")
        lines = []
        file_content = None

        if package_type == "Image":
            # For container images
            if image_uri := details.get("image_uri"):
                lines.append(f'  image_uri = "{image_uri}"')
            lines.append('  package_type = "Image"')
            return lines, None

        # For zip packages
        code_location = details.get("code", {})
        if s3_bucket := code_location.get("s3_bucket"):
            # S3 source
            lines.append(f'  s3_bucket = "{s3_bucket}"')
            if s3_key := code_location.get("s3_key"):
                lines.append(f'  s3_key = "{s3_key}"')
            if s3_object_version := code_location.get("s3_object_version"):
                lines.append(f'  s3_object_version = "{s3_object_version}"')
        else:
            # Try to get inline code
            if code := self._get_lambda_code(code_location):
                file_content = {
                    "index.py": code  # or index.js, depending on runtime
                }
                # Add archive configuration
                lines.extend([
                    '  filename = "${' + f'data.archive_file.{resource_name}_lambda.output_path' + '}"',
                    '  source_code_hash = "${' + f'data.archive_file.{resource_name}_lambda.output_base64sha256' + '}"'
                    f'  source_code_hash = data.archive_file.{resource_name}.output_base64sha256'
                ])

        return lines, file_content

    def _format_function_layers(self, layers: List[str]) -> str:
        """Format Lambda layers configuration"""
        if not layers:
            return ""
        layer_arns = '", "'.join(layers)
        return f'  layers = ["{layer_arns}"]'

    def _format_image_config(self, image_config: Dict[str, Any]) -> List[str]:
        """Format container image configuration"""
        if not image_config:
            return []

        lines = ["  image_config {"]
        
        if command := image_config.get("command"):
            commands = ", ".join(f'"{cmd}"' for cmd in command)
            lines.append(f"    command = [{commands}]")
            
        if entry_point := image_config.get("entry_point"):
            entry_points = ", ".join(f'"{ep}"' for ep in entry_point)
            lines.append(f"    entry_point = [{entry_points}]")
            
        if working_directory := image_config.get("working_directory"):
            lines.append(f'    working_directory = "{working_directory}"')

        lines.append("  }")
        return lines

    def _get_main_file_name(self, handler: Optional[str], runtime: str) -> str:
        """Get the main file name from handler and runtime"""
        # ハンドラーが有効な場合、そこからファイル名を抽出
        if handler and '.' in handler:
            file_base = handler.split('.')[0]
        else:
            # ハンドラーが無効な場合はデフォルトのファイル名を使用
            file_base = "index"
        
        # ランタイムに基づいて適切な拡張子を追加
        if runtime and "python" in runtime.lower():
            return f"{file_base}.py"
        elif runtime and "node" in runtime.lower():
            return f"{file_base}.js"
        else:
            # デフォルトはPython
            return f"{file_base}.py"

    def _generate_archive_file(self, resource_name: str, files: Dict[str, str], runtime: str, handler: Optional[str] = None) -> List[str]:
        """Generate archive_file data source configuration"""
        main_file = self._get_main_file_name(handler, runtime)

        lines = [
            f'data "archive_file" "{resource_name}_lambda" {{',
            '  type        = "zip"',
            f'  output_path = "${{path.module}}/files/{resource_name}.zip"',
            "",
            "  source {",
            f'    content  = <<EOF',
            files.get(main_file, "# Empty function"),
            'EOF',
            f'    filename = "{main_file}"',
            "  }",
            "}"
        ]
        return lines

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for a Lambda function"""
        try:
            function_name = resource.get("id")
            details = resource.get("details", {})

            if not function_name or not details:
                logger.error("Missing required Lambda function details")
                return None

            # Generate resource name
            resource_name = self._generate_resource_name(resource)

            # Get source configuration and potential inline code
            source_config, file_content = self._get_source_config(details, resource_name)

            # Start building HCL blocks
            hcl_blocks = []

            # Add archive_file data source if we have inline code
            if file_content:
                archive_block = self._generate_archive_file(
                    resource_name,
                    file_content,
                    details.get("runtime", ""),
                    details.get("handler", "")
                )
                hcl_blocks.extend(archive_block)
                hcl_blocks.append("")  # Add spacing

            # Start main Lambda resource
            hcl = [
                f'resource "aws_lambda_function" "{resource_name}" {{',
                f'  function_name = "{function_name}"',
                f'  role          = "{details.get("role")}"',
            ]

            # Add package type
            if package_type := details.get("package_type"):
                hcl.append(f'  package_type = "{package_type}"')

            # Add source configuration based on package type
            if package_type == "Image":
                if image_uri := details.get("image_uri"):
                    hcl.append(f'  image_uri = "{image_uri}"')
            else:
                # Add source configuration for Zip packages
                hcl.extend(source_config)
                # Add handler and runtime for zip packages
                if handler := details.get("handler"):
                    hcl.append(f'  handler = "{handler}"')
                if runtime := details.get("runtime"):
                    hcl.append(f'  runtime = "{runtime}"')

            # Add optional fields
            if description := details.get("description"):
                hcl.append(f'  description = "{description}"')

            if memory_size := details.get("memory_size"):
                hcl.append(f'  memory_size = {memory_size}')

            if timeout := details.get("timeout"):
                hcl.append(f'  timeout = {timeout}')

            # Always add publish parameter with default value false
            publish = details.get("publish", False)
            hcl.append(f'  publish = {str(publish).lower()}')

            # Add layers if present
            if layers := details.get("layers", []):
                layer_config = self._format_function_layers(layers)
                if layer_config:
                    hcl.append(layer_config)

            # Add environment variables
            environment = details.get("environment", {})
            if environment and environment.get("variables"):
                hcl.append("  environment {")
                hcl.append("    variables = {")
                for key, value in environment["variables"].items():
                    hcl.append(f'      {key} = "{value}"')
                hcl.append("    }")
                hcl.append("  }")

            # Add VPC configuration
            vpc_config = details.get("vpc_config", {})
            if vpc_config:
                hcl.append("  vpc_config {")
                if subnet_ids := vpc_config.get("subnet_ids", []):
                    subnet_ids_str = '", "'.join(subnet_ids)
                    hcl.append(f'    subnet_ids = ["{subnet_ids_str}"]')
                if security_group_ids := vpc_config.get("security_group_ids", []):
                    sg_ids_str = '", "'.join(security_group_ids)
                    hcl.append(f'    security_group_ids = ["{sg_ids_str}"]')
                hcl.append("  }")

            # Add dead letter config
            dead_letter_config = details.get("dead_letter_config", {})
            if dead_letter_config and (target_arn := dead_letter_config.get("target_arn")):
                hcl.append("  dead_letter_config {")
                hcl.append(f'    target_arn = "{target_arn}"')
                hcl.append("  }")

            # Add tracing config
            tracing_config = details.get("tracing_config", {})
            if tracing_config and (mode := tracing_config.get("mode")):
                hcl.append("  tracing_config {")
                hcl.append(f'    mode = "{mode}"')
                hcl.append("  }")

            # Add file system config if present
            file_system_config = details.get("file_system_config", {})
            if file_system_config:
                hcl.append("  file_system_config {")
                if arn := file_system_config.get("arn"):
                    hcl.append(f'    arn = "{arn}"')
                if local_mount_path := file_system_config.get("local_mount_path"):
                    hcl.append(f'    local_mount_path = "{local_mount_path}"')
                hcl.append("  }")

            # Add tags
            tags = resource.get("tags", {})
            if tags:
                hcl.append("  tags = {")
                for key, value in tags.items():
                    key = key.replace('"', '\\"')
                    value = value.replace('"', '\\"')
                    hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            # Add the main resource block to our blocks
            hcl_blocks.extend(hcl)

            return "\n".join(hcl_blocks)

        except Exception as e:
            logger.error(f"Error generating HCL for Lambda function: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for Lambda function"""
        try:
            function_name = resource.get("id")
            if not function_name:
                logger.error("Missing function name for import command")
                return None

            # Generate resource name matching the one in generate()
            resource_name = self._generate_resource_name(resource)

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_lambda_function.{resource_name} {function_name}"

        except Exception as e:
            logger.error(f"Error generating import command for Lambda function: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_compute/ec2.py</source>
<document_content># terraform_aws_migrator/generators/aws_compute/ec2.py

from typing import Dict, List, Any, Optional
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class EC2InstanceGenerator(HCLGenerator):
    """Generator for aws_instance resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_instance"

    def _get_name_from_tags(self, tags: List[Dict[str, str]]) -> Optional[str]:
        """Get Name tag value from tags list"""
        for tag in tags:
            if isinstance(tag, dict) and tag.get("Key") == "Name":
                return tag.get("Value")
        return None

    def _get_short_instance_id(self, instance_id: str) -> str:
        """Get shortened version of instance ID (last 4 characters)"""
        return instance_id[-4:] if instance_id else ""

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from Name tag or instance ID"""
        instance_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = self._get_name_from_tags(tags)
        short_id = self._get_short_instance_id(instance_id)

        if name_tag:
            # If there's a Name tag, use it with the short instance ID as suffix
            base_name = name_tag.replace("-", "_").replace(" ", "_")
            return f"{base_name}_{short_id}"
        else:
            # Fallback to instance ID if no Name tag
            return instance_id.replace("-", "_")

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate HCL for an EC2 instance"""
        try:
            instance_id = resource.get("id")
            if not instance_id:
                logger.error("Missing required instance ID")
                return None

            # Generate resource name based on Name tag or instance ID
            resource_name = self._generate_resource_name(resource)

            # Start building HCL
            hcl = [
                f'resource "aws_instance" "{resource_name}" {{',
            ]

            # Add instance details if available
            details = resource.get("details", {})

            # Add required fields with fallback values
            if details.get("ami"):
                hcl.append(f'  ami = "{details["ami"]}"')

            if details.get("instance_type"):
                hcl.append(f'  instance_type = "{details["instance_type"]}"')

            # Add optional fields if present
            if details.get("availability_zone"):
                hcl.append(f'  availability_zone = "{details["availability_zone"]}"')

            if details.get("subnet_id"):
                hcl.append(f'  subnet_id = "{details["subnet_id"]}"')

            if details.get("key_name"):
                hcl.append(f'  key_name = "{details["key_name"]}"')

            # Add VPC security groups if present
            vpc_security_groups = details.get("vpc_security_group_ids", [])
            if vpc_security_groups:
                security_groups_str = '", "'.join(vpc_security_groups)
                hcl.append(f'  vpc_security_group_ids = ["{security_groups_str}"]')

            # Add IAM instance profile if present
            if details.get("iam_instance_profile"):
                hcl.append(f'  iam_instance_profile = "{details["iam_instance_profile"]}"')

            # Add monitoring configuration
            monitoring = details.get("monitoring", False)
            hcl.append(f"  monitoring = {str(monitoring).lower()}")

            # Add root block device if present
            root_block_device = details.get("root_block_device")
            if root_block_device:
                hcl.extend([
                    "  root_block_device {",
                    f'    volume_size = {root_block_device.get("volume_size", 8)}',
                    f'    volume_type = "{root_block_device.get("volume_type", "gp2")}"',
                    f'    encrypted = {str(root_block_device.get("encrypted", False)).lower()}',
                    "  }",
                ])

            # Add EBS block devices if present
            ebs_block_devices = details.get("ebs_block_device", [])
            for device in ebs_block_devices:
                hcl.extend([
                    "  ebs_block_device {",
                    f'    device_name = "{device.get("device_name")}"',
                    f'    volume_size = {device.get("volume_size", 8)}',
                    f'    volume_type = "{device.get("volume_type", "gp2")}"',
                    f'    encrypted = {str(device.get("encrypted", False)).lower()}',
                    "  }",
                ])

            # Add user data if present
            user_data = details.get("user_data")
            if user_data:
                hcl.append(f'  user_data = "{user_data}"')

            # Set user_data_replace_on_change to false (Terraform default)
            # This setting determines whether changes to user_data should trigger instance replacement
            hcl.append("  user_data_replace_on_change = false")

            # Add instance tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for EC2 instance: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for EC2 instance"""
        try:
            instance_id = resource.get("id")
            if not instance_id:
                logger.error("Missing instance ID for import command")
                return None

            # Generate resource name matching the one in generate()
            resource_name = self._generate_resource_name(resource)

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_instance.{resource_name} {instance_id}"

        except Exception as e:
            logger.error(f"Error generating import command for EC2 instance: {str(e)}")
            return None
</document_content>
</document>

<document>
<source>terraform_aws_migrator/generators/aws_compute/security_group.py</source>
<document_content># terraform_aws_migrator/generators/aws_compute/security_group.py

from typing import Dict, List, Any, Optional
import logging
from terraform_aws_migrator.generators.base import HCLGenerator, register_generator

logger = logging.getLogger(__name__)


@register_generator
class SecurityGroupGenerator(HCLGenerator):
    """Generator for aws_security_group resources"""

    @classmethod
    def resource_type(cls) -> str:
        return "aws_security_group"

    def _get_name_from_tags(self, tags: List[Dict[str, str]]) -> Optional[str]:
        """Get Name tag value from tags list"""
        for tag in tags:
            if isinstance(tag, dict) and tag.get("Key") == "Name":
                return tag.get("Value")
        return None

    def _generate_resource_name(self, resource: Dict[str, Any]) -> str:
        """Generate a safe resource name from Name tag or security group ID"""
        sg_id = resource.get("id", "")
        tags = resource.get("tags", [])
        name_tag = self._get_name_from_tags(tags)

        if name_tag:
            # Use Name tag value, sanitized for Terraform
            return name_tag.replace("-", "_").replace(" ", "_").lower()
        else:
            # Fallback to security group ID
            return sg_id.replace("-", "_").lower()

    def _format_rule(self, rule: Dict[str, Any], rule_type: str) -> List[str]:
        """Format a single security group rule (ingress or egress)"""
        lines = []

        # Start rule block
        lines.append(f"  {rule_type} {{")

        # Add from_port and to_port
        from_port = rule.get("from_port")
        to_port = rule.get("to_port")
        
        # For egress rules, use default values if not specified
        if rule_type == "egress" and (from_port is None or to_port is None):
            from_port = 0
            to_port = 0
            
        lines.append(f"    from_port = {from_port}")
        lines.append(f"    to_port = {to_port}")

        # Add protocol
        protocol = rule.get("protocol")
        if protocol == "-1":
            protocol = "all"
        lines.append(f'    protocol = "{protocol}"')

        # Add CIDR blocks
        cidr_blocks = rule.get("cidr_blocks", [])
        if cidr_blocks:
            cidr_blocks_str = '", "'.join(cidr_blocks)
            lines.append(f'    cidr_blocks = ["{cidr_blocks_str}"]')

        # Add IPv6 CIDR blocks
        ipv6_cidr_blocks = rule.get("ipv6_cidr_blocks", [])
        if ipv6_cidr_blocks:
            ipv6_blocks_str = '", "'.join(ipv6_cidr_blocks)
            lines.append(f'    ipv6_cidr_blocks = ["{ipv6_blocks_str}"]')

        # Add security group references
        security_groups = rule.get("security_groups", [])
        if security_groups:
            sg_str = '", "'.join(security_groups)
            lines.append(f'    security_groups = ["{sg_str}"]')

        # Close rule block
        lines.append("  }")

        return lines

    def generate(self, resource: Dict[str, Any]) -> Optional[str]:
        try:
            sg_id = resource.get("id")
            details = resource.get("details", {})

            if not sg_id or not details:
                logger.error("Missing required security group details")
                return None

            # Generate resource name
            resource_name = self._generate_resource_name(resource)

            # Start building HCL
            hcl = [
                f'resource "aws_security_group" "{resource_name}" {{',
                f'  name                   = "{details.get("name")}"',
                f'  description            = "{details.get("description", "Managed by Terraform")}"',
                f'  revoke_rules_on_delete = {str(details.get("revoke_rules_on_delete", False)).lower()}',
            ]

            # Add VPC ID if present
            vpc_id = details.get("vpc_id")
            if vpc_id:
                hcl.append(f'  vpc_id = "{vpc_id}"')

            # Add ingress rules
            ingress_rules = details.get("ingress_rules", [])
            for rule in ingress_rules:
                hcl.extend(self._format_rule(rule, "ingress"))

            # Add egress rules
            egress_rules = details.get("egress_rules", [])
            for rule in egress_rules:
                hcl.extend(self._format_rule(rule, "egress"))

            # Add tags
            tags = resource.get("tags", [])
            if tags:
                hcl.append("  tags = {")
                for tag in tags:
                    if isinstance(tag, dict) and "Key" in tag and "Value" in tag:
                        key = tag["Key"].replace('"', '\\"')
                        value = tag["Value"].replace('"', '\\"')
                        hcl.append(f'    "{key}" = "{value}"')
                hcl.append("  }")

            # Close resource block
            hcl.append("}")

            return "\n".join(hcl)

        except Exception as e:
            logger.error(f"Error generating HCL for security group: {str(e)}")
            return None

    def generate_import(self, resource: Dict[str, Any]) -> Optional[str]:
        """Generate import command for security group"""
        try:
            sg_id = resource.get("id")
            if not sg_id:
                logger.error("Missing security group ID for import command")
                return None

            # Generate resource name matching the one in generate()
            resource_name = self._generate_resource_name(resource)

            prefix = self.get_import_prefix()
            return f"terraform import {prefix + '.' if prefix else ''}aws_security_group.{resource_name} {sg_id}"

        except Exception as e:
            logger.error(
                f"Error generating import command for security group: {str(e)}"
            )
            return None
</document_content>
</document>

